{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9af85ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69567c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: c:\\Users\\amaguaya\\OneDrive - Kienzle Automotive GmbH\\Desktop\\tesis_code\\repos\\loci\\local_run_test\n"
     ]
    }
   ],
   "source": [
    "# change main directory\n",
    "import sys\n",
    "sys.path.append('C:/Users/amaguaya/OneDrive - Kienzle Automotive GmbH/Desktop/tesis_code/repos/loci')\n",
    "\n",
    "import os\n",
    "print(\"Working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c36d2a",
   "metadata": {},
   "source": [
    "## Main imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12c78e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from causa.loci import loci, loci_w_marginal\n",
    "from causa.datasets import MNU, Tuebingen, SIM, SIMc, SIMG, SIMln, Cha, Multi, Net\n",
    "from causa.utils import plot_pair\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b01b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9764839707402513\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEmCAYAAAB8oNeFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACiOUlEQVR4nOz9eXQU+ZnnC38ics+UlIt2iUVKSaigNhCSocq1I8p293W1uwqqxjM+PW6PAc/7npn7njPT4Jq5d/pO95ymqO7pee97531twDP2cd8edxWUXWO7bZclal+gJCQoFiEkpQDtW+57ZkS8f4iIkkASgpLQQnzO4aDMjIz8ZWREPL/n9zzP9xEURVHQWXYEg0EOHjxIVVUVHo8Hv99PfX09ra2t7N27d1E/u7m5mX379nH48GEaGxsX9bN0dG7FgQMH2LdvH16v9wvtp6qqiqampi+8ny/CSr22jEs9AJ2b8fl87Ny5k9OnT+NyubTnd+/eTUNDw6J/fmNj44o6iXVWN8ePHwfg0KFD835Pc3MzHo+Huro67bljx44tqZGAlXttiUs9AJ2b2b17N4cOHZpmJOD2LhQdndVAc3Mzhw4d0ozFjQSDwRmfn+lamWo0dG4P3VAsM4LBIG1tbTOe1F6vl127dgFw5MgRmpubOXDgAM3NzQC0tbVRVVWlPd63bx9VVVXApJeivufVV1+lra1t1v3o6CwXfD4fu3btwu/3a+esSnNzM1u3buXIkSMcP36cffv2cfz4cZqbm/H7/bz22mscOXIEuPnaUN//6quvcvz4cV599VXtef3amgFFZ1lx+vRpZT4/i9fr1f52uVza33v37lWampoURVGUQCCgbbd//37l9OnTiqIoSk9Pj/b3fPajo7NUHDt2TFGUyfN37969N72+d+9e5dChQ4qiTJ7XdXV1iqIoyq5du7RzfOq26jk9dVtFUZS6ujolEAgoiqJfWzOhexTLDHUN1efzzfi66mr39PRo3sd82LdvHzt27GDnzp00NzdrHsvt7kdH525x5MgR/H6/tuz0+uuvz7ides14PJ5Zl6Ju5Pjx49NiBVPjgfq1dTO6oVhmuFwu6urqZj25VBf2wIED+P1+6urq8Hg8t9yvx+Oht7eXffv2cezYMc0lv9396OjcLTweD3v37mXXrl0cOnQIr9c7Y6zixljeVILB4KyTrtnQr62b0Q3FMuTYsWMcOHDgptnRkSNH2LVrF0eOHCEYDE7zPoLBIM3NzbhcLu19U9dFDx48iMvlYteuXRw7doyenp4596Ojs5T4fL6bDMBLL73Ea6+9dtO2s3kRfr9/ViPR2Ng47Tz3+XxarEG/tm5GUBS9jmI5MlMdxYsvvojL5cLn82m55cFgkJaWFgBefvll/H4/hw8fZufOnQDs3LmTY8eOaRee1+vF5/Px4osv4vf7Z9zP1772Nf7Nv/k31NfXz5h9paOzmKgBYK/Xy7Fjx4DJ6+HAgQMcOXKEQ4cOsX//ftra2ti9ezd1dXUcPXqUgwcPcuTIEY4ePYrX6+Xw4cNUVVVp2+7Zs4f6+noOHz4MTC4/+Xw+7Wa+a9cu/dqaBd1QrHKCweCKORl1dFYS99K1pRsKHR0dHZ050WMUOjo6OjpzohsKHR0dHZ050Q2Fjo6Ojs6c6IZCR0dHR2dOdEOho6OjozMnuqHQ0dHR0ZkT3VDo6Ojo6MzJqmhcdP/992uSv4tJOBwmLy9v0T/ndtHHdfv09PRw4cKFpR7GXWcxr5Xl/HvPxHIbbziRIZ6WMIjCTa/542n6/HHMRpGNJbOPWZJlivNsCDfv4o7p6elZHYaiqqqKX/7yl4v+OadOnWLbtm2L/jm3iz6u2+e5555b6iEsCYt5rSzn33smltN4M5LML88O4rSaZnz91bcukRqJ8sdbyvnDB0tn3CaVlch3WNhelb+gY3vuuef0pScdHR2dpaZrJDrrzXgknOTySBQBeMQ7uxFIZCRqinMWZXy6odDR0dFZQjKSzKXhMLmzeBPvXx4DoNJlwOMwz7ofk0Gc8/Uvgm4odHR0dJaQ7tHorDGFdFbmw+5xADYXz24EkhmJNW4bwkIGJ6agGwodHR2dJSIjyXQMhsm1zOxNfHrFTywtke8w43UZZt1PIitRXZi7WMPUDYWOjo7OUtE5HEac5S6sKArvdI4C8FRtIeIs3oKiKNiMBpz2mY3NQqAbCh2dFc7u3buXegg6d0AyI9E5HCVnFm+idzzG1Yk4RlHgseqCWfcTS0t4Cx2LNUxgldRR6OjcqzQ3N8/aX13n7iBJEi0tLQwPD1NSUkJDQwMGw+zLRCqf9YcwGWaPKbzTORnEbqjwkGs1MTTLdllJpqJgcQ2F7lHo6KxQ1P7NaitPnaWhpaWF7u5uDAYD3d3dtLa23vI9kWSGqxMx7OaZ5+rBeJqWK34Anr6vcNb9yIpCrs04634WCt2j0NFZoTQ3N7Nr1y4OHTo06zY/+9nP+NnPfqY97u7u5tSpU4synkAgsGj7XgwWarzvv/8+oigyPDwMwODg4C3f0zaUIiMrjM1QhQ3w3rUUWVmhPNdA1j/AZT9EozEuX+6ctl00o7DBY+RU9NoX/h5zoRsKHZ0VSFtbG42Njbfc7pvf/Cbf/OY3tcfPPffcolUjL6dK5/mwUONVFIXu7m6cTiehUIiampo59zsYTNBnmMA1S/A5mZE41/YZAN/YWsGGdW4ALl/uZMOG2mnbhhIZ/nBzGSbD4i4O6YZCR2eF0tzcDIDP5+PIkSPs3bt3iUe0MrndGIO6/cDAAIFAgLy8PFKpFOl0mpqaGurr62d9b1aSab3iJ882+633g65x4mmJ4jwLD691zbpdRpIpzrMsupEAPUaho7MiqaurY9euXbhcrqUeyorndmMM6vY+n4/Tp09z7do1LBYLa9asYdu2bXMamc/6g8iKMmuqa1aWaeoYAeDZTSWzbgcQTWW5r2TxaiemonsUOjormMbGRnp6epZ6GCua4eFhnE4nAE6nk6Gh2fKLpm/f0dFBQUEBfr+f2traW74vEEvRPRbDY5+9wrr1SgB/LE2u1cijtxD3sxhF8nMsc26zUOgehY6Ozj1NSUkJoVAIgFAoRGnpzOqsN27vcrkYHx/H4/EQCASYmJjgzTff5OTJk0iSNO09kqzwcc8EedbZ5+ayrPDrc5PGZsd9RXMuKUWTWaqLchdNsuNGdI9CR0fnnqahoQFBEBgaGpozxjA1NpFKpaioqCA/Px+n00kwGMRkMmnLV4IgTAton+kLkJFlLMbZq6dbrvgZDiWxmw3suK94zjFnFXnRi+ymohsKHR2dexqDwTCv7Cc1NuF0OrFYLKxbt05735tvvglAZ2cnfr+fgYEB6uvrMRgMDIeS9IzG5lR2lWWFX3026U08u6kYm3n2OEdWkinIsWA13bqob6FYNoYiGAxy8OBBXnrpJerq6pZ6ODo6OvcwkiRx8uRJPvzwQwAee+yxOWMZJSUl/O53vyMcDgNgsVhobW3loS31fHB5lNG+Xi6HgrhdbmpqahBvEHj69Iqf4XASxzy8iUgqy5brKbN3i2VjKFpbW7VKUx0dHZ2lQpIkfvzjH9PU1KTFANrb2/F6vdx333243W6tXkKloaGB999/n0wmg9vtprq6moHBIYI5Ywxc7WV8dBi7w87Q0BCCwLR6iKws86vPJov0nr2/ZE5vQlEULEaRory7E8RWWTaGorGxkaampqUeho6Ozj1OS0sL58+fR5ZlRkZGEASBoqIiYrEY2WwWSZK0WMbUGgyPx0NJSQlut5tgMETGvR5DJks0EsTusANgd9jxBwLTPu+DrnFGwilyLEaeqS2ac2yxdJb7SvPuWhBbZdkYitvhbsoSTGW5ShTo49LRWTiGh4dZs2YNw8PDJBIJ4vE4sixTUlKC0+nkG9/4hrbtyZMntbiF2WzWDAmedQyHUrT86peMjo2Sl5dHVVUVyUSSsrLPs6oSaYlfnp30Jp57uGxObwIgKytULrIA4EysSENxN2UJprJcJQr0cenoLBwlJSWEw2E2b97M2bNnicVirF+/nmvXrvHOO+9QXl6uVW9PjVu43W4kSWLj9mf4xXtt9HScIx5PYDKZCPgD9Jn7cDldjE9MoHR2UlNTw2/PDxFJZinJs/LEhtmlxAEykkK5y47FePeC2Cor0lDo6OjoLBQ3SnjU1dUhCAIDAwOUlZUhyzL9/f0UFRURDof5h3/4By5cuMC3v/1tSkpKpuk8WYsq+awvRDYeIpPNYrFOxhIsFguiIGIymTAajQwNDRFJyzR1xAHYtXUNxtk6GF0nKcGmsrxFPx4zsWwMxY26+nrmk46Ozt1gatrr1BqId999l+LiYmKxGLFYjPHxcYqKisjNzeXcuXO0trZqNRh9ff10+zP09pzDLp4lJycHg8FAMpEEJmMTsizTebmTgD+A2+NmLN9GRjKwoTiHh9c45xyjJCs4TAJO2+J1sZuLeRmKt99+m2eeeWZRB9LY2DgvNUwdneXK3bhOdBae2dJeZVlGlmXC4TB+v59MJoPJZCIcDiPLMr/4xS9QFIUtdVs5M5Sg5dI5ooFxMtkMDrud0tIy8px5GESRjZs2cerUKfqu9ZGXl0f3eIqA1YAowD9pWHfL4HQkmcHrXrp5/bwkPObSu9fR0ZnkVteJmmOvs7yYTcIjPz+fVCqlGYb8/HwGBgb44IMPeP/99+nu7qbtfCf/v199wqefXSIWnCCdThMJR4hEoqTSKR5+6CGef/4FNt63EVmWMVvMjE0EiBQ/DMCOjcWs89jnHJ+iKFhMIh7r0ikuzeuTKysr+Zu/+ZvFHouODrKsIMkKGUkmnZVJZSWSmbn/pbKT/9JZmYwkk5VkJFlBlhUURblrY1cUhR/96Eezvn7w4MG7Nhad+VNXV0cqleKDDz7g/PnzNDc3c+jQIcbGxigrK0MURSorK7HZbAwPDzM8PEx+YREjkp23zg8SCQUwCzImswl/wE88EWd0bJRoJMq5c5NptgBGg5F0Ko2wfitZkwMLGf7o4bJbji+SzHJ/mfOup8ROZV6+zA9/+EMAjh49yksvvURe3tIEVHSWN7KskJZkUtnJm/xoLEvncJhkRiKRlklJEpmsgqQomkGQUVCUyTVY4PqNXQAUEARQuP43KJP/aShMboJy/VmB6xtNed/1dwgCCAiIImQlhVgqu+Df//e//z2hUIijR4+yZ88e7fmf//zn/NVf/RXt7e0LaiyOHz8OTK6x79y5U1+6vQMkSeKnP/0p58+fR5Ik+vv7sVqtlJaWEgqFuHbtGrm5uYyMjOB2u4mnMlhLa+gXirGkDQQ7zrFhfblmRCLhCAoKeXl5pDNpOjo6uHbtGvc/cD8VFRWMpQQ6zOsAeLRYmZcMh8EgsD7fwZhvsY/G7NzWoteePXt44403qKqqYvPmzYs0JJ3lTDSe5L//3d/T2XuN4jWVfPnpZ8koIqlMls6ubvz+ANFYlGgkRsA/waMjSYwGkUgkTL7bRe2GDRhEEYNB4C5K1Uwjns4iL5Kn4XQ62bNnD3/zN3/DxMQEhw8fJj8/n/379y+oHHhzczM+n4/9+/dTV1fH7t27OX369ILt/15BLa7Lzc3l7NmzRKNRjEYjNpuNixcv0tPTg9liJW2wYclbjy23mlgsQjQwRkSRWbduHZ2dnYyOjiJJEnaHnXA4TDqVpu9aH1arlXA4TCKRwGJz0OvZCoLIenuGhsq5ZcQBIqkMG0vzMMzSMvVucdvRkRdeeIGXX36Z1tZWXnzxRfLy8vQg3ipElhUiqSzBWJrRSJKJaIrzHZd5/6MPCQbCWG0W2nwtdA+M861/9s/ov+IjPD5CJBCgo6MDs2VSAO3jD96hoKCAqqoqRoaHMYgCGzbUIssyXV1dBIKBWfVvVho/+tGP8Hq9HDp0iKamJrxeLz/60Y94/vnnF/yzpiZ/+Hy+Obuq6czOwMAA6XSad999l3g8TiKRoKCggE8//ZRL3b3krt2EbHcjJ9OkFYE1xflcvRpFMZmwO+yMjI7S19ePIArk5OQQj02mu8biMSLRCHa7nbLSMkLhEBNCKQhWxGyShx0xqqtvXWOkKFBVmLPYh+GWzDvryev1cvz4cX74wx/i8/lobGxk//79CIKAx+Ohq6trsceqs4gkMxLj4QRNH7dysv0iGURqNmyguqoau8WIr+syHefa6eu5TDqVxmaz4Xa7aPn0UybGx7HarDz88MNEohEMRgNSNovNZicajWpd2KbKF3R1dTE0NDSr/s1KZO/evbjdbvbs2cMPf/hDKisrOXr0KFeuXKGiomLRPvfw4cOzBtLvporBSqvEDwQCtLS08MknnxAIBIjH41itVjq7ugmZPKRL7mc0ksCSGMPhcCAoRkbHRvHkezAaDHR39xCNRRFFESkrEQ6FQZhUoxUFAVlRCIfDZDIZJOda7JseBMDQ8wEXRuG+qoo5xxfPKKzNM9LWOqCNd6mO77wMxa5duwiFQjQ2NnLo0CFeeOEF7TWfz8f3v//9RRugzuKQkWT80RRX/XHGIiliqQzvv/8+n378IWPDQ0iyROsnLnbu3MnOxp28+967jI2NkYgn8Af8mM1mRkZHMBlNpFIpEokEp0+fZk35Gvqz/ZgtZtKZNDk5OQiCQHt7OyMjI1RUVFC/tZ5AMDCn/s1KZO/evVo8T0VdrgXweDwLHt979dVXefnll/F6vTO+fjdVDFZaJf6pU6dwOp04HA7C4TBGoxFPUQlJdzVCJoM5GSOWTRFNSYgGkQceeABJlhgZHmE8GCQcDqOgIFyPg0mShCBOxsdsuTlk0hkkSUI25WDd/AcAWCe6MMZHSJvdt5wYhZMZvv5wmdbAaCmP77wMRWNjI0ePHtVyjafi9Xp5+eWXF3xgOguLJEm8/f6H/PT4rxlLCeS4ChgeGcE/MozDbqGsrAxfj4+hoSEymQxGo5FkMslvfvMbxsbGCIfDxONxDAYDsiwTi8YwW8zk5ebh9/vxeDyMj4+zZfMWbZ3WP+Hnsccfo7W1lcGBQfLz84lEIrzz7jusW7tO8yjisfg0/ZuVymyz+hdeeIE33niD73//+wvqeTc3N9PY2EhdXR3Hjx9n165dC7bv1U46neaNN97gzTffZGxsjGw2i2AwkUjnYQ5MYBQFUqkUmXQGg9Ew6Wl0diKKIqlUCr/fjyzLiKJIVsqiXE/GMBgMKIpCKpnEZDZjsdkxN3wDxWRFCgxiHzmLYjJRWFQ45/iiqQz3leTN2eXubjIvQ3Ho0KEZjYTKli1bFmxAOguDqqf/7gcfEcwaGYhk6fJdIxQMkooGCQQDSNns9QwhhY6LHSiKjBrjzWQyAMiSzKlTpyjIL9CKjhRZwe6wk0qlCIaCeDwekqkk+fn5JBIJnE4nFesrkBWZjfdtpLW1lZoNn0syDw4OsuOZHQgC+AMByspKqa6umelrrCjmukZeeOEFXnvttQX7LJ/Px+7du/F6vQSDQRobG3VDcRv85Cc/4Re/+AUjIyNEo1EURcHqrUNQZOKROKIoYjQaEQ0iAgIBfwCj0YjBYCCRTJDJZBBFUUt9BbDarKAoZDKTyRKJRALPludQ7PkI2STW3vfxlBTgyHHw5JNPzjo2RVFQgA3FSx+bUJmXoaisrFzscegsENFolO/9y/8H77d+RszoxGh3giJjJEsmnSGZSpLNTEkNVdS0VHnG/WUyGaKRKJl0hkQiTiKZRBRFMukMRoOReCyOzWqjuKiYTDrDu+++g9FkIpFIUFpSSk11DaWlpXR3d5OXl0c4HNYC19XVNXR1deEPBOjs7AQgFA6tmuD2jSyk5+31egmsguW6pUCSJH7+85/T19dHNptFURQMeUUYbC6kVHxavYJBFBEEEUmSJmMN15eXZFlGkRUEUUCRFUxmE1aLlVgspvXLtj+wAwqrUOQstt4P2Lr5ATbUbqAgP3/OiVEkmeWhtU6My8SbgGWk9aRz+0iSxMcff8x/+S//hXg8jsmey4X+AIGsgawxHykZg8SI6jTcEYIokJWyJIJxRHFy2UktYjNiRBAFYrEosXiMdDrNxMQEJpMJh91BT08P77z7Ds88/QyCIDA4OEhNTQ1PP/U0MD2gfeZMOwgCVVVVcwa3V3K2lO55Lw9aWloYHx8nnU5PnsuCiKW8FjkZvV6CI6BICqIoUlBYQDAYRFFk7TxTDQGgLTnJkowgCponbqlqwFazHYBY2z8iRfoIFdhxu+eOTciKgtEo4C1YPt4E6IZixSJJEv/tv/03fnj4MJevDiPkrwdzFjmTQsmmp237RUoGJEkik05jMBpJpVJI2SyKAlkxi2gVsdlsSFmJifEJcnJyEEVRC+6ZzCYGBwcxGo3sbNyp3eRbT7fidrmZ8E9oAe2sJGkGSA1uz2QUVmO2lM7dZXh4mPz8fAwGA9lsFoOzBAQjkJq8VhQFo8mIIEAsGptcYhJERFFAFEUcDgeJRIJUKqXtU5IkAv5JD89W/SVyHnoWgOj5t5GHL5E2mRgcGqTj4kU23rdx1rGFEhke8eYjLnHdxI3ohmIFkkgk+PZ3vstvP2xFyilGKN143XuILPyHKWhG4sYlq6yUxaJYsFgsmEwmSkpLSKVTpNNpTGYTJkyUlX0uUXDjTV4VWbM77BgNhutl1mjB7ZmMwmrMltK5u5SUlJCbm4vNZiMWi2Ep9qKkYtO2yWQyyIoBUklEQSR5/X+j0UhWmr2q31azjdyHvwJArOMD4pc+xGKxYJiH15uRZJw2E2tuof20FHwhQ9Hb24sgCIuaI64zmaHxk5/8hObmZt79pIWwmIchJx9yS1HSCZRM6tY7uQ3UpSpBFFAUhXQ6Pc1IGIwGDAYDAgJZwUBBaQUpWeBqWMG+/iGEdJa4wYQ9x8VoTg0//qiXjKQwMh5CVhxI4yApDrKyAqKRzJCEItSiKPDJZRlBzEUcSZBMpVCwTo5JdEBfGAQjiqJcl+4Ag2jkH66cQRBAFCZ9GUEAQRAQr8t2TD6efF28bozMC3rEdFYSDQ0NVFVVTWYt2Z0IJstNXjiAlJWQspJ2PUhIyIo8zZP4HIGch3Zi33B9uanjA2IX3gEml0udThflZeVs3LRp1nFFU1mevb9kQb7jQnPbhuJHP/oR3/3ud4HJvHCfz8fx48f5t//23y744O51JEninXfeYe++7zEQTGAoqEB0VSGmEjfNgBYSUTSAyYZgzcWY40Kw5mGx5yFaHIhWB6I1Z/Jvsx1EkdCU98Zu+Pudy+NT93zDJwmAjKbtpD4nKUAWmKLxocbalevbqNlZEjDHDG829PSMexeDwYDL5cLpdBK2lSGl4nNuP3XpVo1JTEUwWshr+CMs5fcBED13gnjnR9rrJpOJ+x+4n6eeeoraWZZJ4+ksFQWOJes3cSvmbSjOnDkDQE9Pj/Y3TFYLdnd3L/S47nkkSeLIkSP8+aH/NzFbGcZSE0oyirxQy0sGI4YcD8YcD4YcDwaHB4PDhWjPw2B3Ihhu44TNpiCTxICMSQSDksUoyAhyllybBbOoYDYaKC8twSBCOpnAmetgbXk5ZqOBoYF+/BPj2G1WkskkRYUFhCNhBGB8dIx4Io7dZuXZnc9qAUXlupigrEz+ryhoAoOfPzeZZqg+Vv+Pp7P8rm3Ob7QghMNhfD4fmzdvXvTqbJ35I0kSfX19GE1mDDlupGjo1m+aBaO7DOe25zHkeFCkLOGWN0n1X5y2TU6Ogy1btswam1CUSaHMh9e47ngci828DcXExASHDh3C7/dPEzcTBEEvuFtAVC9i37/8fzKQtmHMW4uSiMCdLC8ZjBgcU4xBjgdD7nWjYL91hbCciCDFQ0iJEEoyAqk4duOk3k2e3UQmFp7UdwqHUFAmC5QAxWjEU1iE3W4nT8hDFEWi0SiRyFUsVgvV1dXku03UlDsRRZHQtTA5nsklJmxW5HSYqmI3Q0ND1K4v1mIWxU7b7R+DGYins/xuQfY0O2+88QaHDx9GEATeeustFEXh5z//+aLoPuncHidPniQejxPHQiZzhyrCohHHxsex1z6KIBqQYgFCp35O1j9w06aKAh63e9ZdhZIZtlV6MBuXb/bevA3Fjh072LFjBydOnGDHjh2LOaZ7lnQ6zb//9/+e/+/hHyGUPYDBJszDgxAQ7U6MeQUYcvIx5uZjyM2fNAr2udsryukEUtSPFJlAigWQYgFMUoqCXCtKKsrY6DCJeOJ6aqABh8NBTlEhTsWEOW2m48JZkskkld5KstksyWQSKSthsVgIBoPEE3EymQwTExMYTUYcDgfZbJYch4OrV65w4cJ57r//AZx5TkZGRqZVaVdX16zogrxAIMDvf/97Tpw4AUzWIrW3ty/xqHQA3n//ffr6+khZPCjpuZedZsJSdh85DzViyPEAkOy/SOT0r2aMFRpNRpwu56znbyojUZBjYa3HcdvjuJvcdoxix44dhMNhTbNGd6m/OJIk8cEHH/Cv//W/5nxnN7bqL6Fk0iB/nq+NIGJ0FmHIK8SYW4AhNx9jzqRREAyz/4yaMbj+LzvlbyWduGl7m8dNLGMiHouT45hMd00mkxgNkyd8Oj2psa8oCqJhsln80OAQLpcLl8uFwWDA7XIzPj5OJpMhnUkTiUQoKiqalPpwuzn72WekkikMBpFEMsmWLVsoKyudZhREUVwVaa9Ti7d8viVsKKCj0dfXx7X+ARTzmtvKFDSXbsCx6UlM7km5GSkeJnrmd6QGL824vSiKeDweqqurZ6z1URSFZFZmh/fWcuNLzW0bir/+67/m9ddfp6WlBUB3qReADz74gO985ztc6R/EVrNdMxKGnHwsazZhKa3B6CqZ1SAoUnbSCEQmkCLjSJGJ6wZhYkZjMBfxWJyc3Byy2QzJZAKz2Typb5NMEo/HSaVSk2v/ikJebt7khEGYlFU2mye1nzLZDIIo4MpxUlFRiSiKBIOTUh/hUJhUOoXBYMBmszE0NITNauX551+49eBWEG63m2effRa3201bWxtNTU3s27dvqYelA6xdu5asyTHvniSW8vuw3/cYJvdkqrecSZHoPkW88+MZs6VUjCYjJpNpVrmOUCLLl7zueTUvWmpu21B4vV7NSIDuUn9RotEo3/rWtxgcHcdWvQ0lk8aYV4jj/qexlE53V+V0kmxoBCkyTva6QZhcNgryedbQFyOdSZOIJ7BabSgoxGPxyRu/IBCLRhFFA2azmXQ6jSRJpFIpLBYL69auJRKJ4g/4WbtmLbm5uQDk5OaQn59PcXExoiBiNpkJhUMoioLL5SKZTC7IuJcbL7zwAl6vl9dee42enh4OHTqkN/taJjzxxBP8t3/8GGVkfM7tLGvux7HpCYx5kwJ+SjZNvPtTEl2fIKfmnoAZDAY8Hg8lxSUzBrHj6SzlbivrlvmSk8ptG4re3t6bntNd6jsjkUjQ0NDA0HgQW80joCjkPLQTW1U9giBO1jAMd5MauERm7ApSbPGLyxRZIR6PI0kSJrNpUkb5+vKJokwuk6XTafLy8kilUwhpsFgspFIpCgoLSCWTrF+/jtr77qO1tZVwaHKZsrKikqKiQiRJpq29jcHBQVLJFE6Xc8bc8pUs1QGTWYKCIPDKK69w9OhRXnvtNbxer95GeBlQt7WerMEKs+ibGXI85G79OubC9cDk8m2i+1Pi3Z/O30MXoCC/gO2PbL/pvM1KMqIoUF/h+ULf425y24Ziy5Yt1NfXs3PnTmBS6ljPero90uk0P/rRj/g//vI/EVQc2DZsx5BTSN7W/wWDwwVA8to5YhfevSvGYSZSqRSp1OQSkcFoQJEV5OsXliBMpn66XC7WrV/H0OAQmUyWQCBAWVkZZeXlGA1GzGYzVVVVxGNxiooKtc52CGCzTmY5bdy0acbc8pUu1dHU1MTu3bs5evQox48f5/XXX+f111/XapAWgiNHjuD1emlra2PXrl2z9qTQmc67n7SSkWY2EubSDeR96Y8RrxfhxTo/ItF1as4lphsRBLCYLTz19FM88/T0zp+KohBOZvjqg6XLRkJ8PtxRMPvYsWNaY/cjR44smNjZaj3xs5JMNJUlmswyEUmw/3/7cz78tB3y78NizcFevQ1bxWYApFiA8Olfkxm92XNbCiRJ0hQzRUHEYrFgsVo0meVoJEphUSHBYJCK9RWIgojH7cYfCGC1TBqDqVIboiiy8b6Nc+rdwMqX6ti6dSsVFRUcP36cffv24XQ6F1SF2efz0dPTw969e2lsbGT37t0cO3Zswfa/mrlwbQxBujlDyVJ+H3nbdiGIIumxq4Q//QVyInzb+7c7HHzlK1/h2Z3P3vRaMJ5he1U+edblWVg3G3ck4ZGfn8/OnTu1QqKFYLFOfEmSaGlpYXh4mJKSEhoaGjAY7ix4JEkSH330ER9++CGyLJOfn09+fj7l5eU0NDQgSRI//elP6bx8GUk0kzA4eK/lHBlFxGK1YDQY6Lp8GWNeEfb7HsNcXI3J87kWUrynhdhnzShS5gt/74VEECZv8CajEaPRiMU8qe0kyzIWi4X8/HxKSko0ORdVPlxdpryTxkRul3tFNzby+Xy43W5aWlq083imZds7pbm5maqqqmmfp3NrFEVBtjpRstPrJ4zuUvK+9DyCKJK4cpbI6V/NujR1Kx588EH+6Tf/6U3PhxJpNpTkrpi4xFRu21AsViHRYp34LS0tdHd343Q66e7uRhCEO24n2NHRQV9fH8FgkPPnzxONRtm6dSv5+fm88867nL3URedggLQxl0g8QTjoR0rGkWUZU0k1ljWbyP/qs4jm6YVj6ZEeohfenbFYZzEQBDCbLciyrMmGT21adOO2osFAbk4uJpMJUZxUjLVYLOTl5bGhdgNutxuHw6Hp7IuiSE1NDdf6riHL8h3VQdTUrOw6ih07dnD48GFOnz6Noih8//vfp6CgYMH2HwwGtV7k6uOZ0HtmTyeckrl6rW9a3EAwmnFu24VgMJIa7CTS+kvuNDkkLy+PhoYGrlyZPilIZBVcFpFExsSpwTtThl32PbOnsliFRPM98eH2Tv73338fURQZHh4GJnOoz58/z8TEBPn5+WzcuHFGD0OSJDo6OqZtd/XqVa5du8b4+DgDQ8MYbTm8195JXmkF8azA0MgYSBmyST/JVBJFVjCX1OB84BmMrmJt33I6QXqkh/RwN+nhHuQbdZsMJkSzBWQFOZNEkbPXZzciiCKCwYggmkA92QUQFBklm0WRsrPOhIwmIwICJpNJq3sIh8NEohGMRiNSdjKLyWCclF9Op9IIgoDVasVoMlJQWMCmTZtIJiYzldZXVLB23VpEQUQB0pLChc7LoExeZo48D1aHnXhG5uzFTu3S0y5BRYBbXDMmh4tIWqH94uW5N5wLBRA+v/DTkvKFpNfnQ2VlJa+88or2+JVXXuHnP//5gu3f5XLNeY2o6D2zp9PS6yf5Tttkb+vr2Dc+MSnlEQsSbvmf3KmRMJqMrF+/jh3P7JhmiOLpLLlWI09uKPpC8uHLvmf2TCx0IdF8T3y4vZNfURTNowiFQlo658aNGwmFQhgMBu29iqKQlRUkWeGTk5+SEc3EFSNd57oYiAtcS5q5FDYyFDIjFGzCbLVgMhgZnQigyBJKMkYqlUKWZQzucnIe2DElcyJJ8upZUgMdZCb6bmoSIRhMCCYLKDJOo4QUvEp0YgQpk55sBORw4HK5EEWR3Nxc+vt9xBJJBKOFqg334cwvJhhPIRvMTIQixBMpjGYLRSWlOF0ukokEgUAQh8NOVXU1DruD+x+8H2T45JNPGBsbo6AgH4vFysTEOKnUpFy4IktYTCaKigp54vHHue++WowGA6IoYBAEjKKAQRQwGgQMoohRnOwKJgpw8fx5Nm9+CFGY3FYUJ9VdDaKAIFxXdQXNWKh9LKacWnPakalHUD2cytRnlc//U19XA/In/35xFfarq6unXSMweZ0EAoEFyXyqr6+f1lq1rq7uC+9ztSPLCgPBBMGJMdKpyeC0IceDvWby+o+0/xYlc2fp2oIoUFxczDe/+U+nGYlEJovNbODxmsJl12Pidrjtq2WxCom+yIkfjKc55ZuYbKijyCgySNcF4LK56wk50lwZC+B0r2MiMYEhY0QZB1lxcvncMMOWAST5upDc9btL26VRhocThIJZzPYS3mvrJJNMgCCSScQmZ9wxIwajARQwW8xkMhkEh4ecTU9hKZvM0FGkDPHuFuKXPvz8JBQNCBY7AjIFHg/bv7SVHY/W09/5GXZR4vHHH6O+vp729nb6+/vx+/1MTEwgiiKPPPIIsixz7NgxwuEwTzzxBLW1tTz66KO0trYyNDREaWkpW7Zs0d4fDIaw5+QQDIYY9/uv76eOhoZ6DAYD/68XntRkuGVZor3tNCPDw5SWllJfX3/HMZ1In5Hqotw7eu9iY1jki7axsZFDhw5pj5ubm3G5XAuWHltXV0drayvNzc34fD6OHj26IPtdzYxHU2QlGY/bg3Rd9cB+32MIooHUUBfp4a473vf27dupq6tj05RU71g6i8Ns5MnawmXV1vROmJeh+MpXvqIVDDU2NmqFRN3d3bzyyisLkvX0RU78VFbGH5ucIQhTexAAgiBSW/t5WmUnnTcFSR3mzw+DLMt0dnbSea6dK1euUFJSglHJwWKEq4N9k015TCbMFvNk0xOLZbIFotGObcsjmMo3Xa+BkEn2niHW8Z6m1yQYzWA0IUX9rHfItH/yPnbr1M4IO6d9r23bts3qKU2t9jx16tQ0z2g+758dkUe2b7/N9+jcyA9/+MNpj1944QXefvvtBf2MvXv3Luj+VjsXh8I4LEYSiQSSJCHa8rCuexCA2MX37ni/9z9wPy88/7wWnwOIpbLk2Yw8XrPyjQTM01A0NjZqVaXHjh3ju9/97jTjsFASHnd64ouCMO+c5JqaGmRF5uKFiwgCFBcXTzYwuf4Dd3V1ceZM+2R2j8XCwMAANTU1RKNRUqmU1gJREAQcdjuGgnWIZQ8g5ZVjFib3kezvIHbhbaTIxOdjtOWSDY+RHrzEptoa3j/xmxuMhM5q4kajEAwGaWlp4ZlnnpnlHTqLSSItMR5N4bKZJ71zQcRe8wiCaCA94iMbGLyj/a5dt5Y//w//Aav18wSVYCLDGpeNL1V6VvRy01TmZShaWlqoqanB6/XS09MzLW11YmKC3t7eZa31dGOVL8pkiq/dYWdkZASD4XMBukAwQCabxWazUVNTw8TEBFarFYfDgcPuIJHMYCkux1DoxVRWS9buAq633hnvJXTubdITU7KXRAOixU7y6lmcJplf/ebXPPPMM3e8nKOzMti7dy87d+7U+oC73W7dA1hCOoZCmK9PJu0OO2lZwVXxMADxyx/f9v4EAWpr7+M//af/hNk8OeGTFYVgPM2Da1xsLF1dFfjzMhQNDQ28/PLLuFwujh8/zq5du7TXFEXhyJEjizbAheDGKl+/f4I1a9cC04u5stkslzsvc+7CeSz2XArKKxEKqxBK1xNghIRlE46cfARhivcipcmNDxPpOkVs5CrZxOfBMME4eQJle1t4oLqS//pf/yuPPfbY3fviOkvGsWPHpnndJ06cID9/+auErkYSaQnfeBzX9e5xjz32GO9cHEQ0WZGiftIjt5eMIwiTigJ/8R//QjMSyYxEWpJ5qraIojzrgn+HpWZehqKqqko76d1u900Vpv/u3/27hR/ZbZCRZIZDSdKSTOb6v3RWJiMppCWZzitBZOxkkyDJdgIpCV9/FgwmUuksRouN99+6xPBEkLhcTXbTfSCIDMNkR86+NOBGuB6XlZMR5ImrMHGNnEwARMgE/ZhMJtLpyViJyZ5DOh4h23+O7Q31/MVf/AWPPPLIEh0hnbvNjXG7hoYGDh48yMGDB5doRPcmiqJwqncC25SmQKIgYqucTJZJ9N5ear8oimzdupVdu3dhNptRFIVQMku+w0RjVfGKUIK9E+ZlKHp6enj77be1paeprVABDh8+zA9+8IPFGN+8uDgU5n/7n+fn2OLGH88JWvGzCeJZIAoYp7V1NpFlbYGTwPA1koERUhMDRAZ7SATHsdvt5Bfka0168vLyCIfC5HvyScoClmyMyjUO/sPhX/PEE0/oS033GCdOnGD37t0IgoDb7aa3t5f9+/cv9bDuOTqGwkzE0jinSGa0XOzBlL8WRZZJXj07732ZTCY2b97M8y88T+2GWpIZiVRWpm6di8oCx03p0KuJeRmKvXv3cuDAAfx+vya1MZWllhkXAJvJgNkoYjIImA0iRoOI2SBiMgqYRJFUIoacTWO3WijwuDEbr79uEDEbRXIsRjovnGFk4Bouh5VkJEBNlRdBCNN86QSJgJ9sNouQTmM2m7BYLWSzWeKxOHl5eUiyhKeggKxg5g82V/H/+cvv68bhHqapqQm/3097e7vmXbzxxhtLPKp7B0VRuDgY5uJQGLd9etLI1dRk4Dk9dBk5Gb3lvgxGAy88/wJb67dSu6EWGQgmM5S77GxdvzL6SXxR5mUonE6nlu43UytUtUp7qXhojYu//OYCpOiufYJ33n2HwcFB1tdUU1Zaxscff4zD4WBgcGCykE404HDkTEpfyAqefA/r1q0jksiQTqfYvt7FX/0ff6YbiXscVV3Z6/VqXSDdc/RN1lk4Yqksp3onCMTSNxmJjCTTn7aDERJXbj3Brb2vlr/8i7/AarUhKwrhZBaH2cCO+4rJz7Es1ldYdtyReiygiQFWVFSsmh7aRqORHc/soLOzk4sXL3Lu3DnSqTSyLFNUVMTI8Agul4vcvFwEBDz5Htasr0QWTRRGR/nOH+1g25fuXHRQZ/Xg8/n43ve+R1dXFwcPHiQUCuH3+/X02EUkI8lcGAzTNRrBYTLitE03ErIs8/rbLShGC1I8THq4e9Z95eblsnXrVv71v/rXGI1GIqkMIgINFW7WeeyreplpJuZlKF566SW8Xi8NDQ1aGmxFRQXt7e288sornDhxgq6uO69qXApma4yj1lHE4wlkWWZ8fJw8Zx5utxu73U5VVRVr1qxlLBgmk5WpXV+GhygP3Ne47HVudO4ee/bsYc+ePQBa86LGxsYlHtXqRFEUrvnjtF0LICLgnmIg1Ot8aGiIn/3Dz0hU7cBc7CV55cxNMjowKSX0+OOPU1FZwdNPPY2EQDSR5r6SXO4ryVsVxXN3wrwMhaIoM2ZrbNmyhR/+8Ie8+OKLCz6wxWa2xjiBYIBMJks0GiWbyU4W4ikKmx54mEgixcMPP8yDGzdQ4bExfvUSE6PDlJbWUF9fv9RfSWeJOXr0KKFQCIBdu3ZRUVFBKBTixIkTmtHQWVgSaYmTvnEmommcNtO0mb4sy/z2t7/hV7/6NYODg4h2FwXFXhRFuWnZyWAw8If/yx/y7X/+bYzGydtiKJHG7TDz9H1F2M2Lqw223JnXt1fXW9vb2zl8+DCBQICdO3dq3brU11cSamMcWZYJBAJcuXaVdFbBbM9jPBInEIphNJnI8RRSsaaUmnXFiPEg+5575POK6nJd6kLnc+rr6zl48CCvvvoqFRUVwGR87/nnn+dHP/oRL774ot4KdQHp98c51evHahRx2ad7ER0dHfzP//k/aWtv0wQA7dUNAGRGe5HjoWn7+su//Es2bdqEKIrIikIgnubhtS5qi3PvuWWmmbgtM7llyxYOHTrEkSNHprV09HiWtverrCjE01lkefJvWflcQ1QQAOX6/9elSgXAmuNkdGSEcCjA6NAQ5WXFxCYG8a4rZ3Opg6vxAUQlwgMb76O6uoIXXvgGp06d0mU3dGbl9OnTvP766zO+9t3vfpcf/ehHC9oK9V5FlhXa+wL4RmO47NO9iHQ6zf/1X/8vWlpaiMfi2vOCyYr1eu1EvOvktP0VFxfzwAMPTO5bUQgmMjy5oYgS5+ornLtT5mUopv4QTqfzpuyNwBK3qbSZDFQX5WA0TKbHmkQRk1HEcF0K22C4/r/4uSy2/HApra2t/OIX7VRXFFBdXYbBYEBK+Xmu8TG6K0o0afI1a9Ys6ffTWRncKqtJz3r64qSyEh9cHieazOJ23Dxp+9k//Iy2tjYSicS05+012xBNFrLBkWlBbEEUqKis0B4HExmeqCnUjcQNzMtQ7N+/f5pk8sTEhPZYURR6e3uXdKaUZzOxqcx5W+9R1VbVfhUGg4FQKERNzWS8QRAEhoaGtMc6OrfiVhOmhZxQqT3rW1pa2Llz5z0RKA8l0rzXOYYoCORYP791ZbNZ3n7nbQYHBnnv/ffIpNMo8ueBatHuwl77KACxjven7dNoNFK3pe76/jNsXuvSjcQMzMtQvPjii+zevXvG11aC1tNcNDQ03GQUZpLs1tG5Fd3d3UQiEXJzb+7BEQ6H6e6ePR3zdlCl+Pfv309dXR27d+/m9OnTC7Lv5cpAIM7HPRPkWo0Yrys9qxlNb7/zNoFAAJvVRiKeIJP5vB+2YDDh3P4CgsFEevQKqYEO7TWDwYDH7cHpcpLMSOTnmNlQvDz7pyw18zIU+/btm7PnxEoWO9ONgs5C8fLLL7Nlyxb++q//mi1btlBRUcGVK1doa2vjwIEDC3Yzb2xs1DwIn8+3qj1eRVHoGApzfiCM+4Z4hJq5GAqGJlNk+67h8XgYHRtFAQz568h5aCcmVwlyKk649X9O9qkRRESDiMlowu12U5CfTyor0+hdufexxWZehuJWjYkWonGRjs5Kx+l08vvf/54XX3yRtrY2BEFAURS2bt3K73//+0XJeDp8+PC0ZeEbuZ3+8l+UQCCwoPuWZIWO8QwTCZkcs8C49rxMf18/58+fx+6wa5/tD4bBs568hi8j5K9HME0uIcmpOJFPXsMoJRAtFjLZ7PXfRqa4uJhISqJw4ipn2wZmGcnyYKGP7+1wbycH6+gsMF6vl9bWVkKhED6fD6/Xi9N5e/Gz48ePz9iHfteuXXi9Xu3xq6++yssvvzztuRu5nf7yX5RTp04t2L7j6SzvdY5RaJNZf0MNQ2dnJyaTiXXr1tHZ2UlMFgkXbcG4sVKT9gdQ0gkY98HVFsqdJpS8clKpFOFweLKvu2jAU5BPbU01X9+8ZtHb435RFvL43i66odDRWQScTucde9pT+73MRnNzM42NjdTV1d3UI2alMxJK8mH3GDaTccZCt0AwgNVmZWRsnGvGMrJFGyf70ANCMkJmqBNpzEdq7CqgYDAYUFJWchw5yJJMYWEhMBnjuHDZxwvjVzCIa+/ul1xh6IZCR2eF4fP52L17N16vl2AwSGNj46owFLKscGEoRMdQBJfNhDhDoZssy4TDYU6d76HfsQGpZNJby45fQbnSCuEhABKxGEajkVQyjWSQNLl3SZLIZDLYbXai8RhOs5Gu9k/g2cfv6nddaeiGQkdnheH1epe8dmmhiaezfNg1TjSdxWOfvai1q6uL4ZSFa+4tKKIRORUje/EEqcFOFEXGarWhKJMqzyhgsVhAALvNhsfjwWazMTw8jCRJ5LoK2FiSi154fWt0Q6Gjo7NkKIpC73iMtmsB7CYjeRbTnNt/1BvkY78dRAFDZIj4p7/AoGQwW8zYrDZyc3Ox2WwMDg1iEA0YjJP1Uel0hrHRMbxeL8888wzBYJCBUT/r3Aa9PfE80A2Fjo7OkvB534gMTqtpRk0ltVZibGycJl+UASZTWIsVP8JIC4rViCiacTldpNIpau+rpcpbxdVrV+no6CAcnuw6abfbEUURQRRYv349ZWsr8XSd4Q92bF/V6cULhW4odHR07ipZSebScJhLQxHsZiNO2+xeRFdXF30DQ7x1NUPAMGkkHKPnEcI91G3eTCxew0D/AAaDgWgsyujoKLFojPyCfO6rvY+hoSEi0QjlZeV48j0oikI4HGbjQ1vZVm7Ra6jmybISVw8Ggxw4cIC2tralHoqOjs4CI8sKveNRfv3ZEN2jUVx2M2bj3LegwfEAb487CBjcCIpMSeAc9SUG8t0eysrL2falL7F//34e3vwwxcXFVFRUEAwGudLbi8Vq0WpXwpEwqWQKo9GIPddFZYEdk0EPTsyXZeVRtLa2EgwGl3oYOjo6C4gkK1zzxzg3ECKdlWddZpqKLMuc/OwSP/cpJBQBg5LF2f8JRbkGIhEjDzz4ANu3fS7zPzQ0pBmF/Px8BgYHSCVT2Ow2ysvKsVgsuN0uNm7aRMnaSjaVOTk7vKhfe1WxrAxFY2MjTU1NSz0MHR2dBSCeztI9GsU3FkOWFXKtRuym+d1y3j7dwc8vxUkrJmyk2WIZJpwjEI1GWbtuLU8+8eS07UtLS+nu7iYvLw+D0cD9m+4nJ8cBwMbHNlG7oRZRFIkkM9SW5GI16e2Kb4dlZSjmy92UJZjKUpbQz4U+Lp3lQjIjMRhM0D0WJZLIYBRFciy3d5v5sHuc1zviyAgU2xSeXWNkfFDB+/Bm7A478VicK1d62bChVnvPM08/gyAIDA4OUl1dzZryNUSikWltjjOSjMVooLZEbx51u9w1QzFfWYL5cDdlCaaylCX0c6GPS2epSGYkQvE0A6EkJ/tT9BmHEAXIMRtx2m6vyZesKPy8bYDfXRgGBNbbM+xYayCdiCMIaLpOdocd/w11JEajkceffIZkVqK7u4erw6M4c2wMDk62OS5b56W7p5u1YpCWeAkNDQ0LdQjuCe6aoVgNlaM6OiudREbizLUAdosRu8mA2SRiFCebfIkC0/SOFAUkRUGWFTKSQjIjEUlmCScyhJMZEhmJrCQjAzajAZMBXHNkMM1FMiPx3z7spb0vCMD/8mAJG+1RAsEgZWWlFBcXMzIyonkUZWWl2nvDyQwGUWCtx05pnpWE7zTFJTbCaYgKdsYnAuSYe8lP9GN3O+nu7tbbm94my2rpqbm5eVrGU11d3RKORkdn9ZFIS1wZj6OgIE1p7iMIoCCAoky5iX7eUlgBRCY7RJqNIiaDiGMGHaY7oS8Q54fv9TASTmEUBb79aAXbb5D8lmUZg0HEHwhQVlZKdXUNGUkmmsry4BonNUW5oMi0tLQw6OskkUiwefNmotEoNRtrGBoawuB2IkkSw8PDnD9/flr/GZ25WVaGYqrOvo6OzuJgNYkYDUufGa8oCh91T/D3n14lIym47Sa+92QVVYU5N20riuK0mEQsncUkinztwVItBnLyZAvd3d3U1tZy9uxZLl++zJNPPkl9fT0tLZOvDQ8P09PTQ1VVFf39/bS2turLo/NgWRkKHR2de4NQIsPfn7pK27UgAA+U5fEvHqsk13rrpatwMkNhjoVHqvKnGbzh4WFN0r2urg5JkjQjoHayPH/+PFVVVVRXV9Pd3c3Q0NDCf7lViG4odHR07hqKotByJcD/+PQa0VQWgyDw3OYyvvZAyYxqsTcSSmRYn29n63r3TXGGkpISuru7cTqdhEIhampqtNfUTpaKotDd3Y3BYCAWi1FaWnrjR+jMgG4odHRWMLt37+bYsWNLPYx50R+I8w8tfVwajgCw1m3jT79cyTqPfV7vD8Yz1Jbm8GC5a8bXVa9haGhIiz/Mtc3atWt1nad5ohsKHZ0Vyo3JH8uViWiK35wf5v2uMRQFjKLAHzxYyh88UDLvWEkgnub+MiebymavgVC9hrmYus2pU6f0QPY80Q2Fjs4KRJW6ud0apLvJUCjBWxdG+KRnAkmZzJ/aut7N7q1rKMixzHs/gViah9Y69UK5JUQ3FDo6K5Dm5mZ27drFoUOH5tzuRhWDgYEBurouL0p/6Gg0xvmOS3T5s5wdzdAfkbTX1jsNPFpuZm1eFv/gFfzz3GcoJbPBYyIoGzl1dWHHu9KUA5ZyvLqh0NFZZtxKxaCtrW3eaeQ3qhg8tuOr1NRsWLD0WEVRGI2k6ByJ8FF/nGuRGBlp0nsQBHi43MXXHiyZMeX1Vvv1x9N8rSKfykLHgoz1RlaacsBSjlc3FDo6y4z5qBg0NzcDk/2zjxw5wt69e+e174FAgqaOETx2M3k2E06biTyrCYtJxCgKM1YsK4pCMjNZ3BZOZhgOJxkJJRkKJekZixJOZqdtX5hj4dHqfL5cVYDHcXsyHjAp5RFMZHikKp91nsUxEjq3h24odHRWGHV1ddTV1WnG4nYYiSR5o21gxtcMwmTVtdojQpIVZEUhnZXJTqnivul9okBFvp1SS4bGLdWUu2x3LJEhyQqhRJonNhRR4rTe0T50Fh7dUOjorFAaGxvp6em5rfcU5VpoqHATSWYJJTKEEhni6clYgqQoJDISiYw043vNBpFcq5HCXAsleVZKnFbWe+xUFDgwGUQuX+5kjXt+qa4zkZFk4uksOzcV43bMP9its/johkJH5x5ijdvOv/hy5bQYhSQrpLISqaxMKiuTzsoIwqSHIQoCJoNAjtWIxbh4qaSJtIQgwlcfKMVxm7LkOouPoCjK7D7lCuH++++nqqpq0T9nYGCA8vLyRf+c20Uf1+3T1tZGf3//Ug/jrlNZcx/F5esWRT11YmKC/Pz8W294A1lZxmQQ8TjM86rOXiiW8/k5E0s13p6eHlB05s3Xv/71pR7CjOjjun2W89hWKivtmOrjnT9LLyGpo6Ojo7Os0Q2Fjo6Ojs6c6IbiNphauLSc0Md1+yznsa1UVtox1cc7f1ZFMFtHR0dHZ/HQPQodHR0dnTnRDYWOjo6OzpzohkJHR0dHZ050Q3ELjh8/zvHjxzlw4MCs2jqq0mcwGNT6BCwGR44cobm5mVdffXVGddH5brPQLKdjdLufuRTHa7UTDAY5cODAsm2qtJJ+82VzLJesgmMF0NTUpBw6dEhRFEXp6elR6urqZtyusbFRqaurU/bv368EAoFFGUtPT4+yf/9+7fGuXbvuaJuFZjkdo9v9zKU4XvcCTU1Nyt69e5XTp08v9VBuYqX95svlWOqiKnPQ2Nio6f77fL5Z++vu27cPr9eL1+vF5XItyliam5unyZTMNBOazzYLzXI6Rrf7mUtxvO4FGhsbaWpqWuphzMhK+82Xy7HUl57myeHDhzlw4MCMr/l8Prxer+bSLgY3Lp3MtJQyn20Wk6U+Rrf7mUt9vHTuPvpvfmfc0x7FrTqJqbz66qu8/PLLs/Yn3r9/v/a+3bt3c/r06QUfq8vluuVJPZ9tFovlcIxu9zOX8nitVOZ7zSxX9N/8zrinDcV8O4k1NjZSV1fH8ePHb3pPc3MzPp+PvXv34vF4Fmuo1NfX89prr2mP6+rq7mibxWC5HKPb/cylOl4rmflcM8sZ/Te/M+5pQ3ErfD4fu3fvxuv1EgwGaWxs1C6UrVu3cuLECRobG2lubqa5uZmmpiaOHj26KGOpq6ujtbVVuwFO/Rx1LHNts1gsp2M0lbk+cymP171Ac3PztCyd5XQzXmm/+XI5lrqEh46Ojo7OnOjBbB0dHR2dOdENhY6Ojo7OnOiGQkdHR0dnTnRDoaOjo6MzJ7qh0NHR0dGZE91QLANU4a8jR45w/Phxjhw5QltbG0eOHFnyce3btw9BEKaNZefOnezcufOuVVjr6OgsLXodxRLj8/nYuXMnp0+fnqZHtHv3bhoaGpZuYExWsR4+fJjXX39dq7oNBoPs3r2bvXv3LunYdO5d2tratPPy0KFD2vOqJtKxY8eAyXP1yJEjeL1efD4fLS0tNDQ00NTUtOD6ST6fT1NPPnbsmKZ/BpOaY+rrU59fUSypJKGOUldXpxw7duym53t6ejRV1qVm//79SmNjo6IoinL48OElHo2OzqSqqtfrven5qcqwe/fuVZqamhRFUZTTp09r29+JEmtTU9Mt3xcIBBRA6enpmfb8crmOvwj60tMSEgwGaWtrm7Ha0uv1ahXOqqjd1H4PbW1tVFVVaY/37dunqWL6fL5pmvtqZae6rKUucc2Xffv2afvSPQmd5Yh6jk/1wv1+v+YJu1wu7e87qW6e6rnMhsvlorGxcdq1deTIEU1zbCWjLz0tIaq42mxiaurzhw4doqenh8bGRtxuN4FAgLq6umlu7KFDh9i6dSswqeL60ksvUVdXp0lrqO666nLv3LmTxsbGeUl+q1LdExMTX+Tr6ugsCsFgkMOHD3P48GFtcqVKdBw+fJiqqiqCwaA2gXrxxRdpbm7G7/cD4PF4pr2vra1NW67av3+/tu1rr71Ga2vrnJOlffv2ceDAAfbv38/x48d58cUXF/8A3A2W2qW5l5nNVZ36+tS/T58+rbhcLu25qa51IBDQXOuenh7F5XIpjY2N2lLR/v37lV27dilNTU1KU1OT0tjYOG8X/PDhw8qxY8emfbaOzlLS1NSkuFwu5fDhw8qhQ4dmbEC0a9cu7Rzv6enRlk9Pnz6t/a0okw2uAoHATY236urqtGtw6r5uBaDs379/yZsNLSS6R7GEuFwu6urqtBnMjTQ3N7Nr1y4OHDjAvn37qKurm5f6qsfjobe3l+bmZg4fPgxMzroaGho0L2S+QbXjx4/T2NiI1+vlwIEDM6rD6ugsBR6PR5vdT83KU/uQzMZrr72Gy+WalrXn8/k0FWSVO5XCb2xspKqqalmJIX5R9BjFEnPs2DEOHDhwk0b+kSNH2LVrF0eOHCEYDGonvs/nIxgM0tzcPE1bf+pJf/DgQVwuF7t27eLYsWP09PSwb9++aZkePp/vlt29jh8/ri1fARw4cICDBw8uwLfW0VlYVIMxn97SUydNage5+d7U1SWsuWhtbV09S07X0T2KJcbr9XL69GkOHjxIVVUVHo8Hv9+vnWjqidzc3EwwGGT//v0cPHhQaxJ0+PBhXC4XLpcLn8/H8ePHyc/Pn5YW+PLLL+Nyudi3bx+vvvqqduOfzTNoa2vj4MGDtLW1TZtV9fT00NbWxoEDB7R96ugsJ9RYxUyoMYmpcQT4PFbY2NjInj17tO1vjCGq75/rvFffs9quDV1mfBURDAZX3Qmqo3Mj6kTm+PHjHD58WJtcNTU10dbWRk9PD83Nzezbt4/GxkYOHTrEwYMHOXLkCEePHmXXrl1ap74bJ02zPa8mg1RVVc2axTR1gnX48OGVWzMxA7qh0NHR0dGZEz1GoaOjo6MzJ7qh0NHR0dGZE91Q6Ojo6OjMiW4odHR0dHTmRDcUOjo6OjpzohsKHR0dHZ050Q2Fjo6Ojs6c6IZCR0dHR2dOdEOho6OjozMnq0Lr6f7779ea9iwm4XCYvLy8Rf+c20Uf1+3T09PDhQsXlnoYd53FvFaW8+89E/p450dPT8/qMBRVVVX88pe/XPTPOXXqFNu2bVv0z7ld9HHdPs8999xSD2FJWMxrZTn/3jOhj3d+PPfcc/rSk46Ojo7O3MzbozAYDEiStJhjWRQkSaKlpYXh4WFKSkpoaGjAYDAs9bB0VjEr9Vq5V9HvEbdm3h7FShWZbWlpobu7G4PBQHd3N62trUs9JJ1Vzkq9Vu5V9HvErZnmUZw4cYKmpiZCoRCKoiAIAvv27WPz5s0IgrBUY/xCDA8P43Q6AXA6nQwNDS3xiHRWA6vxWrlX0e8Rt0YzFH/913/Nzp07eeWVV6Zt8MYbb9DT03PXB7ZQlJSU0N3djdPpJBQKUVNT84X2p7upOsvtWtm9ezfHjh2765+7Wljoe8RqRDMUf/ZnfzbjBi+88MJN/ZxXEg0NDQiCwNDQEDU1NdTX19/xviRJ4sc//jHnz59nzZo1hMNhBEFYUZkTOl+c5XStNDc3z6tPtM7sLOQ9YrUyr2D2Sm6vaTAYFuxG3tHRwfnz58nNzWVwcBBgReVh6ywMV65coaKiYsbX7ua1oholtW2nzp2xkPeI1YpmKP7mb/6Gf/tv/+1SjmXZMzExQXl5OUNDQzgcDvr7+3n88ceXelg6d5kDBw7w2muvLfUwaG5uZteuXRw6dGjWbX72s5/xs5/9THvc3d3NqVOnFmU8gUBg0fa9GOjjnT+aofj0009vmim9/PLLHDx4cCnGtSzJz8/HZDIhCAL9/f089NBDupt6D/L973+fn//85zz//PNLNoa2tjYaGxtvud03v/lNvvnNb2qPn3vuuUWbPesFbIvLUo5XMxSvv/46J06cACZPwv379yMIwl01FMePHwcm09V27tw5rwvhbrJx40YMBgN5eXk8+uijKIrCr371Kz2ofY+xZcsWtmzZsuTGorm5GQCfz8eRI0fYu3fvko3lXuZeSHDRDMWVK1cAqKuro6GhgaamJiorK+/aQJqbm/H5fOzfv5+6ujp2797N6dOnF/xzvsiPqq5l6kFtHZisl/jKV77C7t27CQaDd3Xptq6ujrq6Os1Y6Nw9bryHSJJEb28vTqeT7u7uVXkv0AxFXV0dO3fu5PTp0/h8vrueC97Y2Kh5ED6fb9GWdNTimi/yo7a0tOhB7XuYF198kfb2durq6njllVfw+Xy8/vrrSxLja2xsXNHp6yuRG+8hg4ODbNy4EVi9dRiaoTh69CgvvPACAJWVlZw4cYLm5ma++93v3vSmuYqNFoLDhw8vWoDu/fffRxRFhoeHAbQb/XxQg0nvv/8+kiTR09OD1WplcHAQj8ezZIGm5RqUW67jWgh+//vfax73li1bqKurm3G7xb5WdO4+aoGeJEkMDQ1x/vx54vE4mzdvJhqNrso6DM1QqEZCZceOHXzve9+7yVDMVWzk8/m+8Jrtq6++yssvvzxnyt8XCdApinJTcc1836sGkxRF4fLly4yOjtLf389jjz3GP//n/3zJ1iWXa1BuuY7ri3L06FGtkldlpmXaxb5WdJYGtUBvaGgIn8/HAw88QCKR4PLlyzz55JOrMsFlzjqKAwcO3PTcXMVGoVDoCw2mubmZxsZG6urqOH78OLt27fpC+5uJhSiuUfeRl5fH448/Tn19/aoLXunMzlzLslOvgcW8VnTuPmpsYmBggFQqxdjYGFVVVVRXV2tCkKtxYgRTDMVMTTHUWdJ8T+obZ1m3g8/nY/fu3Xi9XoLBII2NjYtiKOZbXDNT0Pt297FYqGO7du0abW1ttLe3U1VVxT/5J/+EL3/5y5rRmvodCgsLARgbG1u1mRl3i8OHD7Nz586blo/efvvtafGCuQrzvsi1orP4zHT9T41NWCwWamtrsVgsGAyGVS/9oRmK+Z78i1WY5/V6CQQCC77fO2WmoPdiMHWWEggEcLvdlJeXz3kjV8d26tQpPv74YyKRCOPj4/T39yOKIkajkYGBAVpbW4nH46xdu5a2tjYMBgN1dXWzBvElSeLkyZN8+OGHADz22GNs375dmy2pxqm9vR1JktiwYQN/8id/gtlsnvE7vf/++yiKsuqM0p/92Z/xxhtv8Fd/9Vfk5+czMTGBIAi8/PLLPPPMM3zve98DYP/+/bz++utLPFqd20XNavzss8+QZRmDwcCFCxdwuVzTxAPT6TRr1qy5J6Q/pmk9zefkv1cK82ZSlCwtLV3wzzl58iRvvfUWp0+fZnx8nPr6erZt2zbtRn6jMbl8+TL5+fmcO3eO8fFxEokENpuNixcv8rd/+7c88MADXL16lUuXLmm1H36/n7Kysmnf50ZaWlpoamoiHA4Dk0uBRqORbdu2acbp5MmTXLx4EbvdztmzZ+nt7eUv/uIvtPdPNVCyLHPp0iUuXLhAfn7+qvJkXnjhhZviejfy8ssvL3mthc6tmSnd9fz580QiEYaGhigrK+PcuXM8+OCDWCyWO4pvrnSmxSjmc/Ivh8K8u8GdKEreSY3Ghx9+SHd3N2NjYwiCwGeffUZlZeW0ZcCTJ0/S1NREb28v4XCYsrIyurq6iMViJJNJ4vE4V69epbCwkMuXL9PX10coFCIejxMKhdi+fTt9fX3EYjG6urqIx+N4PB4uXLiA0WjUPIfh4WEymQwOhwOAVCqlGRTVcA4PDyOKIqOjo1RWVnL27FlaW1u1JIHh4WHa2tooLy8nmUxy+vRpDAYDO3bsWLU55uFwGL/ff9My03IpzNOZm5nSXdesWYPP5yMvL4+uri6qqqoYHR3lySefZHR0dNV7EDcyZ4xiJpa6MO9uMVPQ+1YNTe60RiMajZKXl0cgECAnJ2eahpQkSfz93/89/f39RCIRstksgUAAm82G1WrFZrPh9/uRZZlMJkM6nSYSiWAymYjH4xgMBi5dukRhYSGyLHP58mXy8vIYGhri8uXLFBcXa2MtLCxkZGSE0dFR7HY7tbW1mhelGs6SkhI6OzspLi7WDKhqTJxOJxcvXqSsrIzx8XEKCgro7e3lqaee0l5fTTnmR48epa2tjZ07d+L3+2dMJYelLczTuTU3rh4MDAxQVFTE+vXr+fTTT3E6nbhcLtLpNAaDgW984xtLO+AlQDMUBw8e5ODBg5w5c2bOHO+lLsy7W9xu0HtgYIDm5mZisRiiKLJx40ZsNtst9/Hoo4/y0UcfEQgESCaTJJNJrFYriUSCgwcP0tLSQmdnJ0ajkVAoRCwWIycnB0VRMJvNiKKIyWTCZDIRjUaJRCLk5OSQyWTIZDJks1nKysqora3lypUrbNy4kY6ODsLhsGZIrl69SjKZ5PHHH6e4uJjBwUH6+/upqKhgy5YtwOeG02w2E41GGRsbw+v1snXrVkpLSzWPwuPxMDExQUVFBaOjozz88MMUFRUBrLqAn8fj4ZVXXpk1DrGcCvN0pqMuLw0NDTE+Po7ZbMbtdhMKhXj88ccxGAw4HA4kSSISiRAMBiktLaW/v3/VecTzQTMUXq+XF198EUEQeOmll6irq5sxY+N2CvNWOjdmDV28eJGhoaFpy0qqFzE8PMzFixdJp9MUFxdz6tQpLdNoLkRR5OGHH2Z8fJxwOExpaSmiKPK3f/u3+P1+rl69SiaTQZIkDAYDyWQSURSJRqNks1lEUUQURQRBQJZlBEEglUpp71ELAjs7O1m/fj0nT57UYhaKomhGyGazkUqliMVi5ObmUlhYyPnz5/m7v/s7vv3tb2uGc9u2bbzwwgu0trZqcRvVBRcEAZvNhsfj0ZbsvvWtb9He3r6qAn4vvvgizz77LMFgEEEQ2LNnz6zbzrcwT+fu0tLSQn9/P0VFRRiNRjo7O5EkSTtHzWaztorQ09NDWVkZPp9v1mt6tes9aYZiz5497NmzRysmOnbsGC0tLVRVVU0TG5tvYd5qYOpSUlNTE/39/fzRH/3RtGUl1W29ePEiBQUFXL16ldHRUdxuNzk5Obf8jLGxMfLy8vB4PNjtdq5evcrg4CCZTIZUKkU2myUcDiOKIpIkAZMVz2o2hiAIZLNZTCYTVqsVl8tFPB4nGAxiNpsxGo1IksTY2Bjf/OY3uXDhAiMjI1itVhRFIZFIUFRUpL3H7/fT399PIpFgw4YNnD17llOnTqEoyozZUFNRDYnKqVOnMJvNq24G9vLLLwNoEyRBEGhoaLjJU5hvYZ7O3UWSJJqbm3nrrbd4++23EUURm83G008/jclkor29XUvgiMfjlJeXMzY2hs1m49KlS5w8efImQ7AQ0kDLmZsK7tTZ0Y4dO4BJCYK55DRg5sK81cDUtUt1GQemr7Wra/cej4fx8XFMJhNFRUXarH8uJElifHycd999V1sOcjgcRCIR7SRUvQN1e0VRtPcbjUbMZjOpVAqz2UxJSQmDg4Pa8pfVatXyvGOxGL/97W8RRRFFUSguLmZgYECLZaxdu5Z4PE4ymWRiYoKCggKy2SyKovDBBx+QSCRuyobasmULP/3pT+nu7qa6unrGVNm5vvtKnYGpy3Hq/wC9vb03bTfXsux8Y4I6C09LSwunTp3i2rVrCIJAMBikvLwcp9PJli1btN9lYGAAWZYZHx9HlmWCwSBFRUUzGoLV3nf7lh3u6uvrqays5OjRo8DchXmr7eSfmvlkMpkwGAxcvHiR9vZ28vPzKSws1NbubTYbmzZt0m7SmzZtumVRVUtLC0ajkeHhYUZHRzEYDIyNjQGTS1KAtj9BEKbdeBRFIZPJYDQacTqdbNy4kb6+PiKRCIqikEqlSKfTCIJATk4ORqORd999l4qKCsxmM9euXSOZTGKz2RBFkZGREUpLS9mwYQOXL1/W6jF8Ph+yLJOXl4fNZmNwcJArV64wMjLCT37yE4aHh9m0aROnT59GFEW+853vzOvYrrYZ2Eyewmy1SSdOnKC3t3dVeuErgeHhYS3hY3x8nFQqxfDwMNFolLa2Np588klg0nNPp9MUFBTQ1tZGdXU1kiTR0dHBwMCAtoyqxhFjsRgOh4OhoSEefPBBbbl4NSDeagOn0zlNd+nw4cOcOXPmpu1OnDix6oqLGhoaqKmpQZIknn32WYqKimhra0MQBEwmE83NzbS3t1NfX095eTkVFRV4vV6+8Y1vUFZWxpo1a+bc//DwMOPj4wiCQGlpqbaEpGYwZTIZbXnJYDCgKAoGg4GcnBwtgJ2bm4vNZqOsrIy+vj7S6TSSJGE0GrVlJ0mSNI/gs88+Y2xsjGg0iiAIiKJIKpXSYhY9PT2aK55Op/F6veTn5zM+Po7P52NoaIhIJMLw8DCXLl1ClmWGhoYoKCjg8uXL8z62q2UG9vOf/3zW1/7sz/6Mnp4eXnzxRf7lv/yXvPjii7z00kvk5+frRmIJKSkpwWg0kkgkkGUZURRJp9NcunSJdDpNf38/J0+eJDc3F7PZTDgcpqamBkEQGBkZIZFIkEwmaW1t1SY8tbW1DA4O0t7ezpo1azCZTLfMklxJzOhRzJX3PVdhnrpctZKZa0nkww8/pKqqSnucTCZ59913effdd0kkEjz44IOcP3+ejo4OioqK6O/vn7UyWZIkRkdH+cUvfkE0GkWWZVwuF36/n2w2iyAImmFQl5tMJhOiKCLLMh6Ph/Xr1zMyMkI0GuWtt97SPAh1icxkMuFwOAiHw9oSViaTIZlMavtU4xQGg4GzZ8+Sn5+P1WpleHiYtWvX8tRTT2EwGOjo6NC+1/j4OKIokkwmsVgsWmX4VJmTW3EndSrLEb/fP+fr86lN0rm7NDQ0sGvXLs6dO6epDlitVvx+P6lUip6eHj744AMCgQBWq5Xa2lottme327n//vvZsGHDtLRwgKKiIoqLi6mtrQVYsZOfmZjRUNzLJ/9cSyL5+fnE43FtrT4SiWC1Wkmn0yQSCXp7e6mrq+PixYtYLBbMZvOsyypq1oXD4dAK5yRJwm63k81micVimrEwGo0UFBQgSRKxWAxZlnG73VqKq6IoxONxzYNQUYPpZrNZy4iamk5rNBqJRqPaMpcsy1y7dk0LgqvyIF/60pf46le/ytNPP83vfvc70uk0wWCQtWvXkkqlsNlsNDQ08K1vfWvGYzqT8V0IcUYdnTvBYDDwne98h5/85Cd0dHRgsViQJIm8vDxCoRDt7e1UV1fT1dVFZWUl7e3thMNhXC4XVqtViz/W1NRMU6M2m83a8vBKnvzMxC1jFLPxN3/zN0xMTLBz507q6+tXTWxiriWRjRs3snHjRi37p7CwkI0bN9LV1UUikcDv9xMKhRAE4ZbLKmoVdFlZGRMTE9hsNtauXUsikaCrqwuz2YyiKFoMQfUOjEYjdrudZDJJKBTC5XJRUlLC5cuXtcI9dQxq5tXAwIC2XKbGOpxOJ6lUCkVRyGaz5OTkaN6M0WgkHo+TyWS4dOkS5eXlwORM7P3336eyspJoNIrD4cDj8fBv/s2/mXMtVq0sz2Qy2tLao48+uqJjErfDX//1X+N2u/nud7+7oFXay7118HKmpaWFdDqtTZYcDgcmk4mJiQnMZjPFxcV4vV5GR0cxmUzk5+djs9kwm81aVmN/fz8lJSXads8++yzAqqzcvmND4XQ6eeGFF2hubmb//v1UVVXx8ssvr/iGLHMtiai1BI8++igweQPs7u6mpqaGeDyOzWajpqaGyspKenp6GBkZYWBgYMbAllqn0N/fTyAQmFZJra6fGo1GDAYD+fn5CILA4OAgBoOBdDqN3W7XZjCCIFBRUaGd5Ha7nTVr1mhGLBqNUlpaSnd3N+FwWMuEqqmpIRaLaYFtmAyiT136stvt2jKZwWDgiSeemHZ8vF7vLbOXPvzwQy2jKxwO8+GHH2rH8F6gsbERl8vFG2+8QVNT04IYirvVOng1oqodDA4OIggCVquVWCyGyWSioKCAaDSKJEls3bqV1tZWIpEIqVQKi8WCz+cjPz+fVCrF2rVricVi1NbWrvpq7Ts2FMFgkMrKSvbs2UN9fT1btmzhRz/60YowFHPFIW5nSWTqtl/96le1vhSSJHHp0iUGBgamBbZunEHbbDZte6fTiSAI+P1+LbU1m82STCY1fSVJkhAEgWg0SjgcZt26dXi9Xnw+Hw0NDaxfvx5ZlhkcHNQK+KxWK42NjVp1qclkorq6mkwmQ05OjhZwHxsb07KlFEXRlqpMJpNWa3Hy5EkGBgZIJBL09fVhMBiIx+NYrVbcbveqyF66Xdxu96yvvfjii3zpS1+irq6OQCBAIBDghz/84YJ87t1qHbwaaWlpYWJiAkDLDpQkCVEUqayspLCwUEvmUBNBZFkmEAiQm5vLpUuXyM3NZXR0dFo67WpmRkMx18mvsmvXLp599lncbjcNDQ1s2bJlWo7/cmauOMRs0h1TS/6nGpeZtlW9gKnB/RuXn8bGxtiyZQuXLl3SsilU+Yup6bCJREKTElArs1WjpigKAwMDbNy4kdraWkwmE4qisG7dOgYHB0mlUly8eBG/309JSQlOp5OysjIaGhro6emhv7+frVu3Mjg4qMUsBgYGGBsbw263U1lZic1mw+v18t5775FKpdi8ebO2lFVXV8eJEydYs2aNtsz17rvvasdIlmVgskCvubmZVCqF0+nkscceW7DfcqmZK1b38ssv4/f7OX36NE1NTfj9/kXRepqrdfAXaRt8u6yU1rfvvvsumUyGaDRKIpFAURTNo+/v70eWZex2O6dPn9YSQdRWp+qS7NDQENlsVtM0uxvfeymP74yGYj6B6srKSn7/+99Pe87j8SzMqBaZW6Vmzta0RC35n6ufg/q+8fFxjEaj1idi6vJTOp3m1KlT/OM//qPm/oqiyPj4OJIkkc1mSafT2tKPKq+hegGqwYhEIiSTSdra2hgdHcXr9ZJOp7V4Q2dnJ3a7nZqaGgwGA48//jgWi4VsNkttbS3r16+np6dHEyS0WCzk5eWxfv16BEGgrKyM6upqamtr6ezs1No9XrlyhWg0it1u1/RvamtrOXPmjJbK293dTTqd5tFHH2X79u0YjcabJD9WO2pB3o4dO7RudzMV5n0RbtU6+Iu0Db5dVkrr23PnziHLMmazWbvOJEkik8nQ09NDbm4uACMjI2QyGXJzc8lkMgSDQUpKSpiYmNBWAmpqau5aG+SlPL6aoZirG9d8WSmZULdKzZzJ4xgeHtZm9mpDdTX1VX3Pe++9RyKR4KGHHuLatWtcvHgRo9FIXV3dtOUnNdsiGAySSqW0FNhsNqupVMqyjM1m0/6p66aJRAJBELBYLNo24XCY0dFRYrEYZrOZq1evYrVasVqtmuDfn/zJn7Bu3Tqi0ShdXV14vV68Xi/vvvsup06dIh6PE41GWbduHVarlfLycjZt2qSlA4fDYa5evUpHRwdms5lEIsEHH3xAYWEh27dv1zK2NmzYAEwa4I6ODmDpOwIuNF+keddCSnjcjdbBqxG32601IVIzCaemlZ87d06rHUqlUkxMTHDfffchCIK21KqWBiiKQktLC3V1dbS1ta1IpYH5oBXcrVYZjpmYWkg3UxxiJo+jsLCQS5cu8Ytf/IJPPvkEl8vF7373O/7zf/7P/PjHP+by5cuEQiHC4TDvvPOOVhshCAIXL15kZGSE/v5+AD744AMALRVWTY1V02LVbIyysjKefvppbDYbTqeT/Px8HA4Hsixrchv5+fnY7XbS6TSxWAxAK9RTjUswGOTjjz/m2LFj9PX1UVdXxyeffMJ//I//kXPnzhGJRCgtLdUyrcrLy3nmmWd48sknKSkp4de//jW9vb0MDw/j8/mIRCK4XC6i0ahWdFdYWIiiKJw5cwZJkgiFQhQUFNzFX/XuoTbvmoqq/3S3UFsH79mzh6qqKpqamu7q569kSkpKEEVRE85UswHViWAoFKK/v59MJkM8HiedTpNKpdixYwdVVVU89NBD0zzw7u5u/u7v/o7Lly/T3d3NP/zDP/CTn/xkWqr6SkfzKL7//e/fMw1WbjXDncnjUDOBYrEYVquVvr4+LBYLmUxGa3Ti8XgYHBxkaGiIdevWIYqiFgBTsyUAcnNzGRsb0zwHNbYjyzLpdBqTyUReXh41NTWkUinKysrweDwMDAxw6dIlTCYTNpsNmMzXfuihh7h06RKKomCxWCgsLCQSiWgnvizLtLe3azEL1bXOZDJ89tlnwGQWVnFxsSZFUltbS319PSdPniQajWoZWdXV1QwMDGhZVxUVFbz99tsMDw/z4IMP8o//+I+cPHmSqqqqVSdjoLIcmnctt9bBKw3Vg1czC9XCU7VuKR6PY7fbtYSOSCRCV1cXhYWFZLNZCgoKiEQixONxnE4nH3zwAfF4nL6+PjweD2fOnOEnP/nJqunsqHkUW7Zs4fnnn59TkuBeYSaPY2xsjNraWrZt20ZJSQnDw8MAWo/r/v5+qqurcTqdFBcX43Q6Wb9+PRMTE3R0dDA+Po7dbgcms2FisRihUAiz2YzL5dLSUU0mE6Wlpaxdu1a7aT/22GNcuHCB3t5eUqkURqNRy+3esGEDDQ0NfP3rX+eBBx6gsrKSP/7jP2bDhg2YzWbWrVuH3W7XCuv6+vq4cOGCpnczVeZjaGiIiYkJXC4XW7Zs0bSntmzZonksPT09uN1urUWkqgGVTqfp6ekhFosRi8Xo6+vjt7/97YoIbt4uU5t3HT58mKamJrq6upZ2UDrzRs3wKykpYcOGDeTl5ZFIJLQMKLVBUTKZpKCgAJPJRE5OjibBX1VVhcPh0BI4QqEQRqORrq4uTdVgZGSEs2fP0t3dzc9+9jP+9//9f+eNN97g5MmTK9LTuCmYvRq7cd2uUulMHkdJSQlnz57l4YcfJh6Pa1lE1dXVhMNhzVv46le/ypYtW2hvb+fQoUMoisKmTZuYmJjg/PnzwKS0RllZGQaDgZ6eHq3CWnWB1VoIURRxOBx0d3djtVrJycnR1kVdLhc5OTkUFRXx9NNP89BDD/GXf/mXWhbVn//5n3Pt2jVGR0f5L//lv5CTk6PNmCRJIpVKAeByuTSvRlWV/T//z/+TH//4x/zpn/4pGzdu1JbDQqEQbrdbC3jn5eWxZs0a8vLyMBgMBINBBgYGADQBwblqJm7s9wGTF/Fyn4HdK827VislJSXEYjGi0SjBYFBLlVUna+qNPDc3F1mWycnJwWq1snnzZq5evYrf78disWC1WgkGg/T19Wn7uHTpEh6Ph3A4TGFhIYODg4TDYe15Nca40mJ2mqFYzd24FkKptKGhgXPnztHZ2YnNZuOll17CaDQyOjqqLdOoNzZVDlyWZa1Zu81m4/z585w8eZLh4WEaGhq0E2x0dBSbzYaiKKTTaXp7eykoKCAnJ4euri6tZanH4yEWixGJRLBYLNoSUnNzM3/+53/O8PCwpp3f3d3Ngw8+SF5eHk899ZTmGiuKgsfj0bSaqqqqEARBO5H7+vq0i+cHP/gBX/rSlzAajZSXl1NUVITNZsPhcPC1r32NUCjEhx9+iMPh4OGHH9bWbR0Oh6ZMO9/fpampSUu5Xe71GPdS867VSENDAxaLhbGxMRKJhPb8VP0zmLyO165dSygUYmhoiJ/85CeaGoHVagUml4s//fRT/H4/Ho8Hi8WC3+/HbDbT2dnJtm3b6O7upry8HL/fT21t7YrUgJrmUazWblwLoVRqMBgQRVHTrb927Ro1NTUzVmSqN8C1a9dy8eJFABKJBJs2baK7u5tUKoXH4+E3v/kNw8PDGI1GrfeDWmy3bt06zGYzkiRpldShUIhMJoPFYiEej3PmzBlGRkY0uWS3283w8LB2o3/ggQf4+te/jsvl4vLly9o6bG5uLgUFBdTU1GjLT263m0uXLjE4OEg2m8VisRAOh7l8+TJPPvkk6XRaM0xOp5NoNMrg4KAWsB4ZGWHnzp0UFBTQ3NyMzWajoKBgzpqJG/t9qBfocleTvZead61W1NRXNdPpRnJzc7V+L6qHoHY0VONzp0+fJhKJaNXdwWAQj8dDTk4OzzzzDKdPn6anp4f169eTTqfxeDwrVgNKMxS36sa1UorpZmKhlEonJibYuHEjMPfNTL0BPv744wwPD3P27FnWrFmDxWJheHiYdevW0dbWRl9fH3l5eaTTaa2tqSp73N7eTk1NDaIo8vWvf52Ojg7Onz+vpfT19/cjSRLJZJJr165pa6NqxejWrVu1mXkwGOTFF19kYGCAzz77jEgkwpYtW3jqqad44403CAaD2vJSMpnUDFc2m8Vut7N582Yt7fbs2bO4XC7WrVuH2+3G4XAAkxWuo6OjfOc73+Ghhx5iaGgIv98/76QBddkN5hZUW64Nj6ZmDa7ka+VeoKWlhUAgoE3OZiKbzVJZWUk6naaiooKLFy9it9vx+/1ajCMSiWidJqfqmLlcLvr6+jTtp6qqKkKhEE6nkzVr1qzIOiLNUNyqyY5aZbsSWSil0vz8fO0HDwQCZDIZ3nzzzZtuWOoNcHh4mIKCAh5++GH8fj9+v59YLKZlR5WVlZFIJLSZipqeGgwGSSaTpFIpCgoKGBwcpKKigvz8fAYHB7V+2WrATZZlksmktuy0fv16nnrqKUwmk1YlHQ6HkSSJDz/8kGAwqGlAqcVzaq1GYWGhZlyCwSCSJNHa2ko6naatrU0b27Vr1/B4PNqxdTqdlJaWTovvnDp1as6b+NTfZb6Casu14dHUSdVKvlbuBYaHh7WY4mwkEgkGBwfZuHEjly9fJpFIaOmyqoFRJ3cqFotFU5hWu+Ft3LhxWZyfX5Q71npaSSxUwdfGjRsxGAwMDQ1pMwi1CnnqDauhoQFZlvn973+vLSWtWbMGn89HRUUFIyMj2hqpajwMBoNWyGY0GrWbjdvtpq2tTVN7VXthi6KoVXmbzWasVit5eXmUlJSwadMmTCaTNjOvr69HEAROnjypVXoPDw/T0dExTRpEEATy8/MpKyvTmhAZjUbOnj1LLBZjaGiIZDJJMBikuLhY67K3bt06HnvssVsa4Jm8gdv9XVZLwyOdpaOkpASHwzGn55dIJBgaGsJsNpOTk6OJZarKzbIsI8uypr+mTvRgMlllbGyMTz/9VMueXA5e7xfhnjAUC4l6co2Pj2sNSm68YanxDK/XSzgcZmhoSJOvcDqdfPTRR5w7d47x8XEt6K16Erm5udoSVDqdxuFwcOHCBSwWiyY3LssyRUVFWh69y+XigQce4KGHHtIaGX300UdUVVWRTqd58803CQQCmvqsKIpEIhGi0agm+ieKIjk5OTgcDqxWK8XFxVRWVmIwGLh06RJ+v39a9Wo4HObpp59mx44d81bOXAhvYLU0PNJZPCRJ4uTJk1o7gMcee4zt27drN+u6ujr6+/tnjU/A53UWQ0NDVFVVkclkcDgcWp2T2tdezZQqKCjA4XBw5coVrZbpypUr9PT0YDQaV7xXoRuKKdxq/VttcuJ0OkkkEpw9e5a6ujpNbvujjz7STk5ZlnnwwQfp7e3FbDYTDAZZv349ly9f5vz58wwPD2v1C6pnYjabiUajmodgt9u1Pry9vb1aap1qSEpKSti+fTsDAwOUl5fT19fHyMiI1uf3Jz/5CQBlZWW4XC5tKQome3GrFd1q9fbu3btZv369lhMeDocJhUJIkqRJF6hd8srLy9m6dSulpaXzPr4L4Q3oDY905iKdTvMf/sN/4Ne//jXBYBCDwcCxY8f47ne/y549ezAYDLS0tHD16lXtJj8bavLHtWvXkCQJt9tNcXEx4+Pj2Gw2rFYrRqORVCpFNptlZGREe4/T6SQSiTA6Oroq1GV1QzEFtcGOupyjNthRmZiYYMOGDVy6dIlUKkU4HCadTmuV201NTVr3O/W1hx9+mGg0itfrxWq10tvbq+Vcq/namUxGC2KrlaKqmquqHfX+++9rzd4dDgeZTEZLl/X7/bzzzjtUVVUxODhIJpPh8uXLRCIRbRu73U5xcTFut5tQKITdbicSiWjfNS8vD0mSaGxspL6+no8//pi/+qu/YmxsDI/Hg8fjwefzsWXLFk0scMOGDWSz2RnjNDNxu97AbIZ7pc/OdBaP//7f/zuvvfYaAwMDmvBfPB7nBz/4ASaTiW9/+9u8//77c3oTU5FlmVAopHkPmUwGt9tNQUEBfX19moCn2vxITbcdHR2lsrKSvr4+Hn/8cWD5JmLMh2VlKI4cOYLX66WtrY1du3bNqoi5WNyqwU5+fj5nzpzRjEFRURFr1qxh27ZtvPnmm2QyGWw2G/39/Vy5coWBgQEtoL1mzRrOnj2racjcWJ2p1l6ongTAwMAAiqJoQeZAIKDFGGCyQnhwcBBA8xYmJiaIRCLaCTs1AOd2uzUp8tLSUkKhEKIoYrfb8Xg8DA0NafUgJpOJhoYG3n77bS2Av3PnTnJzc/njP/5j6uvrb3sp6Xa9geUauNZZXqTTaX7605/S3d3Nr371Ky0bSRXRVBND/sf/+B9s2LCBvr4+TWF5PqgefCwWI5vNYrVaNW8kGo1qCrRqmqyq1dbX14eiKFy9elUr5Ovt7V2R5/NtG4pwOLworpTP56Onp4e9e/fS2NjI7t27OXbs2IJ/zhdh48aNTExMaDnR1dXV2vJJSUkJJpOJS5cuaRIPZrOZjz76CLvdTk5ODk6nUxMYm8rUntXq0g6gpeCNjIxQUVFBeXm51pBI7fNrsViYmJjA4XBw9epVrZmQOmNSFIWJiQktppHJZLSqVLVeQlEUrRGSiqqWqy5PqZ5FXV2dZkxmWkqaOmuamJiYVoh4u97ASg9cL9a1cq+jnmMDAwMEAgFaW1sZHR1l48aNhMNhTRRTnYyp11F7ezs//elPqaiowGic/61v6qRObSQGk9dtOp3W0roFQdA8c5vNht1u58qVK/zgBz+gqqoKr9fLgw8+CKy88/mmo/X222/P+YZjx47xgx/8YMEH0tzcTFVVlfbY5/Mt+GfcCrXBTjqdnrHBzkytQNXlEzXTyefzaSdJOBwmm81y7do18vLyGBkZ0byRqciyjMlk0m7KquFQA9fpdJrLly9rsh5r165lYGAAh8NBLBbT3GOr1aplS5lMJq2gSE3bs1qt+P1+FEXR6iZU8TOYvAhOnTrFo48+SklJCe+88w5lZWWMjIwAYLVaMZlMnDp1ClEUtR4VDz30EJ999hl2u50f//jHmM1mrafwTJ395styD1wv1bVyr6N6mkNDQ/h8Pjo7O3E6nVpHyYsXL864pBMIBDh27Bjf+c53tGyl2615kWV5WjW32n/FZDKRTCZpaGggHA5TVlbGp59+isViIZ1OMzAwgN/vJ51OaxmTakr4SuAmQ7F371527tw56wFcrL68wWAQl8s17fFsLFbXLkEQWLt2rda1Cpi230AgoBXIdXR0UFBQgCRJnDp1CkmS6OjoIDc3V3NrBwYGtO5Y6k15NlRXOTc3F0EQtBM5EoloM5RIJAJATk4Osixrx0jN51alygVB0AyGqoyp9qvIZDIkEgnN4xBFEVEUMZvN9Pf389Of/pSOjg5GR0fp6upifHxcC3irS2mnTp2iqKgIi8XC6dOneeutt7TltV/+8peUlZVRX1+Poii89957d/RbqB0FP/vsMxRFYcuWLdqxXi4s1bVyr6N6muo1aLfbSaVSXLt2DYvFQnV1NaOjo1o/iam/TyKR4L333sNqtS5IYaSatWi1WnG5XDgcDq5du6Ylq6jejMFgIJvN0t/fr93rVpI44E2G4vDhw9NaeE7lxIkT/NN/+k8XZSAul2tO4zCVxezaNZOAnerqfvbZZzzxxBMzdrQ6efIkFouFZ555hn/8x3/E5/NN6zOhFuzMhNqEyGw2Y7FYcLvdFBUVce3aNZLJJIB2M1eXfFRPQM24UIv11DaOamW1Kllut9t56qmnCAQCnD59WqvvUD0cs9lMNpvl7NmzlJSUsHnzZgYHB7VOemoDpUQiwfr169m4cSOdnZ3U1NRgsVi0zK4NGzYwMDCgtXN98skn7+i3UWs+vvKVr2jexHJbz12qa+VeYqYAsOppulwufD4fjz/+OD09PZommtvtZtOmTVpb3qk3ZFmWuXr16rR+9V8UWZa1VPPW1lYt9qEuSRUWFhKPx7U2Ber/H374oRboXu6INz4x24kPk8srv/3tbxdlIPX19Vo2ELCsdKZUV1cURbq7u2ltbb1pG3WW09vbi8vlora2FofDocUj5qrWVWc9iURCC4yFw2GMRiOZTEYrtFMURZNCVtNp7XY7BoNBq8Ww2+3k5uaSn59PQUGBJi5YUVFBUVERxcXFPPDAA9qMymAwYLFYNHda1Xjq7u4GwO/388ADD2C321EUBZvNxpe//GVCoZB2QZSWlmoxD6fTSW1tLePj46xdu/aO01dXQnxiqa6Vewn12lMLW1tbW7U2AF6vV/v7u9/9Lv/sn/0z8vPzcblcXLx4keLiYk2fbCrpdJpwOLygFfRq97u+vj4tmSSVShEKhRgbGyMvLw+TyUQgEND61KhKyyuBmwzFVE6cOIHH4yE/P5/q6mrcbveiDaSuro6qqiqam5s5cuQIR48eXbTPul2m3rRycnJ49913efPNN6dpy5eUlEy7eZaXl2OxWKZJh8+GWsimbpNKpbScbIPBoMUuEokEJpMJj8eDw+HQFGcdDgd5eXnk5eXhcrlwu9089NBDfPnLX6ayshKXy8X999/P1atXGR8fZ2BgQFtbLSkpobCwkNLSUtxuN/fffz+KonD+/Hl8Ph8ej4dkMsnGjRuprq7mySefZPv27dTU1OB0OnE6nTz99NMUFxdroon19fU899xzKIrCr371q1tq8KsFUlOPqXo8YVL76XbqNZaCu3mtqBw5coTm5mZeffXVJYnp3Q1unDD09/fT0tKiSdPU1tZqy6sul4uqqiqKi4u16uv8/HxycnK0/WWzWa3YdL5LT+q1MlWuYypq6mw8HteWm7LZrDbJU5NE/H4/0WiUsbExXC4Xa9as+YJH5+4xZ+i/ubkZv99Pe3u71ij+jTfeWLTB7N27d9H2/UVQXV2AM2fOaCfO1BQ3NfVzYGAAi8XC/fffT1tbGz6fTwtWz4TNZtO60EmShMPh0NJh1cbukUiEoqIizWOAybXRQCBASUkJ69atY3R0FEEQKCgo0Go4nnzySQoKCvjNb37DZ599htls1prDOxwOLQ1YXRKrqKjg6aef5q233tKWttLpNFarldzcXB5++GEti2nbtm3U19fT2trK0NAQf/qnfwpM5o+XlpZq67FFRUW3TAWcKQ12pRXW3e1rZSVkCS4E6rWXk5PDmTNnGBsbo7CwkM2bN9PU1ISiKOTm5mpp3C6Xi7Vr17J161Y6Ozu1nhPRaHTafm/Hm1BTW1U1Z3VJSY3zqfUVUw2P+h6Xy0U8HsftdjM2NqZt63A4eOKJJxbgCN0d5jQUjY2NwGTbxStXrlBRUXFXZkrLDfWmNTg4iN1uZ8OGDcD0JRH15rllyxb+7u/+jl/+8pc4nU5NWnhqgc/U+IbRaNROvLy8PK3WQS3Oy2Qy2O12rfPd1772NY4ePUoqldI64125coW1a9cSi8VwuVw88sgjTExM8H//3/83siyzfv16Ta/mzJkzZLNZJiYmMBqNhMNhbDYb1dXVFBQU4PP5qKmpQVGUaWq0Dz/88E2xGYPBoNVTqGvIX//61zEYDLz55ptaK9bh4WHOnz+PoigzFhnNtMy00grr7va1shyyBBeTqSmwqVSKvr4+BEEgNzdXk7+/cuUKV69epaSkBIvFwtDQEE6nk76+Pmw2Gx6Ph3Q6jcvlmrasfaek0+kZn5+reE9VU1B7zqjZiGazeUWd33MaCp/Px/e+9z26uro4ePCgtrTyzDPP3K3xLQum3rQURZkzZbOtrQ2LxaKJABYVFeFyuejt7Z3mwgqCQGVlJeFwWGvyoxbsPPTQQ9qyi+oiO51OvvrVr3LmzBmtDWM0GmV8fBxZlqmsrCQ/P5+hoSFyc3Pp6OjQGiul02ny8vKwWq0kEglsNhuxWExTrc3NzSWdTjMyMkJxcTHV1dWaEOD4+DiVlZU4nc4ZUw5nK4pTOwJ2d3fT09NDVVXVrJ7Fck+DnQ93+1qZb5bgYmUIzkQgEFiwfZ8/f57+/n4tBXxkZASDwYDP5yOTydDR0UE8HicUCmnp4WvXriUcDpObm8uVK1eoq6tjYGBAK0pdKNQCvPkgSRI+nw9JksjPzyeVSiFJEqOjozPGOudiIY/v7TKnodizZw979uwB4JVXXuHo0aPazOleQp3dvP/++3z5y1/G6/XOKoetzo49Ho/WmF2t6FQzl9Qb/xNPPEE2m+XMmTP09vYiyzIDAwM4nU42bdpEXV0doigyOjrKhg0bGBsbIxQK4XA4mJiY0HK3LRYLV65cwWQyactOfX19wOfpsYB2kppMJk1g0GKxaEtmatN4s9lMOBzWOuapbVCnHgvVg1DHC5/Hb4aGhigsLKSsrEwzEtXV1Zry7o2stGWmmbjb18p8swQXM0PwRk6dOrVg+x4aGqKoqEh7/Nvf/pZsNsvmzZvp6OggkUjwh3/4h1y9elXzIFTV5Y6ODsrKygB45JFHuHjxIufOnZvVI7hdbDabprasehNqD5cbUdsZqymyubm5SJJEUVHRbR+rhTy+t8ucwewb2bNnD+3t7Ys1lmXL1Kyn3t5eYHIWPDQ0REtLy7RArRqEVbWQYFJGIy8vD4fDgcFgIC8vjz/6oz/i2WefJZvNaqX+oigSj8fx+Xw4nU6qqqqIRqNaTUdpaSler1fzSNRm76lUisHBQa5evUoikeDKlSuEQiGt+1Y2myWVShEIBDSvIhaLaTELVYrAZrMxNjbGyMgIhYWFbNiwQcu+6u/v59ChQ/yrf/Wv+M1vfgNMzk6DwaBmRM6cOUMymcRgMNDb24vBYOAb3/gGJSUlWoe+mYLSqsf2jW98g23btq0Y/Zu5WOxrZTlnCS4ENyYzrF27Vltqe/TRR3niiScoKyvjmWee0QyKKrI5Pj6Oz+fj97//PZ9++ikul0u7hhaCbDaLzWabFqOYbflJkiRCoRBGoxFRFCktLaWiooL169cv2HjuBnN6FNXV1Tc1jvf5fFrh2b3C8PAwOTk5XL16lUAgwMcff8z27dtxu91cvnyZCxcu4HK5tOOSSqWw2Wz8i3/xL3jvvfd466238Pv9WibE+vXr+ZM/+ROtbzV8Xp1tNpvxer189atf5d1330VRFBRF4c0338Rms1FeXs6mTZvo6uoiFAohCIK2DhqPxzVxQVEUyWQyBAIBXC4XXq8XWZbp7+8nHo9rS12xWAyTycTatWsBGBsbIxwOa8HsjRs3cuHCBfr6+qiurqa/vx+/308gECA3N5fc3FyefPJJhoaGGBsbIzc3l87OTqqrq7l8+bJWsf7BBx8gyzLxeJz+/n7Ky8tXlCjarbjb10pdXR2tra00Nzfj8/mWVZbgQnCjl1lZWUlvby+1tbUEAgGSyST9/f18+umnGI1GioqK8Hg8dHZ2sn79eiKRCAMDA9hsNnJzc6mvr+c3v/nNvMUAZ0Ot5lalO9TrczbS6bQmTZ7JZPB6vdjt9hUVyIZ5BLMPHTqkPW5ubsblct1TRkKdoahZLZs2bcLv9zM6Oorb7WZkZEST+fb5fFRVVVFQUEBLS4vWjW7dunUUFRUxNDSE1WrlpZde0lzIxsZGurq6uHLlCrIsk5ubyx/90R+xbds2+vv7effdd2lvb0dRFLq6usjNzSWZTGK32wkGg5r6q1p/oepFqfURTqeTWCzG6Oio1mzI4/Fgt9tJJBJEo1GeeOIJxsfHgcnZ2yOPPKItLU1MTJBMJvH7/eTn5+N2u/H5fESjUXJzc1EUhaKiIu677z4KCwsJh8MMDg4Si8VYu3bttF7jQ0NDtLe3U1VVpRm3lRTQm4uluFaWa5bgQnBjMoMkSRiNRq1pmCrxrcryDwwMcOnSJc3LVpUKZFkmlUpx6dIlPB4Po6OjX2hcUzvbWSwWbaI3F8lkkvXr1+N2u4nFYjz//PMrbnl1TkPxwx/+cNrjF1544Zb6NquNlpYWrWI6kUggSRJbtmyhv7+f2tpaTV8mEAhQUFCA3+/H5/Px3nvvadpMBQUFVFZWUl9fz2OPPTZteUXVnTl+/DiSJFFRUQHAoUOHOHXqFAMDA6xbt06TOv7ss88wmUzk5OQgCAIjIyNafQWgtUjNZrPk5uZqtRwWi4WCggL6+/tJJpOau56Xl8fg4CDZbJZ4PM7zzz9PSUkJbrcbi8UCTBbh5eXlMTQ0hMvl0jSlBEHA6/Vy7tw5JiYmeOihh3jnnXcYGhqiuLiYZ555BkmSeO+99wiHw/T397NmzRr8fj+1tbXLsojuTtGvlcVlquF48803tYZaqmqyy+VieHiYoqIi0um0Jq0vSRIXLlwgkUjgcrm+sKEAtOJWRVE0T2EuzGYza9euZfPmzUiStCInR3MaihtP9GAwSEtLyz2V9TQ8PIzb7ebBBx/UYg2lpaUUFhYiSRIPPvigJt2tehTqkpEqORyLxSgtLeXKlSucP3+e8+fP8+1vf1uT5CgpKeF//V//VwAuXrzI8ePHSafT2ix/cHBQ64A3tWq7pKRE8zAALUg9FdXjUFNh6+rqtKB2IpEgFAppN3abzUYmk6GmpoYtW7bQ3t7O+fPneeSRR7R+28PDw2zfvp1IJEI8Huf8+fOsW7cOSZI0I7Zu3Tpyc3P53e9+x8cff0xXVxc5OTmaRtb27ds5ffo0drudkydProolKP1auXuoWXIej4dMJkNpaSkjIyNs2rSJTZs2MTIywtmzZ7XlocHBQQoLCzEajVgslpvUm+8EtU7iVhIgoihqEjp+v5+GhoYv/NlLwZyG4kbRM7fbvard3ZlQT8rq6mouXbpETk4OmUwGl8tFSUkJf/AHf0B7e7tWBep0OiktLdVc3nA4zMTEhLY+qurTC4KgZckUFhby1ltvcfXqVdra2ojH41gslmmpsQMDA8Tjca2hUSAQwGg0YrVacTqdWh9fQRCora0llUqRSqW0wLfb7SaVSmnLRydOnCCbzWqif6Iosn37dkRR1GY827Zt09KBh4aGWLduHVVVVRQVFXHixIlpWVGFhYWaOqbH40GSJG2ZKScnB0mSWL9+PdFoVCta2rBhw4rT5Z8N/Vq5e6jxC5vNRkNDA9FolMrKSu08tNlsWlOhnp4eTR1BUZQFMxTzQRAELBYLBoNBG+u3vvWtu/LZC82chuLYsWNalem9ytSg2qOPPkptba3W3nTqTW5q4dnOnTs5deoUn332GUajkYqKCnp7e4lEIlpa6jvvvKMZCphMBwwEAhgMBiKRCMlkUuuY5fP5NJ0nNdXWaDQSj8e1hkZqbwpZlpmYmKCwsJBvfetbBAIBzp8/T1dXF4WFhfT19TE4OMiVK1c0jwcmW6Neu3YNWZandayrq6vjwoULfPLJJ5SXl7N+/Xotq2rDhg04HP//9s4tpq3sbMOvsTFgG58AYwiHYJNQhoQQA200zaHtbNL0Im2nA0krVZ1DO7EqVe1NC6FSrxFRLlr1ojLJqK2qakTwtCP1otPYiegoE5GAnQFCEgh2hsQJJ4MdH4AA9v4v+PeqDcYYBmMb1nOTYG+zF8tr7XX6vvcVIzc3FwqFAocOHSL5EDdu3CA5JPPz8xCJRHjttddw4MABklAHJK+O02ahfWXn4LahuEi5vr4+OBwOYrDFrTBevHiB/Px8BAIBMpHajA9FNGLJpeBWMIWFhfjzn/+c0qvmqLUWqeHfvHlzTy2nQ/dG79y5Q7I/gfCHXGjimUgkwokTJ5Cfn4/c3FwEg0H88Y9/hNfrhVQqhd/vJ+YnwEqkkUqlglqtJnutCwsLSE9Ph1gsBsuykEqlePz4MTmDSEtLg0QiAcuyEAgEOHLkCJlFSSQSVFdXkzDYhYUFMuOfmJiAx+NBeno6ZmdnEQwGSQd6/Pgx/H4/hoeH4fF4wmZhr7/+Oh4/foxr164RQ57FxUWUlpZCrVajqKgIdXV1ZFA9fPgwnj59igMHDpAoKy5HgrOcTUVd/vWgfSW+rGcjGjpohMKyLMxmM2QyGQYHB5Gbm4vMzMw1Uh5bZaNBgsfjQSAQQCQS4ejRoyk9SAARBopvf/vb6148MzODJ0+e4MmTJ3si8ml14wwGg+tmEYfKUCgUCgQCAXz/+9/HyMgIGUy4s4Lc3Fyo1WpyH6VSiYGBAYyOjhJPbLFYDJlMBrfbjerqavJwDzVcmZubw3e+8x2MjIzA5XKFOeQNDw+jp6eHGMJPTU1hbm4OHo8HSqWS2EVySYHcTEsqleLGjRtQq9Xo6emBRCJBTk4OgsEgnj17BrvdjmPHjkEmk4FlWTidTpw4cSJMA4qru7/+9a8AgDNnzpD3ucPFhw8fIjMzkxzepyK0r+wcm7XFPXbsGMn/kUqlyM7Oxvj4OAnj3q7ku/VIS0sjibe7QW5+zUBRVlaGlpYWAIDRaATDMEQqwG63w2q17pmGH9o4R0ZGMDw8jCNHjuDBgwdYXl5GRUUFmUmGDiCc5WggEMCDBw9gt9tRWFhIQmDT0tLCdOgfPXpEZvBzc3OQyWTYt28fcnNz8fTpU3Imwg0UfD6fxGc7HA6SKOf3+8GyLOkMcrmc5E5w4micKZJQKERaWhpycnKwsLCAhYUFIoEwNTWFyclJaDQaFBUVoaenh7jbFRcXE0kQr9eLN998M2KH5fP5qKqqWvNeb28vHjx4gIMHD5JQxu2IREkEtK/sHJuVnQ8VruS2X7Ozs+H3+5GRkUG2buOBQCBAXl4eDh06hK9+9asRPW5SjTUDRWiYX21tbdiSuqysbE1S0W4mtHFOTU3BZrNBoVDA5XJBq9UiIyMD9+7dC1OP5eK809PTwefzcffuXTidTmJiBKw09NDopNHRUQgEAigUCgSDQWIG/+jRIywsLBDNJj6fD4FAQKKWXr16hYGBAYjFYsjlcpSWluLFixfEPjUtLQ0ejwc+n494XnC+F8FgkHhVACDbWC9fviSvcRaOXPRISUkJeDweyfqurq7G0aNH0dPTs2ZLIFqdFhUV4cWLFxCLxXA4HClj3rIa2ld2js3ogYXuBDidTggEAhQVFeHZs2coLS0Fn8/H7Ows6UfbCY/Hg1qtRlVVFc6dO4cf//jHKb/tBGxwRmG1WtfssUZ6bbcS2jg5yezZ2Vnk5ubC5XKF5QJwM5hAIIDLly/D4/GQiCefz0c0mfLz83HixAkiBcJ9lsuTcLvd5ECam+lnZWURRzouu5kzK/J4PMjMzERxcTFGR0eRnZ0Nt9uN9PR0zMzMQCaTkUNuziKVO/PgJMu5bNHBwUGIRCIcPnwYXq8XY2NjKCkpwdjYGEQiEerr67GwsICioiKcOnWKHOCPjIxgcnIS3d3dGBoawjvvvLNu51Cr1cRVz+FwoLq6OuWSjyKx1/tKvNmMHlhom/z000+xf/9+vPHGG6ioqIBarcbnn3+OYDAIp9OJ//73v9tiicohEolw9uxZ/P73v4/qQZNqbHiYffr0adTW1gJYafh6vX5HCpYMhDZO7nA2OzubCN1Fmtn09vZifn6eaC75/X6y3cQlwTmdzrB46qNHj8Lr9eKzzz4j20tSqZS4YT19+pQMFAKBgJwncNmnmZmZ5D0AyM/PJybwGRkZZGXBbTlx4mmZmZl4/vw50cnnbE+HhoaQnZ1NMq05uez+/n6cPn06TG58YmICk5OTRLV2cHAQfX196+4fc3UqlUrDzjZSnb3eV+LNRrLzoauI4eFhACCaZSMjIyTo4vjx4/B6vRgYGEBeXh4aGhpw/fr1L10+Ho8HiUSCuro6vP3227tqkAA2GCjeeOMNaDQaGI1GACtL7bKysh0pWDIQ6XCWO6CSyWQk0ieUiYkJ1NTUYHR0FMPDwxAKhRCLxUTgbHl5GUqlMkzRs6SkhCSxcVFJ3L+vXr0iqwBuYMjIyCDbSJxm09LSEpqamtDX10eMhmw2G1F39Xg8JJOU2+bikoZ8Ph+CwSCJpnr27BkOHz6M3NxczMzMwOv1Ij8/HyUlJZiensa//vWvMP/i7u5usv9bVFQUdf841XwmYmWv95VEExpJNzk5CY/Hg4qKCgSDQWRlZaG/v594W3NqCQ6HA2+++Says7O/lMmUSCRCbm4uVCoVzp8/vytWyKvZMKi4rKwMv/nNb3aiLEnNeoezq8nLyyMNViwWo7KyEpOTk+TM4uzZsygsLMTAwAD5Xdws+/r161AqlcjLy8PExATxr+D8qgOBANLS0qBUKgGsnCHs378fMpkMR44cwdDQEHw+H2ZmZpCbm4upqSnIZDJMTk5CJpNBIpHgzJkz+Pe//w2pVEqkzTm4Fc/y8jJJ6ONyIBYXF3H79m0UFRWRFQrnRDc0NITBwUEUFRVBpVIlvW1pvKB9JXHcunULHo8HYrGYbL96vV4UFRXB4/FApVKhsrISf//738Hj8VBeXk5UEzo7O/HBBx/gF7/4xYZyHKvJycnByZMncezYsTXyPLuJNQMF584FrJUlAFYSi/70pz/FvWCpROiyd2JiAmNjY7Db7fjiiy/Asiyqqqogl8uhUqngdrtRWVkZNuvmZtnLy8swm83o7u6GUCiEVqvF5OQkkQvnMjwXFxfB5/Oh1WrJttXdu3cRCAQwMzMDp9MJl8uF/Px8Ep6blpaG+fl5mM1mlJaWEkHA5eVliEQiokS7vLxMVh0qlQoqlYr4/LIsi7KyMmLOND4+jrq6OrzzzjvEErWgoGBXzqgiQftKcsLj8VBbW4tvfetbGB8fx8jICJH8z8jIgMvlCruez+fjpz/9KT744ANYLJaYDrj5fD6USiVxnNxtW02rWTNQMAwDo9GImpqaNbIEAGCxWHa0gKlA6LK3p6eHhJhyUgH9/f3IysoidojcQXgoXOPMyspCRkYGiouLIRKJsLS0BI/Hg7S0NGRlZZGopaysLIyPj6O8vBzl5eXo6enB2NgYCWGdn5/H0tISnE4nyZVYXFzE6OgoxGIxFhcXsW/fPiIXzg0SfD6feGFkZmbC6XTihz/8Ibq7u7G0tASLxQK3242MjAyUlpaS84jVK61AIID79+9jfHw8pmioVIT2leTh+PHjMJvNePXqFWQyGU6dOkXaZE9PDwlKKS0tJeeAIpEIx48fB7Dy4L9+/TrOnz+PmzdvrruyEAqFyM7OhlarxXvvvYd333131w8SQISBYnR0lPw/kizBXjMuCl0tzMzMRDx8DV32Tk9Pk4czsNIAhUIheDwe5HI5pqensby8vGbW3dvbiydPnqCyshI+nw/Pnj3D5OQkFhYWoNFoMD09TQ6bOXXYtLQ03L59G319fUhPTydmKpOTk8jMzMTw8DDRxJFIJJiZmUFaWhrJ1hYIBCgpKUEwGIRKpcLBgwcRCAQwNjZGwnBnZmZgMplQU1ODsbExEnuekZEBp9OJjz76CIODg1AoFGEeE729vSRSbLfoOa2G9pXkgUuwi7SqDQ1KOXPmDJaWltDZ2Ynnz5/j/v37qKurg1AohEwmwyeffAKfz4ef//znuHXrFkQiEQ4dOkRyl3Q6Hb7xjW/s2i2m9Yh6RsHj8fD555+jpqYGV69ehc1mQ2tr606VLSkITbpzOBxrInq4B6vD4UBOTg4kEgmZzQcCASwtLZEtGy7SSalUrmlkoTkbOp0OTqcTUqkUOTk5cDqdROtpaWkJi4uLEAgEmJ6eJmcYnMz4y5cvif8Dl+Ht9XqJHlRhYSHkcjnm5ubAsiyKi4vh8/ngcDjw4sULaDQaACtudSqVClqtFgKBAFVVVXC5XCgrK8PExASWl5fR19eHuro6WCyWNR4TExMTEIvFALam57SeZEOyQvtKYokWJLH6vatXr4JlWRQUFODevXsQCAR47733yPsSiQR/+9vf4l7mVCKqFarJZIJMJsOVK1fQ1dWFixcv4tq1aztVtrgSCATQ09ODjz/+GD09PevuS4Y+wMViMXngcZ+/fPkyMRKamZlBTk4OTpw4gRMnTkChUCA7Oxv79u2DRCIhYoKRDntDrR99Ph94PB4KCgqQkZGBubk5LC8vQyaTQSgUEjtTLp+CU8TkQl65kFrO4KW4uBiZmZlElnlxcREFBQXQ6XTIz88nYoV2ux337t3D3NwcfD4fPB4PhoeHMTAwgMuXL8PlcqGkpAR5eXnw+XyQyWTwer0kryR0QFCr1fD7/QCwrgVqNLgBms/nY3R0dNNG9DvNTvcVo9EIo9GIlpYWmM3muN1nNzI6OkqsUXNzczEyMpLgEiU/UVcUtbW1KCsrg9FohF6vh0wm2zUhf7Fqx4Qm3XG+EqGf5wTyxGIxpFIpJBIJTp06hampKRw+fBharRY2mw1DQ0NgWRYNDQ0RD3tDl8cajQafffYZ+vr64PV6yQG21+uFTCYjKxUurJVTx/T7/UQG/dWrV2SVU1hYiJMnTyIrKwsDAwPw+/14++23sby8jH/+85/wer0kz4LzmRCLxfD7/USOpLa2Fk6nE263GwcPHkROTg4EAgFmZ2fhdDrX5JXU19fjwYMHCAQCGyZIRWKzkg2JZif7Cmd/2tzcDJ1Oh6amJnoesgnKy8tJfa3OaaJEJupAYbfboVAo0Nvbi66uLgAIyyhOZdZ7EK3e8tDpdOQBXlxcTB543OdDZbQrKyvJQ7G3txfDw8MYHBxETU0NCgsLceDAgZiWx93d3RgYGMDDhw+Ju5xarYbL5SIHcVlZWVAoFMQXmyvLxMQE5ufnAYAcso2Pj0MoFEKhUKC4uBglJSV499138fHHH0MqleKLL76ASCSCx+PB8vIyhEIh5ufnieR5ZmYmPB4P8vLykJmZicrKSkgkEvT39yM/Px8qlWpNXkms4cTrsRnJhmRgJ/sKwzBgGIbcd69EmcXKRtuWnF99d3c3jh8/nrIeETvJhgl3BoMBFosFLMvi4sWLZMmW6qz3IIomSXHnzh3S4LjPR5LR5lYbFRUV6O/vx8jICJG8iIU//OEPmJiYgFAoJA/vb37zm1AoFHA4HBgaGiKm8Zw8yGuvvQZgJRuVy7XgoqakUinm5uYwPz8PoVAIkUiEv/zlL/jKV76CwsJCiMVi2Gw2CIVCCIVCcq3T6SRbW5w73fvvv088JUJVYbebzUg2JAOJ6isGgyHMq3s1H374IT788EPy8+joKO7cuROXsrhcrrj97s1w//59OBwOiMVi9Pf348GDB6iqqgq7pqqqCoWFhVAoFCkTdJDI+o06UJSVlaG1tRVPnjxBWVkZ9Hr9rtl6Wu9BFKskxepIitAH5sTEBCQSCYaHh0lS29e//vWYy8bNRDMzM+H3+zE3N4fHjx/jwIEDqKiogMvlwosXL4iJkVKpxPT0NEQiEfbv3w+XywWWZcHj8ZCdnY2Kigr4/X5yyF5eXo7BwUFUVlaiuroag4OD+N73vgePx4Pbt29DrVbj6dOn8Pv9EIvFUCgU8Hq9qKqqwk9+8pMdCQdMtQzu7ewrRqMRdrt9zeuNjY0k2AAALl26hNbW1rDXVvOjH/0oTAXgu9/9btzq9c6dO0nxnY2PjxNPeADr+lQnS3ljJZHljTpQfPTRRzAYDODxePjPf/4DAPjHP/6BH/zgBztSuHiy3oMoVkmKaA8ytVqNTz75BI8ePYLb7YZSqYTZbIZAIIjpiy4rK8PAwADcbjcCgQCxFh0aGkJ+fj4xROEMh9LT06FWqzE7O4u6ujoUFhbi4cOHkMvlCAQCUKlUEIlEuHnzJvbv34/5+XkUFRVhamoqLFlOpVJhenoaz58/h1KphFgshs/nw4EDB8Dn8/HLX/5yT8SMb4Xt7CuNjY0bXmM2m8EwDHQ6HYxGY0yf2StE2i2ItB1FiZ2oA4XL5cL169dx48YNACsPsFRZpm2V7ZCkqK+vx6effoqlpSUUFhaioKAAi4uLMR/I/upXv8Lvfvc7uFwuSKVSKJVKBINBElbLmRAJBAKytcStcHQ6HV6+fAmGYYiaLWcV6fF44PV6UVhYSP6u1QMe5wxmt9vh8XhQU1MDgUCA6urqlJp97TQ72Vfsdjuampqg0WjgdrvBMAwdKEKItFsQKXiFEjsxGciGVmqkJfF2wQmq9fb2oqGhgRzY7SR8Pv9LS1Lw+XycPHmSOMrNz88TL99YOHHiBM6ePUtyFrjfkZ2djeHhYYjFYvD5fLx69Qp8Ph9qtZrYiU5NTYVtpUXyF472d4U6g3EexNwhdTLnMSQLO9FXNBrNGhkKyv+ItNqPFLyyVzXJtkLUgUKhUOD06dNQKBSwWq0wmUxxk05OppC/7dgfr6+vRzAYxK1btwCsSAzEOuBwA41arcbExAS6uroQCASQl5dHtogKCgrg8/kgEolQUVERk4tWLH/Xeh7ElOjsZF+hbJ5Ui6JLNqIOFG+99RY0Gg06OzsxOjqK3/72txFN5LeD3Rbyx+fz8frrr2/ZBjF0+bx//36IxWLMzs5CJpMRORC5XA6ZTIb8/PxtLj1ls+xkX6FEJlpYbKTtqGRP4kwmNtx6Onr0aFiDv3r1Kn72s5/FtVCJDvkLBAJ4+PAhybSurKwEn89PSHgaZ63ISYf7fD6Ul5cTl7h9+/YhPz8/KcISV5Ms4ZI7RSL6CuV/REuiTbUoumQj4kBx9epVdHV1oaGhAb/+9a8BrEgqc0vqrTT+VAr56+npQUZGBiorK4lF6de+9rWEhKexLIuRkRFMTU3B4XDg+PHjYQ5zQPKG+SVrubaTePQVytZItWz+VGLNQMFp1eh0Oty9exdXr17F9evXYTQaoVAocOXKlS3dKJVC/pKpwe1W69DdQLz6CmVr0HOI+LFmoLDZbGEeshcvXkRDQ0PcxQCTKeQvmRocXTInL4nqK5TIpFo2fyqxZqDQarVhP9fX1+Ott96Ke0GSKeSPNjhKLCSqr1AiQydV8WPNQLE6EUWhUIT9vNsO6NaLlKANjrIRe62v7AZiMSKjrGXNQNHc3BwWcTQ7OwulUglg5WDV5XLtqsYfq9w4hbKavdZXdgMbGZFRIhPRM3s9Zy6WZdHR0RH3Qu0kyXRwTUkt9lpf2Q2sZ0RGic6agaK1tTVqotBuyzZNpoNrSmqx1/rKbmA9IzJKdNYMFBtlk+62bFN6cE3ZKnutr+wGQvt7qBEZJToxiQLuZujBNYWydwjt76FGZJTo8FiWZRNdiC9LVVXVmlDFePD8+XPs27cv7vfZLLRcm8dqtcLhcCS6GDtOPPtKMn/fkaDljQ2bzQawlJg5e/ZsoosQEVquzZPMZUtVUq1OaXljJ23HhycKhUKhpBR0oKBQKBRKVOhAsQlCFWuTCVquzZPMZUtVUq1OaXljZ1ccZlMoFAolftAVBYVCoVCiQgcKCoVCoUSFDhQUCoVCiQodKCgUCoUSFTpQbIDRaITRaERLSwvMZnPEazgvcLfbDbfbHbeydHR0wGw249KlSxH9x2O9ZrtJpjra7D0TUV+7HbfbjZaWFlit1kQXJSKp9J0nTV0mLNUvBTCZTGx7ezvLsixrs9lYnU4X8TqGYVidTsc2NzezLpcrLmWx2Wxsc3Mz+bmxsXFL12w3yVRHm71nIuprL2AymdgLFy6wFosl0UVZQ6p958lSl3teFDAaDMOAYRgAK7PT9ZQm9Xo9NBoNNBoN5HJ5XMpiNpvDNHoizYRiuWa7SaY62uw9E1FfewGGYWAymRJdjIik2neeLHVJt55ixGAwoKWlJeJ7drsdGo2GLGnjweqtk0hbKbFcE08SXUebvWei64uy89DvfGvs6RWF0WiMOKNobGyERqMhP1+6dAmtra1hr4XS3NxMPtfU1ASLxbLtZZXL5Rs26liuiRfJUEebvWci6ytVibXPJCv0O98ae3qgaGxs3PAas9kMhmGg0+lgNBrXfMZsNsNut+PChQvELzke1NXVobOzk/ys0+m2dE08SJY62uw9E1VfqUwsfSaZod/51tjTA8VG2O12NDU1QaPRwO12g2EY0lFqa2tx48YNMAwDs9kMs9kMk8mEK1euxKUsOp0OfX195AEYeh+uLNGuiRfJVEehRLtnIutrL2A2m8OidJLpYZxq33my1CXVeqJQKBRKVOhhNoVCoVCiQgcKCoVCoUSFDhQUCoVCiQodKCgUCoUSFTpQUCgUCiUqdKBIAjjhr46ODhiNRnR0dMBqtaKjoyPh5dLr9eDxeGFlaWhoQENDw45lWFMolMRC8ygSjN1uR0NDAywWS5geUVNTE+rr6xNXMKxksRoMBly7do1k3brdbjQ1NeHChQsJLRtl72K1Wkm7bG9vJ69zmkhdXV0AVtpqR0cHNBoN7HY7ent7UV9fD5PJtO36SXa7nagnd3V1Ef0zYEVzjHs/9PWUIqGShBRWp9OxXV1da1632WxElTXRNDc3swzDsCzLsgaDIcGloVBWVFU1Gs2a10OVYS9cuMCaTCaWZVnWYrGQ67eixGoymTb8nMvlYgGwNpst7PVk6cdfBrr1lEDcbjesVmvEbEuNRkMynDlRu1C/B6vVCq1WS37W6/VEFdNut4dp7nOZndy2FrfFFSt6vZ78LrqSoCQjXBsPXYXPzs6SlbBcLif/30p2c+jKZT3kcjkYhgnrWx0dHURzLJWhW08JhBNXW09MjXu9vb0dNpsNDMNAoVDA5XJBp9OFLWPb29tRW1sLYEXF9fz589DpdERag1uuc0vuhoYGMAwTk+Q3J9U9MzPzZf5cCiUuuN1uGAwGGAwGMrniJDoMBgO0Wi3cbjeZQJ07dw5msxmzs7MAAKVSGfY5q9VKtquam5vJtZ2dnejr64s6WdLr9WhpaUFzczOMRiPOnTsX/wrYCRK9pNnLrLdUDX0/9P8Wi4WVy+XktdCltcvlIktrm83GyuVylmEYslXU3NzMNjY2siaTiTWZTCzDMDEvwQ0GA9vV1RV2bwolkZhMJlYul7MGg4Ftb2+PaEDU2NhI2rjNZiPbpxaLhfyfZVcMrlwu1xrjLZ1OR/pg6O/aCABsc3Nzws2GthO6okggcrkcOp2OzGBWYzab0djYiJaWFuj1euh0upjUV5VKJZ48eQKz2QyDwQBgZdZVX19PViGxHqoZjUYwDAONRoOWlpaI6rAUSiJQKpVkdh8alcf5kKxHZ2cn5HJ5WNSe3W4nKsgcW5XCZxgGWq02qcQQvyz0jCLBdHV1oaWlZY1GfkdHBxobG9HR0QG3200avt1uh9vthtlsDtPWD230bW1tkMvlaGxsRFdXF2w2G/R6fVikh91u39Ddy2g0ku0rAGhpaUFbW9s2/NUUyvbCDRixeEuHTpo4B7lYH+rcFlY0+vr6ds+W0/9DVxQJRqPRwGKxoK2tDVqtFkqlErOzs6ShcQ3ZbDbD7XajubkZbW1txCTIYDBALpdDLpfDbrfDaDQiJycnLCywtbUVcrkcer0ely5dIg/+9VYGVqsVbW1tsFqtYbMqm80Gq9WKlpYW8jsplGSCO6uIBHcmEXqOAPzvrJBhGLz//vvk+tVniNzno7V77jO7rW9QmfFdhNvt3nUNlEJZDTeRMRqNMBgMZHJlMplgtVphs9lgNpuh1+vBMAza29vR1taGjo4OXLlyBY2NjcSpb/Wkab3XuWAQrVa7bhRT6ATLYDCkbs5EBOhAQaFQKJSo0DMKCoVCoUSFDhQUCoVCicr/Abe9BrGHwxIQAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 400x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We use the MNU pair 55 as an example and without standardized data\n",
    "dataset = MNU(55, preprocessor=None, double=True)  # 1000\n",
    "x, y = dataset.cause.flatten().numpy(), dataset.effect.flatten().numpy()\n",
    "# This function call shows all options of LOCI, we return_function to visualize the estimator\n",
    "score, f_forward, f_reverse = loci(\n",
    "    x, y, independence_test=False, neural_network=True, \n",
    "    return_function=True, n_steps=1000\n",
    ")\n",
    "print(score)\n",
    "plot_pair(x, y, f_forward, f_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ed377aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: avg loss training: 3.6926,... Gradient Norm: 9.3265\n",
      "Epoch 2: avg loss training: 85.3487,... Gradient Norm: 982.0737\n",
      "Epoch 3: avg loss training: 14.0053,... Gradient Norm: 127.5717\n",
      "Epoch 4: avg loss training: 7.2259,... Gradient Norm: 40.7762\n",
      "Epoch 5: avg loss training: 7.6353,... Gradient Norm: 30.7407\n",
      "Epoch 6: avg loss training: 10.7517,... Gradient Norm: 84.3701\n",
      "Epoch 7: avg loss training: 7.4422,... Gradient Norm: 40.9591\n",
      "Epoch 8: avg loss training: 5.3470,... Gradient Norm: 24.0251\n",
      "Epoch 9: avg loss training: 5.0025,... Gradient Norm: 19.1312\n",
      "Epoch 10: avg loss training: 5.5432,... Gradient Norm: 29.5750\n",
      "Epoch 11: avg loss training: 4.9912,... Gradient Norm: 19.9237\n",
      "Epoch 12: avg loss training: 4.4527,... Gradient Norm: 9.8530\n",
      "Epoch 13: avg loss training: 4.2842,... Gradient Norm: 7.6250\n",
      "Epoch 14: avg loss training: 4.1898,... Gradient Norm: 8.2403\n",
      "Epoch 15: avg loss training: 4.0659,... Gradient Norm: 9.5266\n",
      "Epoch 16: avg loss training: 3.8606,... Gradient Norm: 6.0889\n",
      "Epoch 17: avg loss training: 3.7327,... Gradient Norm: 7.7304\n",
      "Epoch 18: avg loss training: 3.5911,... Gradient Norm: 7.1729\n",
      "Epoch 19: avg loss training: 3.4814,... Gradient Norm: 4.8666\n",
      "Epoch 20: avg loss training: 3.5104,... Gradient Norm: 11.3692\n",
      "Epoch 21: avg loss training: 3.4152,... Gradient Norm: 5.3470\n",
      "Epoch 22: avg loss training: 3.4062,... Gradient Norm: 11.1502\n",
      "Epoch 23: avg loss training: 3.2710,... Gradient Norm: 2.1369\n",
      "Epoch 24: avg loss training: 3.2795,... Gradient Norm: 9.5992\n",
      "Epoch 25: avg loss training: 3.2334,... Gradient Norm: 4.9106\n",
      "Epoch 26: avg loss training: 3.1893,... Gradient Norm: 6.5308\n",
      "Epoch 27: avg loss training: 3.1313,... Gradient Norm: 2.7865\n",
      "Epoch 28: avg loss training: 3.1427,... Gradient Norm: 7.0554\n",
      "Epoch 29: avg loss training: 3.1105,... Gradient Norm: 3.8522\n",
      "Epoch 30: avg loss training: 3.0911,... Gradient Norm: 5.4343\n",
      "Epoch 31: avg loss training: 3.0801,... Gradient Norm: 2.3666\n",
      "Epoch 32: avg loss training: 3.1002,... Gradient Norm: 6.6387\n",
      "Epoch 33: avg loss training: 3.0829,... Gradient Norm: 4.3959\n",
      "Epoch 34: avg loss training: 3.0811,... Gradient Norm: 3.4075\n",
      "Epoch 35: avg loss training: 3.1013,... Gradient Norm: 6.8152\n",
      "Epoch 36: avg loss training: 3.0860,... Gradient Norm: 4.1286\n",
      "Epoch 37: avg loss training: 3.0760,... Gradient Norm: 2.8540\n",
      "Epoch 38: avg loss training: 3.0843,... Gradient Norm: 5.3108\n",
      "Epoch 39: avg loss training: 3.0729,... Gradient Norm: 3.7769\n",
      "Epoch 40: avg loss training: 3.0587,... Gradient Norm: 2.1078\n",
      "Epoch 41: avg loss training: 3.0636,... Gradient Norm: 4.7471\n",
      "Epoch 42: avg loss training: 3.0648,... Gradient Norm: 4.1387\n",
      "Epoch 43: avg loss training: 3.0555,... Gradient Norm: 1.1615\n",
      "Epoch 44: avg loss training: 3.0556,... Gradient Norm: 2.7310\n",
      "Epoch 45: avg loss training: 3.0587,... Gradient Norm: 3.5555\n",
      "Epoch 46: avg loss training: 3.0540,... Gradient Norm: 1.5606\n",
      "Epoch 47: avg loss training: 3.0525,... Gradient Norm: 2.0719\n",
      "Epoch 48: avg loss training: 3.0566,... Gradient Norm: 3.3700\n",
      "Epoch 49: avg loss training: 3.0549,... Gradient Norm: 2.6730\n",
      "Epoch 50: avg loss training: 3.0499,... Gradient Norm: 1.1797\n",
      "Epoch 51: avg loss training: 3.0493,... Gradient Norm: 2.4717\n",
      "Epoch 52: avg loss training: 3.0504,... Gradient Norm: 2.8428\n",
      "Epoch 53: avg loss training: 3.0482,... Gradient Norm: 1.7796\n",
      "Epoch 54: avg loss training: 3.0463,... Gradient Norm: 0.9072\n",
      "Epoch 55: avg loss training: 3.0480,... Gradient Norm: 2.3867\n",
      "Epoch 56: avg loss training: 3.0484,... Gradient Norm: 2.3825\n",
      "Epoch 57: avg loss training: 3.0462,... Gradient Norm: 1.0512\n",
      "Epoch 58: avg loss training: 3.0461,... Gradient Norm: 0.8576\n",
      "Epoch 59: avg loss training: 3.0478,... Gradient Norm: 2.0678\n",
      "Epoch 60: avg loss training: 3.0473,... Gradient Norm: 1.7376\n",
      "Epoch 61: avg loss training: 3.0455,... Gradient Norm: 0.6032\n",
      "Epoch 62: avg loss training: 3.0452,... Gradient Norm: 0.8543\n",
      "Epoch 63: avg loss training: 3.0454,... Gradient Norm: 1.5219\n",
      "Epoch 64: avg loss training: 3.0445,... Gradient Norm: 1.0431\n",
      "Epoch 65: avg loss training: 3.0437,... Gradient Norm: 0.2149\n",
      "Epoch 66: avg loss training: 3.0441,... Gradient Norm: 1.0222\n",
      "Epoch 67: avg loss training: 3.0441,... Gradient Norm: 1.4007\n",
      "Epoch 68: avg loss training: 3.0433,... Gradient Norm: 0.8704\n",
      "Epoch 69: avg loss training: 3.0430,... Gradient Norm: 0.3078\n",
      "Epoch 70: avg loss training: 3.0434,... Gradient Norm: 0.9660\n",
      "Epoch 71: avg loss training: 3.0434,... Gradient Norm: 1.3008\n",
      "Epoch 72: avg loss training: 3.0429,... Gradient Norm: 0.8149\n",
      "Epoch 73: avg loss training: 3.0427,... Gradient Norm: 0.3342\n",
      "Epoch 74: avg loss training: 3.0427,... Gradient Norm: 0.7183\n",
      "Epoch 75: avg loss training: 3.0424,... Gradient Norm: 0.7235\n",
      "Epoch 76: avg loss training: 3.0421,... Gradient Norm: 0.3436\n",
      "Epoch 77: avg loss training: 3.0421,... Gradient Norm: 0.3925\n",
      "Epoch 78: avg loss training: 3.0420,... Gradient Norm: 0.5084\n",
      "Epoch 79: avg loss training: 3.0417,... Gradient Norm: 0.3463\n",
      "Epoch 80: avg loss training: 3.0416,... Gradient Norm: 0.1958\n",
      "Epoch 81: avg loss training: 3.0416,... Gradient Norm: 0.4335\n",
      "Epoch 82: avg loss training: 3.0415,... Gradient Norm: 0.4498\n",
      "Epoch 83: avg loss training: 3.0414,... Gradient Norm: 0.3034\n",
      "Epoch 84: avg loss training: 3.0413,... Gradient Norm: 0.2366\n",
      "Epoch 85: avg loss training: 3.0412,... Gradient Norm: 0.4191\n",
      "Epoch 86: avg loss training: 3.0410,... Gradient Norm: 0.4230\n",
      "Epoch 87: avg loss training: 3.0409,... Gradient Norm: 0.2350\n",
      "Epoch 88: avg loss training: 3.0408,... Gradient Norm: 0.2292\n",
      "Epoch 89: avg loss training: 3.0407,... Gradient Norm: 0.3702\n",
      "Epoch 90: avg loss training: 3.0405,... Gradient Norm: 0.2927\n",
      "Epoch 91: avg loss training: 3.0404,... Gradient Norm: 0.1557\n",
      "Epoch 92: avg loss training: 3.0403,... Gradient Norm: 0.2600\n",
      "Epoch 93: avg loss training: 3.0403,... Gradient Norm: 0.4350\n",
      "Epoch 94: avg loss training: 3.0402,... Gradient Norm: 0.3781\n",
      "Epoch 95: avg loss training: 3.0401,... Gradient Norm: 0.1511\n",
      "Epoch 96: avg loss training: 3.0400,... Gradient Norm: 0.2594\n",
      "Epoch 97: avg loss training: 3.0399,... Gradient Norm: 0.3811\n",
      "Epoch 98: avg loss training: 3.0398,... Gradient Norm: 0.3226\n",
      "Epoch 99: avg loss training: 3.0397,... Gradient Norm: 0.1241\n",
      "Epoch 100: avg loss training: 3.0396,... Gradient Norm: 0.2155\n",
      "Epoch 101: avg loss training: 3.0395,... Gradient Norm: 0.3395\n",
      "Epoch 102: avg loss training: 3.0395,... Gradient Norm: 0.2888\n",
      "Epoch 103: avg loss training: 3.0394,... Gradient Norm: 0.0935\n",
      "Epoch 104: avg loss training: 3.0393,... Gradient Norm: 0.1676\n",
      "Epoch 105: avg loss training: 3.0392,... Gradient Norm: 0.2501\n",
      "Epoch 106: avg loss training: 3.0392,... Gradient Norm: 0.1955\n",
      "Epoch 107: avg loss training: 3.0391,... Gradient Norm: 0.0514\n",
      "Epoch 108: avg loss training: 3.0390,... Gradient Norm: 0.1485\n",
      "Epoch 109: avg loss training: 3.0390,... Gradient Norm: 0.2203\n",
      "Epoch 110: avg loss training: 3.0389,... Gradient Norm: 0.1800\n",
      "Epoch 111: avg loss training: 3.0388,... Gradient Norm: 0.0453\n",
      "Epoch 112: avg loss training: 3.0388,... Gradient Norm: 0.1155\n",
      "Epoch 113: avg loss training: 3.0387,... Gradient Norm: 0.1630\n",
      "Epoch 114: avg loss training: 3.0386,... Gradient Norm: 0.1058\n",
      "Epoch 115: avg loss training: 3.0385,... Gradient Norm: 0.0392\n",
      "Epoch 116: avg loss training: 3.0385,... Gradient Norm: 0.1403\n",
      "Epoch 117: avg loss training: 3.0384,... Gradient Norm: 0.1693\n",
      "Epoch 118: avg loss training: 3.0384,... Gradient Norm: 0.1178\n",
      "Epoch 119: avg loss training: 3.0383,... Gradient Norm: 0.0276\n",
      "Epoch 120: avg loss training: 3.0382,... Gradient Norm: 0.1151\n",
      "Epoch 121: avg loss training: 3.0382,... Gradient Norm: 0.1624\n",
      "Epoch 122: avg loss training: 3.0381,... Gradient Norm: 0.1042\n",
      "Epoch 123: avg loss training: 3.0381,... Gradient Norm: 0.0326\n",
      "Epoch 124: avg loss training: 3.0380,... Gradient Norm: 0.1293\n",
      "Epoch 125: avg loss training: 3.0380,... Gradient Norm: 0.1435\n",
      "Epoch 126: avg loss training: 3.0379,... Gradient Norm: 0.0883\n",
      "Epoch 127: avg loss training: 3.0378,... Gradient Norm: 0.0530\n",
      "Epoch 128: avg loss training: 3.0378,... Gradient Norm: 0.1151\n",
      "Epoch 129: avg loss training: 3.0377,... Gradient Norm: 0.1240\n",
      "Epoch 130: avg loss training: 3.0377,... Gradient Norm: 0.0591\n",
      "Epoch 131: avg loss training: 3.0376,... Gradient Norm: 0.0384\n",
      "Epoch 132: avg loss training: 3.0376,... Gradient Norm: 0.1169\n",
      "Epoch 133: avg loss training: 3.0375,... Gradient Norm: 0.1252\n",
      "Epoch 134: avg loss training: 3.0375,... Gradient Norm: 0.0636\n",
      "Epoch 135: avg loss training: 3.0374,... Gradient Norm: 0.0425\n",
      "Epoch 136: avg loss training: 3.0374,... Gradient Norm: 0.1029\n",
      "Epoch 137: avg loss training: 3.0374,... Gradient Norm: 0.1121\n",
      "Epoch 138: avg loss training: 3.0373,... Gradient Norm: 0.0789\n",
      "Epoch 139: avg loss training: 3.0373,... Gradient Norm: 0.0514\n",
      "Epoch 140: avg loss training: 3.0372,... Gradient Norm: 0.0444\n",
      "Epoch 141: avg loss training: 3.0372,... Gradient Norm: 0.0535\n",
      "Epoch 142: avg loss training: 3.0371,... Gradient Norm: 0.0928\n",
      "Epoch 143: avg loss training: 3.0371,... Gradient Norm: 0.0374\n",
      "Epoch 144: avg loss training: 3.0370,... Gradient Norm: 0.0675\n",
      "Epoch 145: avg loss training: 3.0370,... Gradient Norm: 0.0531\n",
      "Epoch 146: avg loss training: 3.0369,... Gradient Norm: 0.0530\n",
      "Epoch 147: avg loss training: 3.0369,... Gradient Norm: 0.0283\n",
      "Epoch 148: avg loss training: 3.0369,... Gradient Norm: 0.0265\n",
      "Epoch 149: avg loss training: 3.0368,... Gradient Norm: 0.0249\n",
      "Epoch 150: avg loss training: 3.0368,... Gradient Norm: 0.0476\n",
      "Epoch 151: avg loss training: 3.0367,... Gradient Norm: 0.0422\n",
      "Epoch 152: avg loss training: 3.0367,... Gradient Norm: 0.0510\n",
      "Epoch 153: avg loss training: 3.0366,... Gradient Norm: 0.0418\n",
      "Epoch 154: avg loss training: 3.0366,... Gradient Norm: 0.0873\n",
      "Epoch 155: avg loss training: 3.0366,... Gradient Norm: 0.0582\n",
      "Epoch 156: avg loss training: 3.0365,... Gradient Norm: 0.0782\n",
      "Epoch 157: avg loss training: 3.0365,... Gradient Norm: 0.0819\n",
      "Epoch 158: avg loss training: 3.0364,... Gradient Norm: 0.0688\n",
      "Epoch 159: avg loss training: 3.0364,... Gradient Norm: 0.0661\n",
      "Epoch 160: avg loss training: 3.0364,... Gradient Norm: 0.0702\n",
      "Epoch 161: avg loss training: 3.0363,... Gradient Norm: 0.0684\n",
      "Epoch 162: avg loss training: 3.0363,... Gradient Norm: 0.0622\n",
      "Epoch 163: avg loss training: 3.0363,... Gradient Norm: 0.0699\n",
      "Epoch 164: avg loss training: 3.0362,... Gradient Norm: 0.0528\n",
      "Epoch 165: avg loss training: 3.0362,... Gradient Norm: 0.0707\n",
      "Epoch 166: avg loss training: 3.0362,... Gradient Norm: 0.0954\n",
      "Epoch 167: avg loss training: 3.0361,... Gradient Norm: 0.0846\n",
      "Epoch 168: avg loss training: 3.0361,... Gradient Norm: 0.1296\n",
      "Epoch 169: avg loss training: 3.0361,... Gradient Norm: 0.1315\n",
      "Epoch 170: avg loss training: 3.0360,... Gradient Norm: 0.0440\n",
      "Epoch 171: avg loss training: 3.0360,... Gradient Norm: 0.1311\n",
      "Epoch 172: avg loss training: 3.0360,... Gradient Norm: 0.0569\n",
      "Epoch 173: avg loss training: 3.0359,... Gradient Norm: 0.1316\n",
      "Epoch 174: avg loss training: 3.0359,... Gradient Norm: 0.0592\n",
      "Epoch 175: avg loss training: 3.0359,... Gradient Norm: 0.1035\n",
      "Epoch 176: avg loss training: 3.0358,... Gradient Norm: 0.0649\n",
      "Epoch 177: avg loss training: 3.0358,... Gradient Norm: 0.0813\n",
      "Epoch 178: avg loss training: 3.0358,... Gradient Norm: 0.0876\n",
      "Epoch 179: avg loss training: 3.0357,... Gradient Norm: 0.0719\n",
      "Epoch 180: avg loss training: 3.0357,... Gradient Norm: 0.1195\n",
      "Epoch 181: avg loss training: 3.0357,... Gradient Norm: 0.0430\n",
      "Epoch 182: avg loss training: 3.0357,... Gradient Norm: 0.1081\n",
      "Epoch 183: avg loss training: 3.0356,... Gradient Norm: 0.0557\n",
      "Epoch 184: avg loss training: 3.0356,... Gradient Norm: 0.0795\n",
      "Epoch 185: avg loss training: 3.0356,... Gradient Norm: 0.1417\n",
      "Epoch 186: avg loss training: 3.0355,... Gradient Norm: 0.0988\n",
      "Epoch 187: avg loss training: 3.0355,... Gradient Norm: 0.0558\n",
      "Epoch 188: avg loss training: 3.0355,... Gradient Norm: 0.1233\n",
      "Epoch 189: avg loss training: 3.0354,... Gradient Norm: 0.0328\n",
      "Epoch 190: avg loss training: 3.0354,... Gradient Norm: 0.1053\n",
      "Epoch 191: avg loss training: 3.0354,... Gradient Norm: 0.0671\n",
      "Epoch 192: avg loss training: 3.0354,... Gradient Norm: 0.0806\n",
      "Epoch 193: avg loss training: 3.0353,... Gradient Norm: 0.0589\n",
      "Epoch 194: avg loss training: 3.0353,... Gradient Norm: 0.0632\n",
      "Epoch 195: avg loss training: 3.0353,... Gradient Norm: 0.0714\n",
      "Epoch 196: avg loss training: 3.0353,... Gradient Norm: 0.0816\n",
      "Epoch 197: avg loss training: 3.0352,... Gradient Norm: 0.0751\n",
      "Epoch 198: avg loss training: 3.0352,... Gradient Norm: 0.0633\n",
      "Epoch 199: avg loss training: 3.0352,... Gradient Norm: 0.0745\n",
      "Epoch 200: avg loss training: 3.0351,... Gradient Norm: 0.0756\n",
      "Epoch 201: avg loss training: 3.0351,... Gradient Norm: 0.0813\n",
      "Epoch 202: avg loss training: 3.0351,... Gradient Norm: 0.1082\n",
      "Epoch 203: avg loss training: 3.0351,... Gradient Norm: 0.0764\n",
      "Epoch 204: avg loss training: 3.0350,... Gradient Norm: 0.1011\n",
      "Epoch 205: avg loss training: 3.0350,... Gradient Norm: 0.1401\n",
      "Epoch 206: avg loss training: 3.0350,... Gradient Norm: 0.0511\n",
      "Epoch 207: avg loss training: 3.0350,... Gradient Norm: 0.1614\n",
      "Epoch 208: avg loss training: 3.0349,... Gradient Norm: 0.1328\n",
      "Epoch 209: avg loss training: 3.0349,... Gradient Norm: 0.0620\n",
      "Epoch 210: avg loss training: 3.0349,... Gradient Norm: 0.1065\n",
      "Epoch 211: avg loss training: 3.0349,... Gradient Norm: 0.0681\n",
      "Epoch 212: avg loss training: 3.0348,... Gradient Norm: 0.1137\n",
      "Epoch 213: avg loss training: 3.0348,... Gradient Norm: 0.1101\n",
      "Epoch 214: avg loss training: 3.0348,... Gradient Norm: 0.1520\n",
      "Epoch 215: avg loss training: 3.0348,... Gradient Norm: 0.1094\n",
      "Epoch 216: avg loss training: 3.0347,... Gradient Norm: 0.0731\n",
      "Epoch 217: avg loss training: 3.0347,... Gradient Norm: 0.0529\n",
      "Epoch 218: avg loss training: 3.0347,... Gradient Norm: 0.1278\n",
      "Epoch 219: avg loss training: 3.0347,... Gradient Norm: 0.1287\n",
      "Epoch 220: avg loss training: 3.0346,... Gradient Norm: 0.0609\n",
      "Epoch 221: avg loss training: 3.0346,... Gradient Norm: 0.1739\n",
      "Epoch 222: avg loss training: 3.0346,... Gradient Norm: 0.1360\n",
      "Epoch 223: avg loss training: 3.0346,... Gradient Norm: 0.1944\n",
      "Epoch 224: avg loss training: 3.0346,... Gradient Norm: 0.2447\n",
      "Epoch 225: avg loss training: 3.0346,... Gradient Norm: 0.1486\n",
      "Epoch 226: avg loss training: 3.0345,... Gradient Norm: 0.2869\n",
      "Epoch 227: avg loss training: 3.0345,... Gradient Norm: 0.1047\n",
      "Epoch 228: avg loss training: 3.0345,... Gradient Norm: 0.2652\n",
      "Epoch 229: avg loss training: 3.0345,... Gradient Norm: 0.2091\n",
      "Epoch 230: avg loss training: 3.0345,... Gradient Norm: 0.2140\n",
      "Epoch 231: avg loss training: 3.0344,... Gradient Norm: 0.2224\n",
      "Epoch 232: avg loss training: 3.0344,... Gradient Norm: 0.1636\n",
      "Epoch 233: avg loss training: 3.0344,... Gradient Norm: 0.2086\n",
      "Epoch 234: avg loss training: 3.0344,... Gradient Norm: 0.1486\n",
      "Epoch 235: avg loss training: 3.0344,... Gradient Norm: 0.0899\n",
      "Epoch 236: avg loss training: 3.0344,... Gradient Norm: 0.1427\n",
      "Epoch 237: avg loss training: 3.0343,... Gradient Norm: 0.0864\n",
      "Epoch 238: avg loss training: 3.0343,... Gradient Norm: 0.1385\n",
      "Epoch 239: avg loss training: 3.0343,... Gradient Norm: 0.1695\n",
      "Epoch 240: avg loss training: 3.0342,... Gradient Norm: 0.0894\n",
      "Epoch 241: avg loss training: 3.0342,... Gradient Norm: 0.1459\n",
      "Epoch 242: avg loss training: 3.0342,... Gradient Norm: 0.1018\n",
      "Epoch 243: avg loss training: 3.0342,... Gradient Norm: 0.1259\n",
      "Epoch 244: avg loss training: 3.0342,... Gradient Norm: 0.1492\n",
      "Epoch 245: avg loss training: 3.0342,... Gradient Norm: 0.0801\n",
      "Epoch 246: avg loss training: 3.0341,... Gradient Norm: 0.1599\n",
      "Epoch 247: avg loss training: 3.0341,... Gradient Norm: 0.0653\n",
      "Epoch 248: avg loss training: 3.0341,... Gradient Norm: 0.1689\n",
      "Epoch 249: avg loss training: 3.0341,... Gradient Norm: 0.1136\n",
      "Epoch 250: avg loss training: 3.0341,... Gradient Norm: 0.2228\n",
      "Epoch 251: avg loss training: 3.0341,... Gradient Norm: 0.0778\n",
      "Epoch 252: avg loss training: 3.0340,... Gradient Norm: 0.1431\n",
      "Epoch 253: avg loss training: 3.0340,... Gradient Norm: 0.1290\n",
      "Epoch 254: avg loss training: 3.0340,... Gradient Norm: 0.0714\n",
      "Epoch 255: avg loss training: 3.0340,... Gradient Norm: 0.1120\n",
      "Epoch 256: avg loss training: 3.0340,... Gradient Norm: 0.0868\n",
      "Epoch 257: avg loss training: 3.0340,... Gradient Norm: 0.1516\n",
      "Epoch 258: avg loss training: 3.0339,... Gradient Norm: 0.1105\n",
      "Epoch 259: avg loss training: 3.0339,... Gradient Norm: 0.1377\n",
      "Epoch 260: avg loss training: 3.0339,... Gradient Norm: 0.0818\n",
      "Epoch 261: avg loss training: 3.0339,... Gradient Norm: 0.1260\n",
      "Epoch 262: avg loss training: 3.0339,... Gradient Norm: 0.0956\n",
      "Epoch 263: avg loss training: 3.0339,... Gradient Norm: 0.1447\n",
      "Epoch 264: avg loss training: 3.0338,... Gradient Norm: 0.1209\n",
      "Epoch 265: avg loss training: 3.0338,... Gradient Norm: 0.1035\n",
      "Epoch 266: avg loss training: 3.0338,... Gradient Norm: 0.1007\n",
      "Epoch 267: avg loss training: 3.0338,... Gradient Norm: 0.1596\n",
      "Epoch 268: avg loss training: 3.0338,... Gradient Norm: 0.1853\n",
      "Epoch 269: avg loss training: 3.0338,... Gradient Norm: 0.0454\n",
      "Epoch 270: avg loss training: 3.0338,... Gradient Norm: 0.1880\n",
      "Epoch 271: avg loss training: 3.0337,... Gradient Norm: 0.1168\n",
      "Epoch 272: avg loss training: 3.0337,... Gradient Norm: 0.1214\n",
      "Epoch 273: avg loss training: 3.0337,... Gradient Norm: 0.1555\n",
      "Epoch 274: avg loss training: 3.0337,... Gradient Norm: 0.1404\n",
      "Epoch 275: avg loss training: 3.0337,... Gradient Norm: 0.1785\n",
      "Epoch 276: avg loss training: 3.0337,... Gradient Norm: 0.1250\n",
      "Epoch 277: avg loss training: 3.0337,... Gradient Norm: 0.1781\n",
      "Epoch 278: avg loss training: 3.0337,... Gradient Norm: 0.1730\n",
      "Epoch 279: avg loss training: 3.0336,... Gradient Norm: 0.2501\n",
      "Epoch 280: avg loss training: 3.0336,... Gradient Norm: 0.2090\n",
      "Epoch 281: avg loss training: 3.0336,... Gradient Norm: 0.2062\n",
      "Epoch 282: avg loss training: 3.0336,... Gradient Norm: 0.2417\n",
      "Epoch 283: avg loss training: 3.0336,... Gradient Norm: 0.1138\n",
      "Epoch 284: avg loss training: 3.0336,... Gradient Norm: 0.2738\n",
      "Epoch 285: avg loss training: 3.0336,... Gradient Norm: 0.2084\n",
      "Epoch 286: avg loss training: 3.0336,... Gradient Norm: 0.1897\n",
      "Epoch 287: avg loss training: 3.0335,... Gradient Norm: 0.2624\n",
      "Epoch 288: avg loss training: 3.0335,... Gradient Norm: 0.1460\n",
      "Epoch 289: avg loss training: 3.0335,... Gradient Norm: 0.3254\n",
      "Epoch 290: avg loss training: 3.0335,... Gradient Norm: 0.1399\n",
      "Epoch 291: avg loss training: 3.0335,... Gradient Norm: 0.2987\n",
      "Epoch 292: avg loss training: 3.0335,... Gradient Norm: 0.1799\n",
      "Epoch 293: avg loss training: 3.0335,... Gradient Norm: 0.1684\n",
      "Epoch 294: avg loss training: 3.0335,... Gradient Norm: 0.2023\n",
      "Epoch 295: avg loss training: 3.0334,... Gradient Norm: 0.1356\n",
      "Epoch 296: avg loss training: 3.0334,... Gradient Norm: 0.1506\n",
      "Epoch 297: avg loss training: 3.0334,... Gradient Norm: 0.1733\n",
      "Epoch 298: avg loss training: 3.0334,... Gradient Norm: 0.0621\n",
      "Epoch 299: avg loss training: 3.0334,... Gradient Norm: 0.1710\n",
      "Epoch 300: avg loss training: 3.0334,... Gradient Norm: 0.2180\n",
      "Epoch 301: avg loss training: 3.0334,... Gradient Norm: 0.1648\n",
      "Epoch 302: avg loss training: 3.0334,... Gradient Norm: 0.1779\n",
      "Epoch 303: avg loss training: 3.0334,... Gradient Norm: 0.2093\n",
      "Epoch 304: avg loss training: 3.0333,... Gradient Norm: 0.1675\n",
      "Epoch 305: avg loss training: 3.0333,... Gradient Norm: 0.2460\n",
      "Epoch 306: avg loss training: 3.0333,... Gradient Norm: 0.1690\n",
      "Epoch 307: avg loss training: 3.0333,... Gradient Norm: 0.2082\n",
      "Epoch 308: avg loss training: 3.0333,... Gradient Norm: 0.1780\n",
      "Epoch 309: avg loss training: 3.0333,... Gradient Norm: 0.1268\n",
      "Epoch 310: avg loss training: 3.0333,... Gradient Norm: 0.1101\n",
      "Epoch 311: avg loss training: 3.0333,... Gradient Norm: 0.1644\n",
      "Epoch 312: avg loss training: 3.0333,... Gradient Norm: 0.1539\n",
      "Epoch 313: avg loss training: 3.0332,... Gradient Norm: 0.1214\n",
      "Epoch 314: avg loss training: 3.0332,... Gradient Norm: 0.1233\n",
      "Epoch 315: avg loss training: 3.0332,... Gradient Norm: 0.0917\n",
      "Epoch 316: avg loss training: 3.0332,... Gradient Norm: 0.1717\n",
      "Epoch 317: avg loss training: 3.0332,... Gradient Norm: 0.1656\n",
      "Epoch 318: avg loss training: 3.0332,... Gradient Norm: 0.2055\n",
      "Epoch 319: avg loss training: 3.0332,... Gradient Norm: 0.1303\n",
      "Epoch 320: avg loss training: 3.0332,... Gradient Norm: 0.1828\n",
      "Epoch 321: avg loss training: 3.0332,... Gradient Norm: 0.2023\n",
      "Epoch 322: avg loss training: 3.0332,... Gradient Norm: 0.1643\n",
      "Epoch 323: avg loss training: 3.0332,... Gradient Norm: 0.2118\n",
      "Epoch 324: avg loss training: 3.0331,... Gradient Norm: 0.0609\n",
      "Epoch 325: avg loss training: 3.0331,... Gradient Norm: 0.2556\n",
      "Epoch 326: avg loss training: 3.0331,... Gradient Norm: 0.0991\n",
      "Epoch 327: avg loss training: 3.0331,... Gradient Norm: 0.1438\n",
      "Epoch 328: avg loss training: 3.0331,... Gradient Norm: 0.1740\n",
      "Epoch 329: avg loss training: 3.0331,... Gradient Norm: 0.0754\n",
      "Epoch 330: avg loss training: 3.0331,... Gradient Norm: 0.1068\n",
      "Epoch 331: avg loss training: 3.0331,... Gradient Norm: 0.1331\n",
      "Epoch 332: avg loss training: 3.0331,... Gradient Norm: 0.1849\n",
      "Epoch 333: avg loss training: 3.0331,... Gradient Norm: 0.1165\n",
      "Epoch 334: avg loss training: 3.0330,... Gradient Norm: 0.1763\n",
      "Epoch 335: avg loss training: 3.0330,... Gradient Norm: 0.1515\n",
      "Epoch 336: avg loss training: 3.0330,... Gradient Norm: 0.0871\n",
      "Epoch 337: avg loss training: 3.0330,... Gradient Norm: 0.1016\n",
      "Epoch 338: avg loss training: 3.0330,... Gradient Norm: 0.1388\n",
      "Epoch 339: avg loss training: 3.0330,... Gradient Norm: 0.1619\n",
      "Epoch 340: avg loss training: 3.0330,... Gradient Norm: 0.0990\n",
      "Epoch 341: avg loss training: 3.0330,... Gradient Norm: 0.1763\n",
      "Epoch 342: avg loss training: 3.0330,... Gradient Norm: 0.0557\n",
      "Epoch 343: avg loss training: 3.0330,... Gradient Norm: 0.1147\n",
      "Epoch 344: avg loss training: 3.0330,... Gradient Norm: 0.0430\n",
      "Epoch 345: avg loss training: 3.0330,... Gradient Norm: 0.1501\n",
      "Epoch 346: avg loss training: 3.0330,... Gradient Norm: 0.1476\n",
      "Epoch 347: avg loss training: 3.0329,... Gradient Norm: 0.0771\n",
      "Epoch 348: avg loss training: 3.0329,... Gradient Norm: 0.1870\n",
      "Epoch 349: avg loss training: 3.0329,... Gradient Norm: 0.0319\n",
      "Epoch 350: avg loss training: 3.0329,... Gradient Norm: 0.1356\n",
      "Epoch 351: avg loss training: 3.0329,... Gradient Norm: 0.0510\n",
      "Epoch 352: avg loss training: 3.0329,... Gradient Norm: 0.1634\n",
      "Epoch 353: avg loss training: 3.0329,... Gradient Norm: 0.1057\n",
      "Epoch 354: avg loss training: 3.0329,... Gradient Norm: 0.0466\n",
      "Epoch 355: avg loss training: 3.0329,... Gradient Norm: 0.1564\n",
      "Epoch 356: avg loss training: 3.0329,... Gradient Norm: 0.1471\n",
      "Epoch 357: avg loss training: 3.0329,... Gradient Norm: 0.0711\n",
      "Epoch 358: avg loss training: 3.0329,... Gradient Norm: 0.1847\n",
      "Epoch 359: avg loss training: 3.0329,... Gradient Norm: 0.0344\n",
      "Epoch 360: avg loss training: 3.0329,... Gradient Norm: 0.1333\n",
      "Epoch 361: avg loss training: 3.0328,... Gradient Norm: 0.0650\n",
      "Epoch 362: avg loss training: 3.0328,... Gradient Norm: 0.1424\n",
      "Epoch 363: avg loss training: 3.0328,... Gradient Norm: 0.1607\n",
      "Epoch 364: avg loss training: 3.0328,... Gradient Norm: 0.1396\n",
      "Epoch 365: avg loss training: 3.0328,... Gradient Norm: 0.1951\n",
      "Epoch 366: avg loss training: 3.0328,... Gradient Norm: 0.1710\n",
      "Epoch 367: avg loss training: 3.0328,... Gradient Norm: 0.1895\n",
      "Epoch 368: avg loss training: 3.0328,... Gradient Norm: 0.1717\n",
      "Epoch 369: avg loss training: 3.0328,... Gradient Norm: 0.0554\n",
      "Epoch 370: avg loss training: 3.0328,... Gradient Norm: 0.2224\n",
      "Epoch 371: avg loss training: 3.0328,... Gradient Norm: 0.1590\n",
      "Epoch 372: avg loss training: 3.0328,... Gradient Norm: 0.1731\n",
      "Epoch 373: avg loss training: 3.0328,... Gradient Norm: 0.2018\n",
      "Epoch 374: avg loss training: 3.0328,... Gradient Norm: 0.0617\n",
      "Epoch 375: avg loss training: 3.0328,... Gradient Norm: 0.2130\n",
      "Epoch 376: avg loss training: 3.0328,... Gradient Norm: 0.1937\n",
      "Epoch 377: avg loss training: 3.0328,... Gradient Norm: 0.0874\n",
      "Epoch 378: avg loss training: 3.0328,... Gradient Norm: 0.1871\n",
      "Epoch 379: avg loss training: 3.0327,... Gradient Norm: 0.1103\n",
      "Epoch 380: avg loss training: 3.0327,... Gradient Norm: 0.0647\n",
      "Epoch 381: avg loss training: 3.0327,... Gradient Norm: 0.2136\n",
      "Epoch 382: avg loss training: 3.0327,... Gradient Norm: 0.1804\n",
      "Epoch 383: avg loss training: 3.0327,... Gradient Norm: 0.1553\n",
      "Epoch 384: avg loss training: 3.0327,... Gradient Norm: 0.2130\n",
      "Epoch 385: avg loss training: 3.0327,... Gradient Norm: 0.1428\n",
      "Epoch 386: avg loss training: 3.0327,... Gradient Norm: 0.2129\n",
      "Epoch 387: avg loss training: 3.0327,... Gradient Norm: 0.2219\n",
      "Epoch 388: avg loss training: 3.0327,... Gradient Norm: 0.0522\n",
      "Epoch 389: avg loss training: 3.0327,... Gradient Norm: 0.1897\n",
      "Epoch 390: avg loss training: 3.0327,... Gradient Norm: 0.1466\n",
      "Epoch 391: avg loss training: 3.0327,... Gradient Norm: 0.0489\n",
      "Epoch 392: avg loss training: 3.0327,... Gradient Norm: 0.2192\n",
      "Epoch 393: avg loss training: 3.0327,... Gradient Norm: 0.1882\n",
      "Epoch 394: avg loss training: 3.0327,... Gradient Norm: 0.1645\n",
      "Epoch 395: avg loss training: 3.0327,... Gradient Norm: 0.1870\n",
      "Epoch 396: avg loss training: 3.0327,... Gradient Norm: 0.1354\n",
      "Epoch 397: avg loss training: 3.0326,... Gradient Norm: 0.1953\n",
      "Epoch 398: avg loss training: 3.0326,... Gradient Norm: 0.2028\n",
      "Epoch 399: avg loss training: 3.0326,... Gradient Norm: 0.0585\n",
      "Epoch 400: avg loss training: 3.0326,... Gradient Norm: 0.1454\n",
      "Epoch 401: avg loss training: 3.0326,... Gradient Norm: 0.1027\n",
      "Epoch 402: avg loss training: 3.0326,... Gradient Norm: 0.0406\n",
      "Epoch 403: avg loss training: 3.0326,... Gradient Norm: 0.1938\n",
      "Epoch 404: avg loss training: 3.0326,... Gradient Norm: 0.0468\n",
      "Epoch 405: avg loss training: 3.0326,... Gradient Norm: 0.1095\n",
      "Epoch 406: avg loss training: 3.0326,... Gradient Norm: 0.0656\n",
      "Epoch 407: avg loss training: 3.0326,... Gradient Norm: 0.1529\n",
      "Epoch 408: avg loss training: 3.0326,... Gradient Norm: 0.0377\n",
      "Epoch 409: avg loss training: 3.0326,... Gradient Norm: 0.1094\n",
      "Epoch 410: avg loss training: 3.0326,... Gradient Norm: 0.0880\n",
      "Epoch 411: avg loss training: 3.0326,... Gradient Norm: 0.0373\n",
      "Epoch 412: avg loss training: 3.0326,... Gradient Norm: 0.0533\n",
      "Epoch 413: avg loss training: 3.0326,... Gradient Norm: 0.1287\n",
      "Epoch 414: avg loss training: 3.0326,... Gradient Norm: 0.1537\n",
      "Epoch 415: avg loss training: 3.0326,... Gradient Norm: 0.0735\n",
      "Epoch 416: avg loss training: 3.0326,... Gradient Norm: 0.1722\n",
      "Epoch 417: avg loss training: 3.0325,... Gradient Norm: 0.1537\n",
      "Epoch 418: avg loss training: 3.0325,... Gradient Norm: 0.1513\n",
      "Epoch 419: avg loss training: 3.0325,... Gradient Norm: 0.1507\n",
      "Epoch 420: avg loss training: 3.0325,... Gradient Norm: 0.0893\n",
      "Epoch 421: avg loss training: 3.0325,... Gradient Norm: 0.2036\n",
      "Epoch 422: avg loss training: 3.0325,... Gradient Norm: 0.1887\n",
      "Epoch 423: avg loss training: 3.0325,... Gradient Norm: 0.0946\n",
      "Epoch 424: avg loss training: 3.0325,... Gradient Norm: 0.1711\n",
      "Epoch 425: avg loss training: 3.0325,... Gradient Norm: 0.1268\n",
      "Epoch 426: avg loss training: 3.0325,... Gradient Norm: 0.1838\n",
      "Epoch 427: avg loss training: 3.0325,... Gradient Norm: 0.1580\n",
      "Epoch 428: avg loss training: 3.0325,... Gradient Norm: 0.0799\n",
      "Epoch 429: avg loss training: 3.0325,... Gradient Norm: 0.1417\n",
      "Epoch 430: avg loss training: 3.0325,... Gradient Norm: 0.0698\n",
      "Epoch 431: avg loss training: 3.0325,... Gradient Norm: 0.1654\n",
      "Epoch 432: avg loss training: 3.0325,... Gradient Norm: 0.1279\n",
      "Epoch 433: avg loss training: 3.0325,... Gradient Norm: 0.1420\n",
      "Epoch 434: avg loss training: 3.0325,... Gradient Norm: 0.1330\n",
      "Epoch 435: avg loss training: 3.0325,... Gradient Norm: 0.0410\n",
      "Epoch 436: avg loss training: 3.0325,... Gradient Norm: 0.1929\n",
      "Epoch 437: avg loss training: 3.0325,... Gradient Norm: 0.1573\n",
      "Epoch 438: avg loss training: 3.0325,... Gradient Norm: 0.1479\n",
      "Epoch 439: avg loss training: 3.0325,... Gradient Norm: 0.1580\n",
      "Epoch 440: avg loss training: 3.0324,... Gradient Norm: 0.1316\n",
      "Epoch 441: avg loss training: 3.0325,... Gradient Norm: 0.1897\n",
      "Epoch 442: avg loss training: 3.0325,... Gradient Norm: 0.2047\n",
      "Epoch 443: avg loss training: 3.0324,... Gradient Norm: 0.0423\n",
      "Epoch 444: avg loss training: 3.0324,... Gradient Norm: 0.1557\n",
      "Epoch 445: avg loss training: 3.0324,... Gradient Norm: 0.1424\n",
      "Epoch 446: avg loss training: 3.0324,... Gradient Norm: 0.1127\n",
      "Epoch 447: avg loss training: 3.0324,... Gradient Norm: 0.1675\n",
      "Epoch 448: avg loss training: 3.0324,... Gradient Norm: 0.0670\n",
      "Epoch 449: avg loss training: 3.0324,... Gradient Norm: 0.1272\n",
      "Epoch 450: avg loss training: 3.0324,... Gradient Norm: 0.0635\n",
      "Epoch 451: avg loss training: 3.0324,... Gradient Norm: 0.1659\n",
      "Epoch 452: avg loss training: 3.0324,... Gradient Norm: 0.1502\n",
      "Epoch 453: avg loss training: 3.0324,... Gradient Norm: 0.1539\n",
      "Epoch 454: avg loss training: 3.0324,... Gradient Norm: 0.1437\n",
      "Epoch 455: avg loss training: 3.0324,... Gradient Norm: 0.0870\n",
      "Epoch 456: avg loss training: 3.0324,... Gradient Norm: 0.1952\n",
      "Epoch 457: avg loss training: 3.0324,... Gradient Norm: 0.1906\n",
      "Epoch 458: avg loss training: 3.0324,... Gradient Norm: 0.0698\n",
      "Epoch 459: avg loss training: 3.0324,... Gradient Norm: 0.1485\n",
      "Epoch 460: avg loss training: 3.0324,... Gradient Norm: 0.1124\n",
      "Epoch 461: avg loss training: 3.0324,... Gradient Norm: 0.1265\n",
      "Epoch 462: avg loss training: 3.0324,... Gradient Norm: 0.1544\n",
      "Epoch 463: avg loss training: 3.0324,... Gradient Norm: 0.0935\n",
      "Epoch 464: avg loss training: 3.0324,... Gradient Norm: 0.1252\n",
      "Epoch 465: avg loss training: 3.0324,... Gradient Norm: 0.0363\n",
      "Epoch 466: avg loss training: 3.0324,... Gradient Norm: 0.1709\n",
      "Epoch 467: avg loss training: 3.0324,... Gradient Norm: 0.1260\n",
      "Epoch 468: avg loss training: 3.0324,... Gradient Norm: 0.1492\n",
      "Epoch 469: avg loss training: 3.0324,... Gradient Norm: 0.1492\n",
      "Epoch 470: avg loss training: 3.0323,... Gradient Norm: 0.0787\n",
      "Epoch 471: avg loss training: 3.0324,... Gradient Norm: 0.1931\n",
      "Epoch 472: avg loss training: 3.0323,... Gradient Norm: 0.1874\n",
      "Epoch 473: avg loss training: 3.0323,... Gradient Norm: 0.0962\n",
      "Epoch 474: avg loss training: 3.0323,... Gradient Norm: 0.1487\n",
      "Epoch 475: avg loss training: 3.0323,... Gradient Norm: 0.1102\n",
      "Epoch 476: avg loss training: 3.0323,... Gradient Norm: 0.1680\n",
      "Epoch 477: avg loss training: 3.0323,... Gradient Norm: 0.1687\n",
      "Epoch 478: avg loss training: 3.0323,... Gradient Norm: 0.1029\n",
      "Epoch 479: avg loss training: 3.0323,... Gradient Norm: 0.1365\n",
      "Epoch 480: avg loss training: 3.0323,... Gradient Norm: 0.0399\n",
      "Epoch 481: avg loss training: 3.0323,... Gradient Norm: 0.1738\n",
      "Epoch 482: avg loss training: 3.0323,... Gradient Norm: 0.1301\n",
      "Epoch 483: avg loss training: 3.0323,... Gradient Norm: 0.1410\n",
      "Epoch 484: avg loss training: 3.0323,... Gradient Norm: 0.1467\n",
      "Epoch 485: avg loss training: 3.0323,... Gradient Norm: 0.0765\n",
      "Epoch 486: avg loss training: 3.0323,... Gradient Norm: 0.1945\n",
      "Epoch 487: avg loss training: 3.0323,... Gradient Norm: 0.1773\n",
      "Epoch 488: avg loss training: 3.0323,... Gradient Norm: 0.1200\n",
      "Epoch 489: avg loss training: 3.0323,... Gradient Norm: 0.1404\n",
      "Epoch 490: avg loss training: 3.0323,... Gradient Norm: 0.0740\n",
      "Epoch 491: avg loss training: 3.0323,... Gradient Norm: 0.1757\n",
      "Epoch 492: avg loss training: 3.0323,... Gradient Norm: 0.1376\n",
      "Epoch 493: avg loss training: 3.0323,... Gradient Norm: 0.1345\n",
      "Epoch 494: avg loss training: 3.0323,... Gradient Norm: 0.1386\n",
      "Epoch 495: avg loss training: 3.0323,... Gradient Norm: 0.0463\n",
      "Epoch 496: avg loss training: 3.0323,... Gradient Norm: 0.1710\n",
      "Epoch 497: avg loss training: 3.0323,... Gradient Norm: 0.0494\n",
      "Epoch 498: avg loss training: 3.0323,... Gradient Norm: 0.0845\n",
      "Epoch 499: avg loss training: 3.0323,... Gradient Norm: 0.0467\n",
      "Epoch 500: avg loss training: 3.0323,... Gradient Norm: 0.1587\n",
      "Epoch 501: avg loss training: 3.0323,... Gradient Norm: 0.0850\n",
      "Epoch 502: avg loss training: 3.0323,... Gradient Norm: 0.0659\n",
      "Epoch 503: avg loss training: 3.0323,... Gradient Norm: 0.1620\n",
      "Epoch 504: avg loss training: 3.0323,... Gradient Norm: 0.1328\n",
      "Epoch 505: avg loss training: 3.0323,... Gradient Norm: 0.0852\n",
      "Epoch 506: avg loss training: 3.0323,... Gradient Norm: 0.0358\n",
      "Epoch 507: avg loss training: 3.0323,... Gradient Norm: 0.1358\n",
      "Epoch 508: avg loss training: 3.0323,... Gradient Norm: 0.1753\n",
      "Epoch 509: avg loss training: 3.0322,... Gradient Norm: 0.0661\n",
      "Epoch 510: avg loss training: 3.0322,... Gradient Norm: 0.0659\n",
      "Epoch 511: avg loss training: 3.0322,... Gradient Norm: 0.1681\n",
      "Epoch 512: avg loss training: 3.0322,... Gradient Norm: 0.1325\n",
      "Epoch 513: avg loss training: 3.0322,... Gradient Norm: 0.0934\n",
      "Epoch 514: avg loss training: 3.0322,... Gradient Norm: 0.1890\n",
      "Epoch 515: avg loss training: 3.0322,... Gradient Norm: 0.1593\n",
      "Epoch 516: avg loss training: 3.0322,... Gradient Norm: 0.1497\n",
      "Epoch 517: avg loss training: 3.0322,... Gradient Norm: 0.1521\n",
      "Epoch 518: avg loss training: 3.0322,... Gradient Norm: 0.1386\n",
      "Epoch 519: avg loss training: 3.0322,... Gradient Norm: 0.1842\n",
      "Epoch 520: avg loss training: 3.0322,... Gradient Norm: 0.2034\n",
      "Epoch 521: avg loss training: 3.0322,... Gradient Norm: 0.0358\n",
      "Epoch 522: avg loss training: 3.0322,... Gradient Norm: 0.1436\n",
      "Epoch 523: avg loss training: 3.0322,... Gradient Norm: 0.1431\n",
      "Epoch 524: avg loss training: 3.0322,... Gradient Norm: 0.1349\n",
      "Epoch 525: avg loss training: 3.0322,... Gradient Norm: 0.1661\n",
      "Epoch 526: avg loss training: 3.0322,... Gradient Norm: 0.1083\n",
      "Epoch 527: avg loss training: 3.0322,... Gradient Norm: 0.1262\n",
      "Epoch 528: avg loss training: 3.0322,... Gradient Norm: 0.0484\n",
      "Epoch 529: avg loss training: 3.0322,... Gradient Norm: 0.1815\n",
      "Epoch 530: avg loss training: 3.0322,... Gradient Norm: 0.1676\n",
      "Epoch 531: avg loss training: 3.0322,... Gradient Norm: 0.1383\n",
      "Epoch 532: avg loss training: 3.0322,... Gradient Norm: 0.1510\n",
      "Epoch 533: avg loss training: 3.0322,... Gradient Norm: 0.1178\n",
      "Epoch 534: avg loss training: 3.0322,... Gradient Norm: 0.1784\n",
      "Epoch 535: avg loss training: 3.0322,... Gradient Norm: 0.1798\n",
      "Epoch 536: avg loss training: 3.0322,... Gradient Norm: 0.0657\n",
      "Epoch 537: avg loss training: 3.0322,... Gradient Norm: 0.1216\n",
      "Epoch 538: avg loss training: 3.0322,... Gradient Norm: 0.1111\n",
      "Epoch 539: avg loss training: 3.0322,... Gradient Norm: 0.0369\n",
      "Epoch 540: avg loss training: 3.0322,... Gradient Norm: 0.1884\n",
      "Epoch 541: avg loss training: 3.0322,... Gradient Norm: 0.1804\n",
      "Epoch 542: avg loss training: 3.0322,... Gradient Norm: 0.0837\n",
      "Epoch 543: avg loss training: 3.0322,... Gradient Norm: 0.1412\n",
      "Epoch 544: avg loss training: 3.0322,... Gradient Norm: 0.1292\n",
      "Epoch 545: avg loss training: 3.0322,... Gradient Norm: 0.0345\n",
      "Epoch 546: avg loss training: 3.0322,... Gradient Norm: 0.1928\n",
      "Epoch 547: avg loss training: 3.0322,... Gradient Norm: 0.1818\n",
      "Epoch 548: avg loss training: 3.0322,... Gradient Norm: 0.0895\n",
      "Epoch 549: avg loss training: 3.0322,... Gradient Norm: 0.1323\n",
      "Epoch 550: avg loss training: 3.0322,... Gradient Norm: 0.1245\n",
      "Epoch 551: avg loss training: 3.0322,... Gradient Norm: 0.0459\n",
      "Epoch 552: avg loss training: 3.0322,... Gradient Norm: 0.1747\n",
      "Epoch 553: avg loss training: 3.0322,... Gradient Norm: 0.1651\n",
      "Epoch 554: avg loss training: 3.0322,... Gradient Norm: 0.1192\n",
      "Epoch 555: avg loss training: 3.0322,... Gradient Norm: 0.1344\n",
      "Epoch 556: avg loss training: 3.0322,... Gradient Norm: 0.1231\n",
      "Epoch 557: avg loss training: 3.0322,... Gradient Norm: 0.0510\n",
      "Epoch 558: avg loss training: 3.0322,... Gradient Norm: 0.1868\n",
      "Epoch 559: avg loss training: 3.0322,... Gradient Norm: 0.1803\n",
      "Epoch 560: avg loss training: 3.0322,... Gradient Norm: 0.0599\n",
      "Epoch 561: avg loss training: 3.0322,... Gradient Norm: 0.1097\n",
      "Epoch 562: avg loss training: 3.0322,... Gradient Norm: 0.1135\n",
      "Epoch 563: avg loss training: 3.0322,... Gradient Norm: 0.0698\n",
      "Epoch 564: avg loss training: 3.0322,... Gradient Norm: 0.1651\n",
      "Epoch 565: avg loss training: 3.0322,... Gradient Norm: 0.1705\n",
      "Epoch 566: avg loss training: 3.0322,... Gradient Norm: 0.0787\n",
      "Epoch 567: avg loss training: 3.0322,... Gradient Norm: 0.1298\n",
      "Epoch 568: avg loss training: 3.0322,... Gradient Norm: 0.1161\n",
      "Epoch 569: avg loss training: 3.0322,... Gradient Norm: 0.1560\n",
      "Epoch 570: avg loss training: 3.0322,... Gradient Norm: 0.1561\n",
      "Epoch 571: avg loss training: 3.0321,... Gradient Norm: 0.0959\n",
      "Epoch 572: avg loss training: 3.0321,... Gradient Norm: 0.1243\n",
      "Epoch 573: avg loss training: 3.0321,... Gradient Norm: 0.0464\n",
      "Epoch 574: avg loss training: 3.0321,... Gradient Norm: 0.1585\n",
      "Epoch 575: avg loss training: 3.0321,... Gradient Norm: 0.0301\n",
      "Epoch 576: avg loss training: 3.0321,... Gradient Norm: 0.0596\n",
      "Epoch 577: avg loss training: 3.0321,... Gradient Norm: 0.0553\n",
      "Epoch 578: avg loss training: 3.0321,... Gradient Norm: 0.0909\n",
      "Epoch 579: avg loss training: 3.0321,... Gradient Norm: 0.0202\n",
      "Epoch 580: avg loss training: 3.0321,... Gradient Norm: 0.0641\n",
      "Epoch 581: avg loss training: 3.0321,... Gradient Norm: 0.0214\n",
      "Epoch 582: avg loss training: 3.0321,... Gradient Norm: 0.1460\n",
      "Epoch 583: avg loss training: 3.0321,... Gradient Norm: 0.1132\n",
      "Epoch 584: avg loss training: 3.0321,... Gradient Norm: 0.1123\n",
      "Epoch 585: avg loss training: 3.0321,... Gradient Norm: 0.1525\n",
      "Epoch 586: avg loss training: 3.0321,... Gradient Norm: 0.1237\n",
      "Epoch 587: avg loss training: 3.0321,... Gradient Norm: 0.1269\n",
      "Epoch 588: avg loss training: 3.0321,... Gradient Norm: 0.1143\n",
      "Epoch 589: avg loss training: 3.0321,... Gradient Norm: 0.0739\n",
      "Epoch 590: avg loss training: 3.0321,... Gradient Norm: 0.1736\n",
      "Epoch 591: avg loss training: 3.0321,... Gradient Norm: 0.1685\n",
      "Epoch 592: avg loss training: 3.0321,... Gradient Norm: 0.1018\n",
      "Epoch 593: avg loss training: 3.0321,... Gradient Norm: 0.1174\n",
      "Epoch 594: avg loss training: 3.0321,... Gradient Norm: 0.1019\n",
      "Epoch 595: avg loss training: 3.0321,... Gradient Norm: 0.1610\n",
      "Epoch 596: avg loss training: 3.0321,... Gradient Norm: 0.1605\n",
      "Epoch 597: avg loss training: 3.0321,... Gradient Norm: 0.1147\n",
      "Epoch 598: avg loss training: 3.0321,... Gradient Norm: 0.1195\n",
      "Epoch 599: avg loss training: 3.0321,... Gradient Norm: 0.0791\n",
      "Epoch 600: avg loss training: 3.0321,... Gradient Norm: 0.1650\n",
      "Epoch 601: avg loss training: 3.0321,... Gradient Norm: 0.1586\n",
      "Epoch 602: avg loss training: 3.0321,... Gradient Norm: 0.1053\n",
      "Epoch 603: avg loss training: 3.0321,... Gradient Norm: 0.1263\n",
      "Epoch 604: avg loss training: 3.0321,... Gradient Norm: 0.0921\n",
      "Epoch 605: avg loss training: 3.0321,... Gradient Norm: 0.0779\n",
      "Epoch 606: avg loss training: 3.0321,... Gradient Norm: 0.1694\n",
      "Epoch 607: avg loss training: 3.0321,... Gradient Norm: 0.0278\n",
      "Epoch 608: avg loss training: 3.0321,... Gradient Norm: 0.0665\n",
      "Epoch 609: avg loss training: 3.0321,... Gradient Norm: 0.0909\n",
      "Epoch 610: avg loss training: 3.0321,... Gradient Norm: 0.0435\n",
      "Epoch 611: avg loss training: 3.0321,... Gradient Norm: 0.1674\n",
      "Epoch 612: avg loss training: 3.0321,... Gradient Norm: 0.0251\n",
      "Epoch 613: avg loss training: 3.0321,... Gradient Norm: 0.0980\n",
      "Epoch 614: avg loss training: 3.0321,... Gradient Norm: 0.0612\n",
      "Epoch 615: avg loss training: 3.0321,... Gradient Norm: 0.1551\n",
      "Epoch 616: avg loss training: 3.0321,... Gradient Norm: 0.0424\n",
      "Epoch 617: avg loss training: 3.0321,... Gradient Norm: 0.0757\n",
      "Epoch 618: avg loss training: 3.0321,... Gradient Norm: 0.0613\n",
      "Epoch 619: avg loss training: 3.0321,... Gradient Norm: 0.0653\n",
      "Epoch 620: avg loss training: 3.0321,... Gradient Norm: 0.1033\n",
      "Epoch 621: avg loss training: 3.0321,... Gradient Norm: 0.0822\n",
      "Epoch 622: avg loss training: 3.0321,... Gradient Norm: 0.1700\n",
      "Epoch 623: avg loss training: 3.0321,... Gradient Norm: 0.1473\n",
      "Epoch 624: avg loss training: 3.0321,... Gradient Norm: 0.1325\n",
      "Epoch 625: avg loss training: 3.0321,... Gradient Norm: 0.1373\n",
      "Epoch 626: avg loss training: 3.0321,... Gradient Norm: 0.1179\n",
      "Epoch 627: avg loss training: 3.0321,... Gradient Norm: 0.1632\n",
      "Epoch 628: avg loss training: 3.0321,... Gradient Norm: 0.1689\n",
      "Epoch 629: avg loss training: 3.0321,... Gradient Norm: 0.0678\n",
      "Epoch 630: avg loss training: 3.0321,... Gradient Norm: 0.0990\n",
      "Epoch 631: avg loss training: 3.0321,... Gradient Norm: 0.0749\n",
      "Epoch 632: avg loss training: 3.0321,... Gradient Norm: 0.0715\n",
      "Epoch 633: avg loss training: 3.0321,... Gradient Norm: 0.1628\n",
      "Epoch 634: avg loss training: 3.0321,... Gradient Norm: 0.0876\n",
      "Epoch 635: avg loss training: 3.0321,... Gradient Norm: 0.1206\n",
      "Epoch 636: avg loss training: 3.0321,... Gradient Norm: 0.0246\n",
      "Epoch 637: avg loss training: 3.0321,... Gradient Norm: 0.1653\n",
      "Epoch 638: avg loss training: 3.0321,... Gradient Norm: 0.0623\n",
      "Epoch 639: avg loss training: 3.0321,... Gradient Norm: 0.1202\n",
      "Epoch 640: avg loss training: 3.0321,... Gradient Norm: 0.1213\n",
      "Epoch 641: avg loss training: 3.0321,... Gradient Norm: 0.1460\n",
      "Epoch 642: avg loss training: 3.0321,... Gradient Norm: 0.0409\n",
      "Epoch 643: avg loss training: 3.0321,... Gradient Norm: 0.0303\n",
      "Epoch 644: avg loss training: 3.0321,... Gradient Norm: 0.0744\n",
      "Epoch 645: avg loss training: 3.0321,... Gradient Norm: 0.0926\n",
      "Epoch 646: avg loss training: 3.0321,... Gradient Norm: 0.0744\n",
      "Epoch 647: avg loss training: 3.0321,... Gradient Norm: 0.0682\n",
      "Epoch 648: avg loss training: 3.0321,... Gradient Norm: 0.1615\n",
      "Epoch 649: avg loss training: 3.0321,... Gradient Norm: 0.1172\n",
      "Epoch 650: avg loss training: 3.0321,... Gradient Norm: 0.1110\n",
      "Epoch 651: avg loss training: 3.0321,... Gradient Norm: 0.1634\n",
      "Epoch 652: avg loss training: 3.0321,... Gradient Norm: 0.0281\n",
      "Epoch 653: avg loss training: 3.0321,... Gradient Norm: 0.0946\n",
      "Epoch 654: avg loss training: 3.0321,... Gradient Norm: 0.1547\n",
      "Epoch 655: avg loss training: 3.0321,... Gradient Norm: 0.1101\n",
      "Epoch 656: avg loss training: 3.0321,... Gradient Norm: 0.1077\n",
      "Epoch 657: avg loss training: 3.0321,... Gradient Norm: 0.1754\n",
      "Epoch 658: avg loss training: 3.0321,... Gradient Norm: 0.1643\n",
      "Epoch 659: avg loss training: 3.0321,... Gradient Norm: 0.1291\n",
      "Epoch 660: avg loss training: 3.0321,... Gradient Norm: 0.1295\n",
      "Epoch 661: avg loss training: 3.0321,... Gradient Norm: 0.1177\n",
      "Epoch 662: avg loss training: 3.0321,... Gradient Norm: 0.1712\n",
      "Epoch 663: avg loss training: 3.0321,... Gradient Norm: 0.1738\n",
      "Epoch 664: avg loss training: 3.0321,... Gradient Norm: 0.0728\n",
      "Epoch 665: avg loss training: 3.0321,... Gradient Norm: 0.1305\n",
      "Epoch 666: avg loss training: 3.0321,... Gradient Norm: 0.1077\n",
      "Epoch 667: avg loss training: 3.0321,... Gradient Norm: 0.0749\n",
      "Epoch 668: avg loss training: 3.0321,... Gradient Norm: 0.1719\n",
      "Epoch 669: avg loss training: 3.0321,... Gradient Norm: 0.0509\n",
      "Epoch 670: avg loss training: 3.0321,... Gradient Norm: 0.0656\n",
      "Epoch 671: avg loss training: 3.0321,... Gradient Norm: 0.0604\n",
      "Epoch 672: avg loss training: 3.0321,... Gradient Norm: 0.1561\n",
      "Epoch 673: avg loss training: 3.0321,... Gradient Norm: 0.0994\n",
      "Epoch 674: avg loss training: 3.0321,... Gradient Norm: 0.0786\n",
      "Epoch 675: avg loss training: 3.0321,... Gradient Norm: 0.1555\n",
      "Epoch 676: avg loss training: 3.0321,... Gradient Norm: 0.0356\n",
      "Epoch 677: avg loss training: 3.0321,... Gradient Norm: 0.1051\n",
      "Epoch 678: avg loss training: 3.0321,... Gradient Norm: 0.1227\n",
      "Epoch 679: avg loss training: 3.0321,... Gradient Norm: 0.1124\n",
      "Epoch 680: avg loss training: 3.0321,... Gradient Norm: 0.0799\n",
      "Epoch 681: avg loss training: 3.0321,... Gradient Norm: 0.0439\n",
      "Epoch 682: avg loss training: 3.0321,... Gradient Norm: 0.0308\n",
      "Epoch 683: avg loss training: 3.0321,... Gradient Norm: 0.1391\n",
      "Epoch 684: avg loss training: 3.0321,... Gradient Norm: 0.1204\n",
      "Epoch 685: avg loss training: 3.0321,... Gradient Norm: 0.1192\n",
      "Epoch 686: avg loss training: 3.0321,... Gradient Norm: 0.0564\n",
      "Epoch 687: avg loss training: 3.0321,... Gradient Norm: 0.1760\n",
      "Epoch 688: avg loss training: 3.0321,... Gradient Norm: 0.0579\n",
      "Epoch 689: avg loss training: 3.0321,... Gradient Norm: 0.1147\n",
      "Epoch 690: avg loss training: 3.0321,... Gradient Norm: 0.1435\n",
      "Epoch 691: avg loss training: 3.0321,... Gradient Norm: 0.1002\n",
      "Epoch 692: avg loss training: 3.0321,... Gradient Norm: 0.0246\n",
      "Epoch 693: avg loss training: 3.0321,... Gradient Norm: 0.1585\n",
      "Epoch 694: avg loss training: 3.0321,... Gradient Norm: 0.1201\n",
      "Epoch 695: avg loss training: 3.0321,... Gradient Norm: 0.1191\n",
      "Epoch 696: avg loss training: 3.0321,... Gradient Norm: 0.1732\n",
      "Epoch 697: avg loss training: 3.0321,... Gradient Norm: 0.1658\n",
      "Epoch 698: avg loss training: 3.0321,... Gradient Norm: 0.1283\n",
      "Epoch 699: avg loss training: 3.0321,... Gradient Norm: 0.1288\n",
      "Epoch 700: avg loss training: 3.0321,... Gradient Norm: 0.1300\n",
      "Epoch 701: avg loss training: 3.0321,... Gradient Norm: 0.1661\n",
      "Epoch 702: avg loss training: 3.0321,... Gradient Norm: 0.1703\n",
      "Epoch 703: avg loss training: 3.0320,... Gradient Norm: 0.0745\n",
      "Epoch 704: avg loss training: 3.0320,... Gradient Norm: 0.1085\n",
      "Epoch 705: avg loss training: 3.0320,... Gradient Norm: 0.0934\n",
      "Epoch 706: avg loss training: 3.0320,... Gradient Norm: 0.1256\n",
      "Epoch 707: avg loss training: 3.0320,... Gradient Norm: 0.1259\n",
      "Epoch 708: avg loss training: 3.0320,... Gradient Norm: 0.0896\n",
      "Epoch 709: avg loss training: 3.0320,... Gradient Norm: 0.1134\n",
      "Epoch 710: avg loss training: 3.0320,... Gradient Norm: 0.0431\n",
      "Epoch 711: avg loss training: 3.0320,... Gradient Norm: 0.1778\n",
      "Epoch 712: avg loss training: 3.0320,... Gradient Norm: 0.1678\n",
      "Epoch 713: avg loss training: 3.0320,... Gradient Norm: 0.1255\n",
      "Epoch 714: avg loss training: 3.0320,... Gradient Norm: 0.1304\n",
      "Epoch 715: avg loss training: 3.0320,... Gradient Norm: 0.1276\n",
      "Epoch 716: avg loss training: 3.0320,... Gradient Norm: 0.1528\n",
      "Epoch 717: avg loss training: 3.0320,... Gradient Norm: 0.1715\n",
      "Epoch 718: avg loss training: 3.0320,... Gradient Norm: 0.0972\n",
      "Epoch 719: avg loss training: 3.0320,... Gradient Norm: 0.1190\n",
      "Epoch 720: avg loss training: 3.0320,... Gradient Norm: 0.0567\n",
      "Epoch 721: avg loss training: 3.0320,... Gradient Norm: 0.1702\n",
      "Epoch 722: avg loss training: 3.0320,... Gradient Norm: 0.0512\n",
      "Epoch 723: avg loss training: 3.0320,... Gradient Norm: 0.0735\n",
      "Epoch 724: avg loss training: 3.0320,... Gradient Norm: 0.0788\n",
      "Epoch 725: avg loss training: 3.0320,... Gradient Norm: 0.0373\n",
      "Epoch 726: avg loss training: 3.0320,... Gradient Norm: 0.1708\n",
      "Epoch 727: avg loss training: 3.0320,... Gradient Norm: 0.0986\n",
      "Epoch 728: avg loss training: 3.0320,... Gradient Norm: 0.1038\n",
      "Epoch 729: avg loss training: 3.0320,... Gradient Norm: 0.1534\n",
      "Epoch 730: avg loss training: 3.0320,... Gradient Norm: 0.0526\n",
      "Epoch 731: avg loss training: 3.0320,... Gradient Norm: 0.0968\n",
      "Epoch 732: avg loss training: 3.0320,... Gradient Norm: 0.1675\n",
      "Epoch 733: avg loss training: 3.0320,... Gradient Norm: 0.0721\n",
      "Epoch 734: avg loss training: 3.0320,... Gradient Norm: 0.0924\n",
      "Epoch 735: avg loss training: 3.0320,... Gradient Norm: 0.1591\n",
      "Epoch 736: avg loss training: 3.0320,... Gradient Norm: 0.0782\n",
      "Epoch 737: avg loss training: 3.0320,... Gradient Norm: 0.0580\n",
      "Epoch 738: avg loss training: 3.0320,... Gradient Norm: 0.1355\n",
      "Epoch 739: avg loss training: 3.0320,... Gradient Norm: 0.1047\n",
      "Epoch 740: avg loss training: 3.0320,... Gradient Norm: 0.0242\n",
      "Epoch 741: avg loss training: 3.0320,... Gradient Norm: 0.1589\n",
      "Epoch 742: avg loss training: 3.0320,... Gradient Norm: 0.1281\n",
      "Epoch 743: avg loss training: 3.0320,... Gradient Norm: 0.1194\n",
      "Epoch 744: avg loss training: 3.0320,... Gradient Norm: 0.1685\n",
      "Epoch 745: avg loss training: 3.0320,... Gradient Norm: 0.0293\n",
      "Epoch 746: avg loss training: 3.0320,... Gradient Norm: 0.0791\n",
      "Epoch 747: avg loss training: 3.0320,... Gradient Norm: 0.0418\n",
      "Epoch 748: avg loss training: 3.0320,... Gradient Norm: 0.1721\n",
      "Epoch 749: avg loss training: 3.0320,... Gradient Norm: 0.1035\n",
      "Epoch 750: avg loss training: 3.0320,... Gradient Norm: 0.0952\n",
      "Epoch 751: avg loss training: 3.0320,... Gradient Norm: 0.1656\n",
      "Epoch 752: avg loss training: 3.0320,... Gradient Norm: 0.0587\n",
      "Epoch 753: avg loss training: 3.0320,... Gradient Norm: 0.1280\n",
      "Epoch 754: avg loss training: 3.0320,... Gradient Norm: 0.1189\n",
      "Epoch 755: avg loss training: 3.0320,... Gradient Norm: 0.1749\n",
      "Epoch 756: avg loss training: 3.0320,... Gradient Norm: 0.1706\n",
      "Epoch 757: avg loss training: 3.0320,... Gradient Norm: 0.1279\n",
      "Epoch 758: avg loss training: 3.0320,... Gradient Norm: 0.1277\n",
      "Epoch 759: avg loss training: 3.0320,... Gradient Norm: 0.1133\n",
      "Epoch 760: avg loss training: 3.0320,... Gradient Norm: 0.1750\n",
      "Epoch 761: avg loss training: 3.0320,... Gradient Norm: 0.1750\n",
      "Epoch 762: avg loss training: 3.0320,... Gradient Norm: 0.0801\n",
      "Epoch 763: avg loss training: 3.0320,... Gradient Norm: 0.1282\n",
      "Epoch 764: avg loss training: 3.0320,... Gradient Norm: 0.1168\n",
      "Epoch 765: avg loss training: 3.0320,... Gradient Norm: 0.1236\n",
      "Epoch 766: avg loss training: 3.0320,... Gradient Norm: 0.1745\n",
      "Epoch 767: avg loss training: 3.0320,... Gradient Norm: 0.0697\n",
      "Epoch 768: avg loss training: 3.0320,... Gradient Norm: 0.1063\n",
      "Epoch 769: avg loss training: 3.0320,... Gradient Norm: 0.0739\n",
      "Epoch 770: avg loss training: 3.0320,... Gradient Norm: 0.1648\n",
      "Epoch 771: avg loss training: 3.0320,... Gradient Norm: 0.0438\n",
      "Epoch 772: avg loss training: 3.0320,... Gradient Norm: 0.0494\n",
      "Epoch 773: avg loss training: 3.0320,... Gradient Norm: 0.0340\n",
      "Epoch 774: avg loss training: 3.0320,... Gradient Norm: 0.0835\n",
      "Epoch 775: avg loss training: 3.0320,... Gradient Norm: 0.1072\n",
      "Epoch 776: avg loss training: 3.0320,... Gradient Norm: 0.0356\n",
      "Epoch 777: avg loss training: 3.0320,... Gradient Norm: 0.1541\n",
      "Epoch 778: avg loss training: 3.0320,... Gradient Norm: 0.0552\n",
      "Epoch 779: avg loss training: 3.0320,... Gradient Norm: 0.1084\n",
      "Epoch 780: avg loss training: 3.0320,... Gradient Norm: 0.1569\n",
      "Epoch 781: avg loss training: 3.0320,... Gradient Norm: 0.0927\n",
      "Epoch 782: avg loss training: 3.0320,... Gradient Norm: 0.0784\n",
      "Epoch 783: avg loss training: 3.0320,... Gradient Norm: 0.1715\n",
      "Epoch 784: avg loss training: 3.0320,... Gradient Norm: 0.0381\n",
      "Epoch 785: avg loss training: 3.0320,... Gradient Norm: 0.0782\n",
      "Epoch 786: avg loss training: 3.0320,... Gradient Norm: 0.0562\n",
      "Epoch 787: avg loss training: 3.0320,... Gradient Norm: 0.1697\n",
      "Epoch 788: avg loss training: 3.0320,... Gradient Norm: 0.0987\n",
      "Epoch 789: avg loss training: 3.0320,... Gradient Norm: 0.1297\n",
      "Epoch 790: avg loss training: 3.0320,... Gradient Norm: 0.1280\n",
      "Epoch 791: avg loss training: 3.0320,... Gradient Norm: 0.1219\n",
      "Epoch 792: avg loss training: 3.0320,... Gradient Norm: 0.1787\n",
      "Epoch 793: avg loss training: 3.0320,... Gradient Norm: 0.1796\n",
      "Epoch 794: avg loss training: 3.0320,... Gradient Norm: 0.0525\n",
      "Epoch 795: avg loss training: 3.0320,... Gradient Norm: 0.1279\n",
      "Epoch 796: avg loss training: 3.0320,... Gradient Norm: 0.1262\n",
      "Epoch 797: avg loss training: 3.0320,... Gradient Norm: 0.1748\n",
      "Epoch 798: avg loss training: 3.0320,... Gradient Norm: 0.1710\n",
      "Epoch 799: avg loss training: 3.0320,... Gradient Norm: 0.1282\n",
      "Epoch 800: avg loss training: 3.0320,... Gradient Norm: 0.1246\n",
      "Epoch 801: avg loss training: 3.0320,... Gradient Norm: 0.1017\n",
      "Epoch 802: avg loss training: 3.0320,... Gradient Norm: 0.1757\n",
      "Epoch 803: avg loss training: 3.0320,... Gradient Norm: 0.1749\n",
      "Epoch 804: avg loss training: 3.0320,... Gradient Norm: 0.0993\n",
      "Epoch 805: avg loss training: 3.0320,... Gradient Norm: 0.1181\n",
      "Epoch 806: avg loss training: 3.0320,... Gradient Norm: 0.0869\n",
      "Epoch 807: avg loss training: 3.0320,... Gradient Norm: 0.1667\n",
      "Epoch 808: avg loss training: 3.0320,... Gradient Norm: 0.0331\n",
      "Epoch 809: avg loss training: 3.0320,... Gradient Norm: 0.0325\n",
      "Epoch 810: avg loss training: 3.0320,... Gradient Norm: 0.0521\n",
      "Epoch 811: avg loss training: 3.0320,... Gradient Norm: 0.1531\n",
      "Epoch 812: avg loss training: 3.0320,... Gradient Norm: 0.1148\n",
      "Epoch 813: avg loss training: 3.0320,... Gradient Norm: 0.1114\n",
      "Epoch 814: avg loss training: 3.0320,... Gradient Norm: 0.1562\n",
      "Epoch 815: avg loss training: 3.0320,... Gradient Norm: 0.1207\n",
      "Epoch 816: avg loss training: 3.0320,... Gradient Norm: 0.1261\n",
      "Epoch 817: avg loss training: 3.0320,... Gradient Norm: 0.1283\n",
      "Epoch 818: avg loss training: 3.0320,... Gradient Norm: 0.0341\n",
      "Epoch 819: avg loss training: 3.0320,... Gradient Norm: 0.1772\n",
      "Epoch 820: avg loss training: 3.0320,... Gradient Norm: 0.1681\n",
      "Epoch 821: avg loss training: 3.0320,... Gradient Norm: 0.1283\n",
      "Epoch 822: avg loss training: 3.0320,... Gradient Norm: 0.1272\n",
      "Epoch 823: avg loss training: 3.0320,... Gradient Norm: 0.1153\n",
      "Epoch 824: avg loss training: 3.0320,... Gradient Norm: 0.1742\n",
      "Epoch 825: avg loss training: 3.0320,... Gradient Norm: 0.1664\n",
      "Epoch 826: avg loss training: 3.0320,... Gradient Norm: 0.1016\n",
      "Epoch 827: avg loss training: 3.0320,... Gradient Norm: 0.1183\n",
      "Epoch 828: avg loss training: 3.0320,... Gradient Norm: 0.0776\n",
      "Epoch 829: avg loss training: 3.0320,... Gradient Norm: 0.0343\n",
      "Epoch 830: avg loss training: 3.0320,... Gradient Norm: 0.1777\n",
      "Epoch 831: avg loss training: 3.0320,... Gradient Norm: 0.1777\n",
      "Epoch 832: avg loss training: 3.0320,... Gradient Norm: 0.1047\n",
      "Epoch 833: avg loss training: 3.0320,... Gradient Norm: 0.1294\n",
      "Epoch 834: avg loss training: 3.0320,... Gradient Norm: 0.1270\n",
      "Epoch 835: avg loss training: 3.0320,... Gradient Norm: 0.0519\n",
      "Epoch 836: avg loss training: 3.0320,... Gradient Norm: 0.1771\n",
      "Epoch 837: avg loss training: 3.0320,... Gradient Norm: 0.1772\n",
      "Epoch 838: avg loss training: 3.0320,... Gradient Norm: 0.1450\n",
      "Epoch 839: avg loss training: 3.0320,... Gradient Norm: 0.1269\n",
      "Epoch 840: avg loss training: 3.0320,... Gradient Norm: 0.1271\n",
      "Epoch 841: avg loss training: 3.0320,... Gradient Norm: 0.1271\n",
      "Epoch 842: avg loss training: 3.0320,... Gradient Norm: 0.1229\n",
      "Epoch 843: avg loss training: 3.0320,... Gradient Norm: 0.0506\n",
      "Epoch 844: avg loss training: 3.0320,... Gradient Norm: 0.1766\n",
      "Epoch 845: avg loss training: 3.0320,... Gradient Norm: 0.1773\n",
      "Epoch 846: avg loss training: 3.0320,... Gradient Norm: 0.1754\n",
      "Epoch 847: avg loss training: 3.0320,... Gradient Norm: 0.0680\n",
      "Epoch 848: avg loss training: 3.0320,... Gradient Norm: 0.1273\n",
      "Epoch 849: avg loss training: 3.0320,... Gradient Norm: 0.1287\n",
      "Epoch 850: avg loss training: 3.0320,... Gradient Norm: 0.1121\n",
      "Epoch 851: avg loss training: 3.0320,... Gradient Norm: 0.0690\n",
      "Epoch 852: avg loss training: 3.0320,... Gradient Norm: 0.1729\n",
      "Epoch 853: avg loss training: 3.0320,... Gradient Norm: 0.1780\n",
      "Epoch 854: avg loss training: 3.0320,... Gradient Norm: 0.1696\n",
      "Epoch 855: avg loss training: 3.0320,... Gradient Norm: 0.1044\n",
      "Epoch 856: avg loss training: 3.0320,... Gradient Norm: 0.1255\n",
      "Epoch 857: avg loss training: 3.0320,... Gradient Norm: 0.1259\n",
      "Epoch 858: avg loss training: 3.0320,... Gradient Norm: 0.1061\n",
      "Epoch 859: avg loss training: 3.0320,... Gradient Norm: 0.0489\n",
      "Epoch 860: avg loss training: 3.0320,... Gradient Norm: 0.1750\n",
      "Epoch 861: avg loss training: 3.0320,... Gradient Norm: 0.1778\n",
      "Epoch 862: avg loss training: 3.0320,... Gradient Norm: 0.1667\n",
      "Epoch 863: avg loss training: 3.0320,... Gradient Norm: 0.0982\n",
      "Epoch 864: avg loss training: 3.0320,... Gradient Norm: 0.1281\n",
      "Epoch 865: avg loss training: 3.0320,... Gradient Norm: 0.1272\n",
      "Epoch 866: avg loss training: 3.0320,... Gradient Norm: 0.1167\n",
      "Epoch 867: avg loss training: 3.0320,... Gradient Norm: 0.0619\n",
      "Epoch 868: avg loss training: 3.0320,... Gradient Norm: 0.1721\n",
      "Epoch 869: avg loss training: 3.0320,... Gradient Norm: 0.1784\n",
      "Epoch 870: avg loss training: 3.0320,... Gradient Norm: 0.1682\n",
      "Epoch 871: avg loss training: 3.0320,... Gradient Norm: 0.0930\n",
      "Epoch 872: avg loss training: 3.0320,... Gradient Norm: 0.1183\n",
      "Epoch 873: avg loss training: 3.0320,... Gradient Norm: 0.1253\n",
      "Epoch 874: avg loss training: 3.0320,... Gradient Norm: 0.1024\n",
      "Epoch 875: avg loss training: 3.0320,... Gradient Norm: 0.0340\n",
      "Epoch 876: avg loss training: 3.0320,... Gradient Norm: 0.1702\n",
      "Epoch 877: avg loss training: 3.0320,... Gradient Norm: 0.1729\n",
      "Epoch 878: avg loss training: 3.0320,... Gradient Norm: 0.0388\n",
      "Epoch 879: avg loss training: 3.0320,... Gradient Norm: 0.0804\n",
      "Epoch 880: avg loss training: 3.0320,... Gradient Norm: 0.1009\n",
      "Epoch 881: avg loss training: 3.0320,... Gradient Norm: 0.1012\n",
      "Epoch 882: avg loss training: 3.0320,... Gradient Norm: 0.0510\n",
      "Epoch 883: avg loss training: 3.0320,... Gradient Norm: 0.1609\n",
      "Epoch 884: avg loss training: 3.0320,... Gradient Norm: 0.1568\n",
      "Epoch 885: avg loss training: 3.0320,... Gradient Norm: 0.0434\n",
      "Epoch 886: avg loss training: 3.0320,... Gradient Norm: 0.1136\n",
      "Epoch 887: avg loss training: 3.0320,... Gradient Norm: 0.1107\n",
      "Epoch 888: avg loss training: 3.0320,... Gradient Norm: 0.0228\n",
      "Epoch 889: avg loss training: 3.0320,... Gradient Norm: 0.1576\n",
      "Epoch 890: avg loss training: 3.0320,... Gradient Norm: 0.1503\n",
      "Epoch 891: avg loss training: 3.0320,... Gradient Norm: 0.1018\n",
      "Epoch 892: avg loss training: 3.0320,... Gradient Norm: 0.1206\n",
      "Epoch 893: avg loss training: 3.0320,... Gradient Norm: 0.1049\n",
      "Epoch 894: avg loss training: 3.0320,... Gradient Norm: 0.0238\n",
      "Epoch 895: avg loss training: 3.0320,... Gradient Norm: 0.1676\n",
      "Epoch 896: avg loss training: 3.0320,... Gradient Norm: 0.1620\n",
      "Epoch 897: avg loss training: 3.0320,... Gradient Norm: 0.0808\n",
      "Epoch 898: avg loss training: 3.0320,... Gradient Norm: 0.1235\n",
      "Epoch 899: avg loss training: 3.0320,... Gradient Norm: 0.1025\n",
      "Epoch 900: avg loss training: 3.0320,... Gradient Norm: 0.0589\n",
      "Epoch 901: avg loss training: 3.0320,... Gradient Norm: 0.1688\n",
      "Epoch 902: avg loss training: 3.0320,... Gradient Norm: 0.1680\n",
      "Epoch 903: avg loss training: 3.0320,... Gradient Norm: 0.0628\n",
      "Epoch 904: avg loss training: 3.0320,... Gradient Norm: 0.1027\n",
      "Epoch 905: avg loss training: 3.0320,... Gradient Norm: 0.1033\n",
      "Epoch 906: avg loss training: 3.0320,... Gradient Norm: 0.0340\n",
      "Epoch 907: avg loss training: 3.0320,... Gradient Norm: 0.1597\n",
      "Epoch 908: avg loss training: 3.0320,... Gradient Norm: 0.1545\n",
      "Epoch 909: avg loss training: 3.0320,... Gradient Norm: 0.1199\n",
      "Epoch 910: avg loss training: 3.0320,... Gradient Norm: 0.1229\n",
      "Epoch 911: avg loss training: 3.0320,... Gradient Norm: 0.0900\n",
      "Epoch 912: avg loss training: 3.0320,... Gradient Norm: 0.1564\n",
      "Epoch 913: avg loss training: 3.0320,... Gradient Norm: 0.1526\n",
      "Epoch 914: avg loss training: 3.0320,... Gradient Norm: 0.0962\n",
      "Epoch 915: avg loss training: 3.0320,... Gradient Norm: 0.1170\n",
      "Epoch 916: avg loss training: 3.0320,... Gradient Norm: 0.0708\n",
      "Epoch 917: avg loss training: 3.0320,... Gradient Norm: 0.0904\n",
      "Epoch 918: avg loss training: 3.0320,... Gradient Norm: 0.1697\n",
      "Epoch 919: avg loss training: 3.0320,... Gradient Norm: 0.0528\n",
      "Epoch 920: avg loss training: 3.0320,... Gradient Norm: 0.1025\n",
      "Epoch 921: avg loss training: 3.0320,... Gradient Norm: 0.0674\n",
      "Epoch 922: avg loss training: 3.0320,... Gradient Norm: 0.0887\n",
      "Epoch 923: avg loss training: 3.0320,... Gradient Norm: 0.1490\n",
      "Epoch 924: avg loss training: 3.0320,... Gradient Norm: 0.1075\n",
      "Epoch 925: avg loss training: 3.0320,... Gradient Norm: 0.1172\n",
      "Epoch 926: avg loss training: 3.0320,... Gradient Norm: 0.0206\n",
      "Epoch 927: avg loss training: 3.0320,... Gradient Norm: 0.1667\n",
      "Epoch 928: avg loss training: 3.0320,... Gradient Norm: 0.0221\n",
      "Epoch 929: avg loss training: 3.0320,... Gradient Norm: 0.0880\n",
      "Epoch 930: avg loss training: 3.0320,... Gradient Norm: 0.0790\n",
      "Epoch 931: avg loss training: 3.0320,... Gradient Norm: 0.1337\n",
      "Epoch 932: avg loss training: 3.0320,... Gradient Norm: 0.0215\n",
      "Epoch 933: avg loss training: 3.0320,... Gradient Norm: 0.0427\n",
      "Epoch 934: avg loss training: 3.0320,... Gradient Norm: 0.0425\n",
      "Epoch 935: avg loss training: 3.0320,... Gradient Norm: 0.0222\n",
      "Epoch 936: avg loss training: 3.0320,... Gradient Norm: 0.1561\n",
      "Epoch 937: avg loss training: 3.0320,... Gradient Norm: 0.1100\n",
      "Epoch 938: avg loss training: 3.0320,... Gradient Norm: 0.1133\n",
      "Epoch 939: avg loss training: 3.0320,... Gradient Norm: 0.0359\n",
      "Epoch 940: avg loss training: 3.0320,... Gradient Norm: 0.1727\n",
      "Epoch 941: avg loss training: 3.0320,... Gradient Norm: 0.1544\n",
      "Epoch 942: avg loss training: 3.0320,... Gradient Norm: 0.1284\n",
      "Epoch 943: avg loss training: 3.0320,... Gradient Norm: 0.1295\n",
      "Epoch 944: avg loss training: 3.0320,... Gradient Norm: 0.1254\n",
      "Epoch 945: avg loss training: 3.0320,... Gradient Norm: 0.1559\n",
      "Epoch 946: avg loss training: 3.0320,... Gradient Norm: 0.1655\n",
      "Epoch 947: avg loss training: 3.0320,... Gradient Norm: 0.0728\n",
      "Epoch 948: avg loss training: 3.0320,... Gradient Norm: 0.1091\n",
      "Epoch 949: avg loss training: 3.0320,... Gradient Norm: 0.0699\n",
      "Epoch 950: avg loss training: 3.0320,... Gradient Norm: 0.1211\n",
      "Epoch 951: avg loss training: 3.0320,... Gradient Norm: 0.1217\n",
      "Epoch 952: avg loss training: 3.0320,... Gradient Norm: 0.1090\n",
      "Epoch 953: avg loss training: 3.0320,... Gradient Norm: 0.1046\n",
      "Epoch 954: avg loss training: 3.0320,... Gradient Norm: 0.0876\n",
      "Epoch 955: avg loss training: 3.0320,... Gradient Norm: 0.1507\n",
      "Epoch 956: avg loss training: 3.0320,... Gradient Norm: 0.1236\n",
      "Epoch 957: avg loss training: 3.0320,... Gradient Norm: 0.1241\n",
      "Epoch 958: avg loss training: 3.0320,... Gradient Norm: 0.0605\n",
      "Epoch 959: avg loss training: 3.0320,... Gradient Norm: 0.1736\n",
      "Epoch 960: avg loss training: 3.0320,... Gradient Norm: 0.0770\n",
      "Epoch 961: avg loss training: 3.0320,... Gradient Norm: 0.0887\n",
      "Epoch 962: avg loss training: 3.0320,... Gradient Norm: 0.0657\n",
      "Epoch 963: avg loss training: 3.0320,... Gradient Norm: 0.1731\n",
      "Epoch 964: avg loss training: 3.0320,... Gradient Norm: 0.1527\n",
      "Epoch 965: avg loss training: 3.0320,... Gradient Norm: 0.1268\n",
      "Epoch 966: avg loss training: 3.0320,... Gradient Norm: 0.1267\n",
      "Epoch 967: avg loss training: 3.0320,... Gradient Norm: 0.1265\n",
      "Epoch 968: avg loss training: 3.0320,... Gradient Norm: 0.1653\n",
      "Epoch 969: avg loss training: 3.0320,... Gradient Norm: 0.1691\n",
      "Epoch 970: avg loss training: 3.0320,... Gradient Norm: 0.0902\n",
      "Epoch 971: avg loss training: 3.0320,... Gradient Norm: 0.1111\n",
      "Epoch 972: avg loss training: 3.0320,... Gradient Norm: 0.0746\n",
      "Epoch 973: avg loss training: 3.0320,... Gradient Norm: 0.1710\n",
      "Epoch 974: avg loss training: 3.0320,... Gradient Norm: 0.0314\n",
      "Epoch 975: avg loss training: 3.0320,... Gradient Norm: 0.0481\n",
      "Epoch 976: avg loss training: 3.0320,... Gradient Norm: 0.0503\n",
      "Epoch 977: avg loss training: 3.0320,... Gradient Norm: 0.1489\n",
      "Epoch 978: avg loss training: 3.0320,... Gradient Norm: 0.1115\n",
      "Epoch 979: avg loss training: 3.0320,... Gradient Norm: 0.0781\n",
      "Epoch 980: avg loss training: 3.0320,... Gradient Norm: 0.1523\n",
      "Epoch 981: avg loss training: 3.0320,... Gradient Norm: 0.1152\n",
      "Epoch 982: avg loss training: 3.0320,... Gradient Norm: 0.1203\n",
      "Epoch 983: avg loss training: 3.0320,... Gradient Norm: 0.1225\n",
      "Epoch 984: avg loss training: 3.0320,... Gradient Norm: 0.0958\n",
      "Epoch 985: avg loss training: 3.0320,... Gradient Norm: 0.1725\n",
      "Epoch 986: avg loss training: 3.0320,... Gradient Norm: 0.1682\n",
      "Epoch 987: avg loss training: 3.0320,... Gradient Norm: 0.0834\n",
      "Epoch 988: avg loss training: 3.0320,... Gradient Norm: 0.1156\n",
      "Epoch 989: avg loss training: 3.0320,... Gradient Norm: 0.0813\n",
      "Epoch 990: avg loss training: 3.0320,... Gradient Norm: 0.1492\n",
      "Epoch 991: avg loss training: 3.0320,... Gradient Norm: 0.0335\n",
      "Epoch 992: avg loss training: 3.0320,... Gradient Norm: 0.0552\n",
      "Epoch 993: avg loss training: 3.0320,... Gradient Norm: 0.0582\n",
      "Epoch 994: avg loss training: 3.0320,... Gradient Norm: 0.1612\n",
      "Epoch 995: avg loss training: 3.0320,... Gradient Norm: 0.1060\n",
      "Epoch 996: avg loss training: 3.0320,... Gradient Norm: 0.0805\n",
      "Epoch 997: avg loss training: 3.0320,... Gradient Norm: 0.1486\n",
      "Epoch 998: avg loss training: 3.0320,... Gradient Norm: 0.1153\n",
      "Epoch 999: avg loss training: 3.0320,... Gradient Norm: 0.1235\n",
      "Epoch 1000: avg loss training: 3.0320,... Gradient Norm: 0.1233\n",
      "Epoch 1: avg loss training: 3.1687,... Gradient Norm: 4.4229\n",
      "Epoch 2: avg loss training: 246.7446,... Gradient Norm: 2607.2556\n",
      "Epoch 3: avg loss training: 3.7089,... Gradient Norm: 20.2313\n",
      "Epoch 4: avg loss training: 3.2450,... Gradient Norm: 6.4639\n",
      "Epoch 5: avg loss training: 5.1049,... Gradient Norm: 63.5879\n",
      "Epoch 6: avg loss training: 7.9983,... Gradient Norm: 156.6928\n",
      "Epoch 7: avg loss training: 5.5278,... Gradient Norm: 43.2906\n",
      "Epoch 8: avg loss training: 6.8562,... Gradient Norm: 59.3229\n",
      "Epoch 9: avg loss training: 7.7196,... Gradient Norm: 63.4343\n",
      "Epoch 10: avg loss training: 8.7835,... Gradient Norm: 112.1317\n",
      "Epoch 11: avg loss training: 5.6710,... Gradient Norm: 43.4833\n",
      "Epoch 12: avg loss training: 5.0686,... Gradient Norm: 48.9037\n",
      "Epoch 13: avg loss training: 3.6153,... Gradient Norm: 22.3930\n",
      "Epoch 14: avg loss training: 2.9868,... Gradient Norm: 7.7082\n",
      "Epoch 15: avg loss training: 2.9352,... Gradient Norm: 7.5903\n",
      "Epoch 16: avg loss training: 3.0327,... Gradient Norm: 8.7644\n",
      "Epoch 17: avg loss training: 3.1831,... Gradient Norm: 10.8296\n",
      "Epoch 18: avg loss training: 3.0812,... Gradient Norm: 12.5996\n",
      "Epoch 19: avg loss training: 2.7757,... Gradient Norm: 5.6131\n",
      "Epoch 20: avg loss training: 2.6657,... Gradient Norm: 4.8743\n",
      "Epoch 21: avg loss training: 2.7675,... Gradient Norm: 7.9342\n",
      "Epoch 22: avg loss training: 2.7972,... Gradient Norm: 8.4541\n",
      "Epoch 23: avg loss training: 2.6995,... Gradient Norm: 8.0605\n",
      "Epoch 24: avg loss training: 2.5474,... Gradient Norm: 5.6426\n",
      "Epoch 25: avg loss training: 2.4652,... Gradient Norm: 1.2178\n",
      "Epoch 26: avg loss training: 2.5103,... Gradient Norm: 5.3974\n",
      "Epoch 27: avg loss training: 2.5773,... Gradient Norm: 11.3213\n",
      "Epoch 28: avg loss training: 2.5912,... Gradient Norm: 13.1782\n",
      "Epoch 29: avg loss training: 2.4964,... Gradient Norm: 11.5827\n",
      "Epoch 30: avg loss training: 2.3925,... Gradient Norm: 2.7929\n",
      "Epoch 31: avg loss training: 2.4085,... Gradient Norm: 7.5225\n",
      "Epoch 32: avg loss training: 2.3987,... Gradient Norm: 5.5951\n",
      "Epoch 33: avg loss training: 2.4085,... Gradient Norm: 8.6785\n",
      "Epoch 34: avg loss training: 2.3355,... Gradient Norm: 5.6280\n",
      "Epoch 35: avg loss training: 2.2794,... Gradient Norm: 6.5343\n",
      "Epoch 36: avg loss training: 2.2257,... Gradient Norm: 3.3933\n",
      "Epoch 37: avg loss training: 2.2306,... Gradient Norm: 6.3165\n",
      "Epoch 38: avg loss training: 2.2425,... Gradient Norm: 8.8407\n",
      "Epoch 39: avg loss training: 2.1985,... Gradient Norm: 5.1129\n",
      "Epoch 40: avg loss training: 2.1577,... Gradient Norm: 3.9233\n",
      "Epoch 41: avg loss training: 2.1380,... Gradient Norm: 4.8798\n",
      "Epoch 42: avg loss training: 2.1281,... Gradient Norm: 3.9942\n",
      "Epoch 43: avg loss training: 2.1292,... Gradient Norm: 4.7167\n",
      "Epoch 44: avg loss training: 2.1124,... Gradient Norm: 2.8925\n",
      "Epoch 45: avg loss training: 2.1063,... Gradient Norm: 3.2201\n",
      "Epoch 46: avg loss training: 2.0992,... Gradient Norm: 1.7641\n",
      "Epoch 47: avg loss training: 2.1040,... Gradient Norm: 4.7283\n",
      "Epoch 48: avg loss training: 2.0942,... Gradient Norm: 2.9357\n",
      "Epoch 49: avg loss training: 2.0844,... Gradient Norm: 3.0118\n",
      "Epoch 50: avg loss training: 2.0777,... Gradient Norm: 3.3331\n",
      "Epoch 51: avg loss training: 2.0753,... Gradient Norm: 2.0751\n",
      "Epoch 52: avg loss training: 2.0778,... Gradient Norm: 3.0260\n",
      "Epoch 53: avg loss training: 2.0746,... Gradient Norm: 3.2622\n",
      "Epoch 54: avg loss training: 2.0636,... Gradient Norm: 1.9839\n",
      "Epoch 55: avg loss training: 2.0580,... Gradient Norm: 1.6789\n",
      "Epoch 56: avg loss training: 2.0580,... Gradient Norm: 2.9519\n",
      "Epoch 57: avg loss training: 2.0551,... Gradient Norm: 1.6394\n",
      "Epoch 58: avg loss training: 2.0537,... Gradient Norm: 1.1376\n",
      "Epoch 59: avg loss training: 2.0550,... Gradient Norm: 2.2160\n",
      "Epoch 60: avg loss training: 2.0541,... Gradient Norm: 1.1337\n",
      "Epoch 61: avg loss training: 2.0535,... Gradient Norm: 1.7874\n",
      "Epoch 62: avg loss training: 2.0509,... Gradient Norm: 1.6055\n",
      "Epoch 63: avg loss training: 2.0488,... Gradient Norm: 0.4741\n",
      "Epoch 64: avg loss training: 2.0500,... Gradient Norm: 1.8156\n",
      "Epoch 65: avg loss training: 2.0495,... Gradient Norm: 1.6339\n",
      "Epoch 66: avg loss training: 2.0479,... Gradient Norm: 0.4805\n",
      "Epoch 67: avg loss training: 2.0481,... Gradient Norm: 1.3560\n",
      "Epoch 68: avg loss training: 2.0482,... Gradient Norm: 1.6633\n",
      "Epoch 69: avg loss training: 2.0465,... Gradient Norm: 1.1461\n",
      "Epoch 70: avg loss training: 2.0448,... Gradient Norm: 0.4415\n",
      "Epoch 71: avg loss training: 2.0445,... Gradient Norm: 1.1733\n",
      "Epoch 72: avg loss training: 2.0444,... Gradient Norm: 1.4232\n",
      "Epoch 73: avg loss training: 2.0434,... Gradient Norm: 0.9057\n",
      "Epoch 74: avg loss training: 2.0427,... Gradient Norm: 0.2942\n",
      "Epoch 75: avg loss training: 2.0427,... Gradient Norm: 0.8477\n",
      "Epoch 76: avg loss training: 2.0425,... Gradient Norm: 1.2435\n",
      "Epoch 77: avg loss training: 2.0417,... Gradient Norm: 1.0965\n",
      "Epoch 78: avg loss training: 2.0412,... Gradient Norm: 1.0591\n",
      "Epoch 79: avg loss training: 2.0413,... Gradient Norm: 1.2353\n",
      "Epoch 80: avg loss training: 2.0416,... Gradient Norm: 1.5694\n",
      "Epoch 81: avg loss training: 2.0420,... Gradient Norm: 2.4532\n",
      "Epoch 82: avg loss training: 2.0439,... Gradient Norm: 3.8561\n",
      "Epoch 83: avg loss training: 2.0494,... Gradient Norm: 6.1496\n",
      "Epoch 84: avg loss training: 2.0575,... Gradient Norm: 8.8532\n",
      "Epoch 85: avg loss training: 2.0694,... Gradient Norm: 10.8790\n",
      "Epoch 86: avg loss training: 2.0603,... Gradient Norm: 9.5192\n",
      "Epoch 87: avg loss training: 2.0440,... Gradient Norm: 4.2520\n",
      "Epoch 88: avg loss training: 2.0410,... Gradient Norm: 2.3862\n",
      "Epoch 89: avg loss training: 2.0509,... Gradient Norm: 6.8093\n",
      "Epoch 90: avg loss training: 2.0475,... Gradient Norm: 5.3832\n",
      "Epoch 91: avg loss training: 2.0398,... Gradient Norm: 0.6341\n",
      "Epoch 92: avg loss training: 2.0475,... Gradient Norm: 5.5790\n",
      "Epoch 93: avg loss training: 2.0451,... Gradient Norm: 4.5571\n",
      "Epoch 94: avg loss training: 2.0398,... Gradient Norm: 1.1391\n",
      "Epoch 95: avg loss training: 2.0463,... Gradient Norm: 5.1997\n",
      "Epoch 96: avg loss training: 2.0425,... Gradient Norm: 3.5734\n",
      "Epoch 97: avg loss training: 2.0398,... Gradient Norm: 1.7153\n",
      "Epoch 98: avg loss training: 2.0442,... Gradient Norm: 4.6256\n",
      "Epoch 99: avg loss training: 2.0401,... Gradient Norm: 2.4631\n",
      "Epoch 100: avg loss training: 2.0395,... Gradient Norm: 2.1483\n",
      "Epoch 101: avg loss training: 2.0422,... Gradient Norm: 4.0681\n",
      "Epoch 102: avg loss training: 2.0388,... Gradient Norm: 1.6432\n",
      "Epoch 103: avg loss training: 2.0392,... Gradient Norm: 2.1996\n",
      "Epoch 104: avg loss training: 2.0408,... Gradient Norm: 3.4397\n",
      "Epoch 105: avg loss training: 2.0382,... Gradient Norm: 1.0529\n",
      "Epoch 106: avg loss training: 2.0390,... Gradient Norm: 2.1484\n",
      "Epoch 107: avg loss training: 2.0398,... Gradient Norm: 2.9006\n",
      "Epoch 108: avg loss training: 2.0378,... Gradient Norm: 0.5504\n",
      "Epoch 109: avg loss training: 2.0387,... Gradient Norm: 2.1545\n",
      "Epoch 110: avg loss training: 2.0389,... Gradient Norm: 2.3454\n",
      "Epoch 111: avg loss training: 2.0375,... Gradient Norm: 0.0569\n",
      "Epoch 112: avg loss training: 2.0384,... Gradient Norm: 2.0983\n",
      "Epoch 113: avg loss training: 2.0380,... Gradient Norm: 1.6700\n",
      "Epoch 114: avg loss training: 2.0373,... Gradient Norm: 0.5589\n",
      "Epoch 115: avg loss training: 2.0381,... Gradient Norm: 1.9504\n",
      "Epoch 116: avg loss training: 2.0373,... Gradient Norm: 0.9443\n",
      "Epoch 117: avg loss training: 2.0373,... Gradient Norm: 1.0655\n",
      "Epoch 118: avg loss training: 2.0376,... Gradient Norm: 1.6031\n",
      "Epoch 119: avg loss training: 2.0369,... Gradient Norm: 0.1196\n",
      "Epoch 120: avg loss training: 2.0374,... Gradient Norm: 1.4399\n",
      "Epoch 121: avg loss training: 2.0371,... Gradient Norm: 1.1349\n",
      "Epoch 122: avg loss training: 2.0368,... Gradient Norm: 0.5084\n",
      "Epoch 123: avg loss training: 2.0372,... Gradient Norm: 1.4769\n",
      "Epoch 124: avg loss training: 2.0368,... Gradient Norm: 0.7069\n",
      "Epoch 125: avg loss training: 2.0367,... Gradient Norm: 0.7706\n",
      "Epoch 126: avg loss training: 2.0369,... Gradient Norm: 1.2913\n",
      "Epoch 127: avg loss training: 2.0365,... Gradient Norm: 0.3113\n",
      "Epoch 128: avg loss training: 2.0366,... Gradient Norm: 0.9455\n",
      "Epoch 129: avg loss training: 2.0366,... Gradient Norm: 1.0264\n",
      "Epoch 130: avg loss training: 2.0363,... Gradient Norm: 0.1189\n",
      "Epoch 131: avg loss training: 2.0365,... Gradient Norm: 1.0916\n",
      "Epoch 132: avg loss training: 2.0364,... Gradient Norm: 0.8496\n",
      "Epoch 133: avg loss training: 2.0362,... Gradient Norm: 0.2863\n",
      "Epoch 134: avg loss training: 2.0363,... Gradient Norm: 0.9781\n",
      "Epoch 135: avg loss training: 2.0361,... Gradient Norm: 0.4783\n",
      "Epoch 136: avg loss training: 2.0361,... Gradient Norm: 0.5460\n",
      "Epoch 137: avg loss training: 2.0362,... Gradient Norm: 0.8239\n",
      "Epoch 138: avg loss training: 2.0360,... Gradient Norm: 0.0980\n",
      "Epoch 139: avg loss training: 2.0360,... Gradient Norm: 0.7662\n",
      "Epoch 140: avg loss training: 2.0360,... Gradient Norm: 0.7417\n",
      "Epoch 141: avg loss training: 2.0359,... Gradient Norm: 0.1042\n",
      "Epoch 142: avg loss training: 2.0359,... Gradient Norm: 0.7771\n",
      "Epoch 143: avg loss training: 2.0358,... Gradient Norm: 0.5104\n",
      "Epoch 144: avg loss training: 2.0358,... Gradient Norm: 0.3071\n",
      "Epoch 145: avg loss training: 2.0358,... Gradient Norm: 0.6755\n",
      "Epoch 146: avg loss training: 2.0357,... Gradient Norm: 0.2469\n",
      "Epoch 147: avg loss training: 2.0357,... Gradient Norm: 0.4299\n",
      "Epoch 148: avg loss training: 2.0356,... Gradient Norm: 0.5406\n",
      "Epoch 149: avg loss training: 2.0356,... Gradient Norm: 0.0528\n",
      "Epoch 150: avg loss training: 2.0356,... Gradient Norm: 0.4026\n",
      "Epoch 151: avg loss training: 2.0355,... Gradient Norm: 0.2282\n",
      "Epoch 152: avg loss training: 2.0355,... Gradient Norm: 0.2696\n",
      "Epoch 153: avg loss training: 2.0355,... Gradient Norm: 0.3773\n",
      "Epoch 154: avg loss training: 2.0354,... Gradient Norm: 0.0572\n",
      "Epoch 155: avg loss training: 2.0354,... Gradient Norm: 0.3826\n",
      "Epoch 156: avg loss training: 2.0354,... Gradient Norm: 0.2184\n",
      "Epoch 157: avg loss training: 2.0353,... Gradient Norm: 0.1877\n",
      "Epoch 158: avg loss training: 2.0353,... Gradient Norm: 0.3303\n",
      "Epoch 159: avg loss training: 2.0353,... Gradient Norm: 0.0668\n",
      "Epoch 160: avg loss training: 2.0352,... Gradient Norm: 0.2389\n",
      "Epoch 161: avg loss training: 2.0352,... Gradient Norm: 0.1919\n",
      "Epoch 162: avg loss training: 2.0352,... Gradient Norm: 0.0876\n",
      "Epoch 163: avg loss training: 2.0352,... Gradient Norm: 0.1824\n",
      "Epoch 164: avg loss training: 2.0351,... Gradient Norm: 0.0432\n",
      "Epoch 165: avg loss training: 2.0351,... Gradient Norm: 0.2164\n",
      "Epoch 166: avg loss training: 2.0351,... Gradient Norm: 0.1151\n",
      "Epoch 167: avg loss training: 2.0351,... Gradient Norm: 0.1371\n",
      "Epoch 168: avg loss training: 2.0350,... Gradient Norm: 0.2062\n",
      "Epoch 169: avg loss training: 2.0350,... Gradient Norm: 0.0344\n",
      "Epoch 170: avg loss training: 2.0350,... Gradient Norm: 0.1609\n",
      "Epoch 171: avg loss training: 2.0350,... Gradient Norm: 0.1053\n",
      "Epoch 172: avg loss training: 2.0350,... Gradient Norm: 0.0864\n",
      "Epoch 173: avg loss training: 2.0349,... Gradient Norm: 0.1494\n",
      "Epoch 174: avg loss training: 2.0349,... Gradient Norm: 0.0281\n",
      "Epoch 175: avg loss training: 2.0349,... Gradient Norm: 0.1462\n",
      "Epoch 176: avg loss training: 2.0349,... Gradient Norm: 0.1168\n",
      "Epoch 177: avg loss training: 2.0349,... Gradient Norm: 0.0402\n",
      "Epoch 178: avg loss training: 2.0348,... Gradient Norm: 0.1068\n",
      "Epoch 179: avg loss training: 2.0348,... Gradient Norm: 0.0289\n",
      "Epoch 180: avg loss training: 2.0348,... Gradient Norm: 0.1144\n",
      "Epoch 181: avg loss training: 2.0348,... Gradient Norm: 0.0749\n",
      "Epoch 182: avg loss training: 2.0348,... Gradient Norm: 0.0743\n",
      "Epoch 183: avg loss training: 2.0348,... Gradient Norm: 0.0782\n",
      "Epoch 184: avg loss training: 2.0347,... Gradient Norm: 0.0523\n",
      "Epoch 185: avg loss training: 2.0347,... Gradient Norm: 0.0794\n",
      "Epoch 186: avg loss training: 2.0347,... Gradient Norm: 0.0310\n",
      "Epoch 187: avg loss training: 2.0347,... Gradient Norm: 0.0932\n",
      "Epoch 188: avg loss training: 2.0347,... Gradient Norm: 0.0391\n",
      "Epoch 189: avg loss training: 2.0347,... Gradient Norm: 0.0656\n",
      "Epoch 190: avg loss training: 2.0346,... Gradient Norm: 0.0610\n",
      "Epoch 191: avg loss training: 2.0346,... Gradient Norm: 0.0364\n",
      "Epoch 192: avg loss training: 2.0346,... Gradient Norm: 0.0804\n",
      "Epoch 193: avg loss training: 2.0346,... Gradient Norm: 0.0346\n",
      "Epoch 194: avg loss training: 2.0346,... Gradient Norm: 0.0445\n",
      "Epoch 195: avg loss training: 2.0346,... Gradient Norm: 0.0291\n",
      "Epoch 196: avg loss training: 2.0346,... Gradient Norm: 0.0464\n",
      "Epoch 197: avg loss training: 2.0345,... Gradient Norm: 0.0500\n",
      "Epoch 198: avg loss training: 2.0345,... Gradient Norm: 0.0297\n",
      "Epoch 199: avg loss training: 2.0345,... Gradient Norm: 0.0653\n",
      "Epoch 200: avg loss training: 2.0345,... Gradient Norm: 0.0448\n",
      "Epoch 201: avg loss training: 2.0345,... Gradient Norm: 0.0352\n",
      "Epoch 202: avg loss training: 2.0345,... Gradient Norm: 0.0501\n",
      "Epoch 203: avg loss training: 2.0345,... Gradient Norm: 0.0249\n",
      "Epoch 204: avg loss training: 2.0344,... Gradient Norm: 0.0406\n",
      "Epoch 205: avg loss training: 2.0344,... Gradient Norm: 0.0303\n",
      "Epoch 206: avg loss training: 2.0344,... Gradient Norm: 0.0380\n",
      "Epoch 207: avg loss training: 2.0344,... Gradient Norm: 0.0559\n",
      "Epoch 208: avg loss training: 2.0344,... Gradient Norm: 0.0257\n",
      "Epoch 209: avg loss training: 2.0344,... Gradient Norm: 0.0340\n",
      "Epoch 210: avg loss training: 2.0344,... Gradient Norm: 0.0323\n",
      "Epoch 211: avg loss training: 2.0343,... Gradient Norm: 0.0238\n",
      "Epoch 212: avg loss training: 2.0343,... Gradient Norm: 0.0297\n",
      "Epoch 213: avg loss training: 2.0343,... Gradient Norm: 0.0264\n",
      "Epoch 214: avg loss training: 2.0343,... Gradient Norm: 0.0283\n",
      "Epoch 215: avg loss training: 2.0343,... Gradient Norm: 0.0443\n",
      "Epoch 216: avg loss training: 2.0343,... Gradient Norm: 0.0227\n",
      "Epoch 217: avg loss training: 2.0343,... Gradient Norm: 0.0373\n",
      "Epoch 218: avg loss training: 2.0343,... Gradient Norm: 0.0273\n",
      "Epoch 219: avg loss training: 2.0343,... Gradient Norm: 0.0509\n",
      "Epoch 220: avg loss training: 2.0342,... Gradient Norm: 0.0237\n",
      "Epoch 221: avg loss training: 2.0342,... Gradient Norm: 0.0463\n",
      "Epoch 222: avg loss training: 2.0342,... Gradient Norm: 0.0277\n",
      "Epoch 223: avg loss training: 2.0342,... Gradient Norm: 0.0261\n",
      "Epoch 224: avg loss training: 2.0342,... Gradient Norm: 0.0404\n",
      "Epoch 225: avg loss training: 2.0342,... Gradient Norm: 0.0298\n",
      "Epoch 226: avg loss training: 2.0342,... Gradient Norm: 0.0260\n",
      "Epoch 227: avg loss training: 2.0342,... Gradient Norm: 0.0321\n",
      "Epoch 228: avg loss training: 2.0341,... Gradient Norm: 0.0237\n",
      "Epoch 229: avg loss training: 2.0341,... Gradient Norm: 0.0302\n",
      "Epoch 230: avg loss training: 2.0341,... Gradient Norm: 0.0367\n",
      "Epoch 231: avg loss training: 2.0341,... Gradient Norm: 0.0247\n",
      "Epoch 232: avg loss training: 2.0341,... Gradient Norm: 0.0371\n",
      "Epoch 233: avg loss training: 2.0341,... Gradient Norm: 0.0251\n",
      "Epoch 234: avg loss training: 2.0341,... Gradient Norm: 0.0339\n",
      "Epoch 235: avg loss training: 2.0341,... Gradient Norm: 0.0258\n",
      "Epoch 236: avg loss training: 2.0341,... Gradient Norm: 0.0345\n",
      "Epoch 237: avg loss training: 2.0341,... Gradient Norm: 0.0338\n",
      "Epoch 238: avg loss training: 2.0340,... Gradient Norm: 0.0296\n",
      "Epoch 239: avg loss training: 2.0340,... Gradient Norm: 0.0414\n",
      "Epoch 240: avg loss training: 2.0340,... Gradient Norm: 0.0521\n",
      "Epoch 241: avg loss training: 2.0340,... Gradient Norm: 0.0575\n",
      "Epoch 242: avg loss training: 2.0340,... Gradient Norm: 0.0400\n",
      "Epoch 243: avg loss training: 2.0340,... Gradient Norm: 0.0701\n",
      "Epoch 244: avg loss training: 2.0340,... Gradient Norm: 0.0291\n",
      "Epoch 245: avg loss training: 2.0340,... Gradient Norm: 0.0710\n",
      "Epoch 246: avg loss training: 2.0340,... Gradient Norm: 0.0401\n",
      "Epoch 247: avg loss training: 2.0340,... Gradient Norm: 0.0497\n",
      "Epoch 248: avg loss training: 2.0340,... Gradient Norm: 0.0483\n",
      "Epoch 249: avg loss training: 2.0339,... Gradient Norm: 0.0347\n",
      "Epoch 250: avg loss training: 2.0339,... Gradient Norm: 0.0433\n",
      "Epoch 251: avg loss training: 2.0339,... Gradient Norm: 0.0330\n",
      "Epoch 252: avg loss training: 2.0339,... Gradient Norm: 0.0544\n",
      "Epoch 253: avg loss training: 2.0339,... Gradient Norm: 0.0316\n",
      "Epoch 254: avg loss training: 2.0339,... Gradient Norm: 0.0426\n",
      "Epoch 255: avg loss training: 2.0339,... Gradient Norm: 0.0355\n",
      "Epoch 256: avg loss training: 2.0339,... Gradient Norm: 0.0339\n",
      "Epoch 257: avg loss training: 2.0339,... Gradient Norm: 0.0384\n",
      "Epoch 258: avg loss training: 2.0339,... Gradient Norm: 0.0363\n",
      "Epoch 259: avg loss training: 2.0338,... Gradient Norm: 0.0453\n",
      "Epoch 260: avg loss training: 2.0338,... Gradient Norm: 0.0453\n",
      "Epoch 261: avg loss training: 2.0338,... Gradient Norm: 0.0530\n",
      "Epoch 262: avg loss training: 2.0338,... Gradient Norm: 0.0373\n",
      "Epoch 263: avg loss training: 2.0338,... Gradient Norm: 0.0463\n",
      "Epoch 264: avg loss training: 2.0338,... Gradient Norm: 0.0299\n",
      "Epoch 265: avg loss training: 2.0338,... Gradient Norm: 0.0454\n",
      "Epoch 266: avg loss training: 2.0338,... Gradient Norm: 0.0294\n",
      "Epoch 267: avg loss training: 2.0338,... Gradient Norm: 0.0357\n",
      "Epoch 268: avg loss training: 2.0338,... Gradient Norm: 0.0307\n",
      "Epoch 269: avg loss training: 2.0337,... Gradient Norm: 0.0390\n",
      "Epoch 270: avg loss training: 2.0337,... Gradient Norm: 0.0339\n",
      "Epoch 271: avg loss training: 2.0337,... Gradient Norm: 0.0397\n",
      "Epoch 272: avg loss training: 2.0337,... Gradient Norm: 0.0368\n",
      "Epoch 273: avg loss training: 2.0337,... Gradient Norm: 0.0419\n",
      "Epoch 274: avg loss training: 2.0337,... Gradient Norm: 0.0396\n",
      "Epoch 275: avg loss training: 2.0337,... Gradient Norm: 0.0396\n",
      "Epoch 276: avg loss training: 2.0337,... Gradient Norm: 0.0492\n",
      "Epoch 277: avg loss training: 2.0337,... Gradient Norm: 0.0380\n",
      "Epoch 278: avg loss training: 2.0336,... Gradient Norm: 0.0409\n",
      "Epoch 279: avg loss training: 2.0336,... Gradient Norm: 0.0410\n",
      "Epoch 280: avg loss training: 2.0336,... Gradient Norm: 0.0393\n",
      "Epoch 281: avg loss training: 2.0336,... Gradient Norm: 0.0355\n",
      "Epoch 282: avg loss training: 2.0336,... Gradient Norm: 0.0368\n",
      "Epoch 283: avg loss training: 2.0336,... Gradient Norm: 0.0365\n",
      "Epoch 284: avg loss training: 2.0336,... Gradient Norm: 0.0379\n",
      "Epoch 285: avg loss training: 2.0336,... Gradient Norm: 0.0376\n",
      "Epoch 286: avg loss training: 2.0335,... Gradient Norm: 0.0343\n",
      "Epoch 287: avg loss training: 2.0335,... Gradient Norm: 0.0373\n",
      "Epoch 288: avg loss training: 2.0335,... Gradient Norm: 0.0316\n",
      "Epoch 289: avg loss training: 2.0335,... Gradient Norm: 0.0374\n",
      "Epoch 290: avg loss training: 2.0335,... Gradient Norm: 0.0332\n",
      "Epoch 291: avg loss training: 2.0335,... Gradient Norm: 0.0370\n",
      "Epoch 292: avg loss training: 2.0335,... Gradient Norm: 0.0318\n",
      "Epoch 293: avg loss training: 2.0335,... Gradient Norm: 0.0376\n",
      "Epoch 294: avg loss training: 2.0335,... Gradient Norm: 0.0341\n",
      "Epoch 295: avg loss training: 2.0335,... Gradient Norm: 0.0357\n",
      "Epoch 296: avg loss training: 2.0335,... Gradient Norm: 0.0352\n",
      "Epoch 297: avg loss training: 2.0334,... Gradient Norm: 0.0306\n",
      "Epoch 298: avg loss training: 2.0334,... Gradient Norm: 0.0302\n",
      "Epoch 299: avg loss training: 2.0334,... Gradient Norm: 0.0362\n",
      "Epoch 300: avg loss training: 2.0334,... Gradient Norm: 0.0332\n",
      "Epoch 301: avg loss training: 2.0334,... Gradient Norm: 0.0316\n",
      "Epoch 302: avg loss training: 2.0334,... Gradient Norm: 0.0313\n",
      "Epoch 303: avg loss training: 2.0334,... Gradient Norm: 0.0320\n",
      "Epoch 304: avg loss training: 2.0334,... Gradient Norm: 0.0354\n",
      "Epoch 305: avg loss training: 2.0334,... Gradient Norm: 0.0333\n",
      "Epoch 306: avg loss training: 2.0334,... Gradient Norm: 0.0353\n",
      "Epoch 307: avg loss training: 2.0334,... Gradient Norm: 0.0312\n",
      "Epoch 308: avg loss training: 2.0334,... Gradient Norm: 0.0434\n",
      "Epoch 309: avg loss training: 2.0333,... Gradient Norm: 0.0369\n",
      "Epoch 310: avg loss training: 2.0333,... Gradient Norm: 0.0285\n",
      "Epoch 311: avg loss training: 2.0333,... Gradient Norm: 0.0503\n",
      "Epoch 312: avg loss training: 2.0333,... Gradient Norm: 0.0280\n",
      "Epoch 313: avg loss training: 2.0333,... Gradient Norm: 0.0681\n",
      "Epoch 314: avg loss training: 2.0333,... Gradient Norm: 0.0300\n",
      "Epoch 315: avg loss training: 2.0333,... Gradient Norm: 0.0467\n",
      "Epoch 316: avg loss training: 2.0333,... Gradient Norm: 0.0317\n",
      "Epoch 317: avg loss training: 2.0333,... Gradient Norm: 0.0465\n",
      "Epoch 318: avg loss training: 2.0333,... Gradient Norm: 0.0331\n",
      "Epoch 319: avg loss training: 2.0333,... Gradient Norm: 0.0334\n",
      "Epoch 320: avg loss training: 2.0333,... Gradient Norm: 0.0317\n",
      "Epoch 321: avg loss training: 2.0333,... Gradient Norm: 0.0345\n",
      "Epoch 322: avg loss training: 2.0333,... Gradient Norm: 0.0457\n",
      "Epoch 323: avg loss training: 2.0333,... Gradient Norm: 0.0345\n",
      "Epoch 324: avg loss training: 2.0332,... Gradient Norm: 0.0324\n",
      "Epoch 325: avg loss training: 2.0332,... Gradient Norm: 0.0395\n",
      "Epoch 326: avg loss training: 2.0332,... Gradient Norm: 0.0379\n",
      "Epoch 327: avg loss training: 2.0332,... Gradient Norm: 0.0348\n",
      "Epoch 328: avg loss training: 2.0332,... Gradient Norm: 0.0437\n",
      "Epoch 329: avg loss training: 2.0332,... Gradient Norm: 0.0375\n",
      "Epoch 330: avg loss training: 2.0332,... Gradient Norm: 0.0364\n",
      "Epoch 331: avg loss training: 2.0332,... Gradient Norm: 0.0387\n",
      "Epoch 332: avg loss training: 2.0332,... Gradient Norm: 0.0309\n",
      "Epoch 333: avg loss training: 2.0332,... Gradient Norm: 0.0463\n",
      "Epoch 334: avg loss training: 2.0332,... Gradient Norm: 0.0325\n",
      "Epoch 335: avg loss training: 2.0332,... Gradient Norm: 0.0301\n",
      "Epoch 336: avg loss training: 2.0332,... Gradient Norm: 0.0299\n",
      "Epoch 337: avg loss training: 2.0332,... Gradient Norm: 0.0305\n",
      "Epoch 338: avg loss training: 2.0332,... Gradient Norm: 0.0339\n",
      "Epoch 339: avg loss training: 2.0332,... Gradient Norm: 0.0295\n",
      "Epoch 340: avg loss training: 2.0331,... Gradient Norm: 0.0269\n",
      "Epoch 341: avg loss training: 2.0331,... Gradient Norm: 0.0276\n",
      "Epoch 342: avg loss training: 2.0331,... Gradient Norm: 0.0285\n",
      "Epoch 343: avg loss training: 2.0331,... Gradient Norm: 0.0338\n",
      "Epoch 344: avg loss training: 2.0331,... Gradient Norm: 0.0284\n",
      "Epoch 345: avg loss training: 2.0331,... Gradient Norm: 0.0306\n",
      "Epoch 346: avg loss training: 2.0331,... Gradient Norm: 0.0286\n",
      "Epoch 347: avg loss training: 2.0331,... Gradient Norm: 0.0336\n",
      "Epoch 348: avg loss training: 2.0331,... Gradient Norm: 0.0397\n",
      "Epoch 349: avg loss training: 2.0331,... Gradient Norm: 0.0355\n",
      "Epoch 350: avg loss training: 2.0331,... Gradient Norm: 0.0329\n",
      "Epoch 351: avg loss training: 2.0331,... Gradient Norm: 0.0321\n",
      "Epoch 352: avg loss training: 2.0331,... Gradient Norm: 0.0377\n",
      "Epoch 353: avg loss training: 2.0331,... Gradient Norm: 0.0261\n",
      "Epoch 354: avg loss training: 2.0331,... Gradient Norm: 0.0245\n",
      "Epoch 355: avg loss training: 2.0331,... Gradient Norm: 0.0274\n",
      "Epoch 356: avg loss training: 2.0331,... Gradient Norm: 0.0343\n",
      "Epoch 357: avg loss training: 2.0331,... Gradient Norm: 0.0300\n",
      "Epoch 358: avg loss training: 2.0331,... Gradient Norm: 0.0257\n",
      "Epoch 359: avg loss training: 2.0331,... Gradient Norm: 0.0231\n",
      "Epoch 360: avg loss training: 2.0330,... Gradient Norm: 0.0321\n",
      "Epoch 361: avg loss training: 2.0330,... Gradient Norm: 0.0243\n",
      "Epoch 362: avg loss training: 2.0330,... Gradient Norm: 0.0254\n",
      "Epoch 363: avg loss training: 2.0330,... Gradient Norm: 0.0267\n",
      "Epoch 364: avg loss training: 2.0330,... Gradient Norm: 0.0261\n",
      "Epoch 365: avg loss training: 2.0330,... Gradient Norm: 0.0261\n",
      "Epoch 366: avg loss training: 2.0330,... Gradient Norm: 0.0290\n",
      "Epoch 367: avg loss training: 2.0330,... Gradient Norm: 0.0299\n",
      "Epoch 368: avg loss training: 2.0330,... Gradient Norm: 0.0308\n",
      "Epoch 369: avg loss training: 2.0330,... Gradient Norm: 0.0299\n",
      "Epoch 370: avg loss training: 2.0330,... Gradient Norm: 0.0314\n",
      "Epoch 371: avg loss training: 2.0330,... Gradient Norm: 0.0314\n",
      "Epoch 372: avg loss training: 2.0330,... Gradient Norm: 0.0480\n",
      "Epoch 373: avg loss training: 2.0330,... Gradient Norm: 0.0364\n",
      "Epoch 374: avg loss training: 2.0330,... Gradient Norm: 0.0458\n",
      "Epoch 375: avg loss training: 2.0330,... Gradient Norm: 0.0397\n",
      "Epoch 376: avg loss training: 2.0330,... Gradient Norm: 0.0316\n",
      "Epoch 377: avg loss training: 2.0330,... Gradient Norm: 0.0330\n",
      "Epoch 378: avg loss training: 2.0330,... Gradient Norm: 0.0276\n",
      "Epoch 379: avg loss training: 2.0330,... Gradient Norm: 0.0385\n",
      "Epoch 380: avg loss training: 2.0330,... Gradient Norm: 0.0312\n",
      "Epoch 381: avg loss training: 2.0330,... Gradient Norm: 0.0267\n",
      "Epoch 382: avg loss training: 2.0330,... Gradient Norm: 0.0304\n",
      "Epoch 383: avg loss training: 2.0330,... Gradient Norm: 0.0334\n",
      "Epoch 384: avg loss training: 2.0330,... Gradient Norm: 0.0361\n",
      "Epoch 385: avg loss training: 2.0330,... Gradient Norm: 0.0295\n",
      "Epoch 386: avg loss training: 2.0329,... Gradient Norm: 0.0371\n",
      "Epoch 387: avg loss training: 2.0329,... Gradient Norm: 0.0277\n",
      "Epoch 388: avg loss training: 2.0329,... Gradient Norm: 0.0283\n",
      "Epoch 389: avg loss training: 2.0329,... Gradient Norm: 0.0312\n",
      "Epoch 390: avg loss training: 2.0329,... Gradient Norm: 0.0323\n",
      "Epoch 391: avg loss training: 2.0329,... Gradient Norm: 0.0232\n",
      "Epoch 392: avg loss training: 2.0329,... Gradient Norm: 0.0235\n",
      "Epoch 393: avg loss training: 2.0329,... Gradient Norm: 0.0251\n",
      "Epoch 394: avg loss training: 2.0329,... Gradient Norm: 0.0249\n",
      "Epoch 395: avg loss training: 2.0329,... Gradient Norm: 0.0346\n",
      "Epoch 396: avg loss training: 2.0329,... Gradient Norm: 0.0251\n",
      "Epoch 397: avg loss training: 2.0329,... Gradient Norm: 0.0297\n",
      "Epoch 398: avg loss training: 2.0329,... Gradient Norm: 0.0290\n",
      "Epoch 399: avg loss training: 2.0329,... Gradient Norm: 0.0296\n",
      "Epoch 400: avg loss training: 2.0329,... Gradient Norm: 0.0304\n",
      "Epoch 401: avg loss training: 2.0329,... Gradient Norm: 0.0296\n",
      "Epoch 402: avg loss training: 2.0329,... Gradient Norm: 0.0272\n",
      "Epoch 403: avg loss training: 2.0329,... Gradient Norm: 0.0267\n",
      "Epoch 404: avg loss training: 2.0329,... Gradient Norm: 0.0262\n",
      "Epoch 405: avg loss training: 2.0329,... Gradient Norm: 0.0278\n",
      "Epoch 406: avg loss training: 2.0329,... Gradient Norm: 0.0293\n",
      "Epoch 407: avg loss training: 2.0329,... Gradient Norm: 0.0286\n",
      "Epoch 408: avg loss training: 2.0329,... Gradient Norm: 0.0338\n",
      "Epoch 409: avg loss training: 2.0329,... Gradient Norm: 0.0350\n",
      "Epoch 410: avg loss training: 2.0329,... Gradient Norm: 0.0316\n",
      "Epoch 411: avg loss training: 2.0329,... Gradient Norm: 0.0334\n",
      "Epoch 412: avg loss training: 2.0329,... Gradient Norm: 0.0321\n",
      "Epoch 413: avg loss training: 2.0329,... Gradient Norm: 0.0281\n",
      "Epoch 414: avg loss training: 2.0329,... Gradient Norm: 0.0279\n",
      "Epoch 415: avg loss training: 2.0329,... Gradient Norm: 0.0348\n",
      "Epoch 416: avg loss training: 2.0329,... Gradient Norm: 0.0312\n",
      "Epoch 417: avg loss training: 2.0329,... Gradient Norm: 0.0364\n",
      "Epoch 418: avg loss training: 2.0328,... Gradient Norm: 0.0331\n",
      "Epoch 419: avg loss training: 2.0328,... Gradient Norm: 0.0272\n",
      "Epoch 420: avg loss training: 2.0328,... Gradient Norm: 0.0330\n",
      "Epoch 421: avg loss training: 2.0328,... Gradient Norm: 0.0359\n",
      "Epoch 422: avg loss training: 2.0328,... Gradient Norm: 0.0255\n",
      "Epoch 423: avg loss training: 2.0328,... Gradient Norm: 0.0268\n",
      "Epoch 424: avg loss training: 2.0328,... Gradient Norm: 0.0243\n",
      "Epoch 425: avg loss training: 2.0328,... Gradient Norm: 0.0327\n",
      "Epoch 426: avg loss training: 2.0328,... Gradient Norm: 0.0284\n",
      "Epoch 427: avg loss training: 2.0328,... Gradient Norm: 0.0286\n",
      "Epoch 428: avg loss training: 2.0328,... Gradient Norm: 0.0304\n",
      "Epoch 429: avg loss training: 2.0328,... Gradient Norm: 0.0279\n",
      "Epoch 430: avg loss training: 2.0328,... Gradient Norm: 0.0337\n",
      "Epoch 431: avg loss training: 2.0328,... Gradient Norm: 0.0253\n",
      "Epoch 432: avg loss training: 2.0328,... Gradient Norm: 0.0249\n",
      "Epoch 433: avg loss training: 2.0328,... Gradient Norm: 0.0267\n",
      "Epoch 434: avg loss training: 2.0328,... Gradient Norm: 0.0244\n",
      "Epoch 435: avg loss training: 2.0328,... Gradient Norm: 0.0314\n",
      "Epoch 436: avg loss training: 2.0328,... Gradient Norm: 0.0271\n",
      "Epoch 437: avg loss training: 2.0328,... Gradient Norm: 0.0263\n",
      "Epoch 438: avg loss training: 2.0328,... Gradient Norm: 0.0263\n",
      "Epoch 439: avg loss training: 2.0328,... Gradient Norm: 0.0230\n",
      "Epoch 440: avg loss training: 2.0328,... Gradient Norm: 0.0286\n",
      "Epoch 441: avg loss training: 2.0328,... Gradient Norm: 0.0227\n",
      "Epoch 442: avg loss training: 2.0328,... Gradient Norm: 0.0273\n",
      "Epoch 443: avg loss training: 2.0328,... Gradient Norm: 0.0271\n",
      "Epoch 444: avg loss training: 2.0328,... Gradient Norm: 0.0292\n",
      "Epoch 445: avg loss training: 2.0328,... Gradient Norm: 0.0304\n",
      "Epoch 446: avg loss training: 2.0328,... Gradient Norm: 0.0225\n",
      "Epoch 447: avg loss training: 2.0328,... Gradient Norm: 0.0259\n",
      "Epoch 448: avg loss training: 2.0328,... Gradient Norm: 0.0241\n",
      "Epoch 449: avg loss training: 2.0328,... Gradient Norm: 0.0246\n",
      "Epoch 450: avg loss training: 2.0328,... Gradient Norm: 0.0217\n",
      "Epoch 451: avg loss training: 2.0328,... Gradient Norm: 0.0250\n",
      "Epoch 452: avg loss training: 2.0328,... Gradient Norm: 0.0257\n",
      "Epoch 453: avg loss training: 2.0328,... Gradient Norm: 0.0249\n",
      "Epoch 454: avg loss training: 2.0328,... Gradient Norm: 0.0220\n",
      "Epoch 455: avg loss training: 2.0328,... Gradient Norm: 0.0217\n",
      "Epoch 456: avg loss training: 2.0328,... Gradient Norm: 0.0222\n",
      "Epoch 457: avg loss training: 2.0328,... Gradient Norm: 0.0218\n",
      "Epoch 458: avg loss training: 2.0328,... Gradient Norm: 0.0217\n",
      "Epoch 459: avg loss training: 2.0328,... Gradient Norm: 0.0265\n",
      "Epoch 460: avg loss training: 2.0328,... Gradient Norm: 0.0213\n",
      "Epoch 461: avg loss training: 2.0328,... Gradient Norm: 0.0233\n",
      "Epoch 462: avg loss training: 2.0328,... Gradient Norm: 0.0212\n",
      "Epoch 463: avg loss training: 2.0328,... Gradient Norm: 0.0234\n",
      "Epoch 464: avg loss training: 2.0328,... Gradient Norm: 0.0202\n",
      "Epoch 465: avg loss training: 2.0328,... Gradient Norm: 0.0242\n",
      "Epoch 466: avg loss training: 2.0328,... Gradient Norm: 0.0253\n",
      "Epoch 467: avg loss training: 2.0327,... Gradient Norm: 0.0224\n",
      "Epoch 468: avg loss training: 2.0327,... Gradient Norm: 0.0234\n",
      "Epoch 469: avg loss training: 2.0327,... Gradient Norm: 0.0237\n",
      "Epoch 470: avg loss training: 2.0327,... Gradient Norm: 0.0253\n",
      "Epoch 471: avg loss training: 2.0327,... Gradient Norm: 0.0262\n",
      "Epoch 472: avg loss training: 2.0327,... Gradient Norm: 0.0258\n",
      "Epoch 473: avg loss training: 2.0327,... Gradient Norm: 0.0240\n",
      "Epoch 474: avg loss training: 2.0327,... Gradient Norm: 0.0238\n",
      "Epoch 475: avg loss training: 2.0327,... Gradient Norm: 0.0203\n",
      "Epoch 476: avg loss training: 2.0327,... Gradient Norm: 0.0237\n",
      "Epoch 477: avg loss training: 2.0327,... Gradient Norm: 0.0212\n",
      "Epoch 478: avg loss training: 2.0327,... Gradient Norm: 0.0231\n",
      "Epoch 479: avg loss training: 2.0327,... Gradient Norm: 0.0230\n",
      "Epoch 480: avg loss training: 2.0327,... Gradient Norm: 0.0293\n",
      "Epoch 481: avg loss training: 2.0327,... Gradient Norm: 0.0215\n",
      "Epoch 482: avg loss training: 2.0327,... Gradient Norm: 0.0205\n",
      "Epoch 483: avg loss training: 2.0327,... Gradient Norm: 0.0241\n",
      "Epoch 484: avg loss training: 2.0327,... Gradient Norm: 0.0215\n",
      "Epoch 485: avg loss training: 2.0327,... Gradient Norm: 0.0227\n",
      "Epoch 486: avg loss training: 2.0327,... Gradient Norm: 0.0260\n",
      "Epoch 487: avg loss training: 2.0327,... Gradient Norm: 0.0212\n",
      "Epoch 488: avg loss training: 2.0327,... Gradient Norm: 0.0201\n",
      "Epoch 489: avg loss training: 2.0327,... Gradient Norm: 0.0210\n",
      "Epoch 490: avg loss training: 2.0327,... Gradient Norm: 0.0194\n",
      "Epoch 491: avg loss training: 2.0327,... Gradient Norm: 0.0224\n",
      "Epoch 492: avg loss training: 2.0327,... Gradient Norm: 0.0248\n",
      "Epoch 493: avg loss training: 2.0327,... Gradient Norm: 0.0332\n",
      "Epoch 494: avg loss training: 2.0327,... Gradient Norm: 0.0198\n",
      "Epoch 495: avg loss training: 2.0327,... Gradient Norm: 0.0251\n",
      "Epoch 496: avg loss training: 2.0327,... Gradient Norm: 0.0221\n",
      "Epoch 497: avg loss training: 2.0327,... Gradient Norm: 0.0231\n",
      "Epoch 498: avg loss training: 2.0327,... Gradient Norm: 0.0203\n",
      "Epoch 499: avg loss training: 2.0327,... Gradient Norm: 0.0251\n",
      "Epoch 500: avg loss training: 2.0327,... Gradient Norm: 0.0280\n",
      "Epoch 501: avg loss training: 2.0327,... Gradient Norm: 0.0237\n",
      "Epoch 502: avg loss training: 2.0327,... Gradient Norm: 0.0213\n",
      "Epoch 503: avg loss training: 2.0327,... Gradient Norm: 0.0275\n",
      "Epoch 504: avg loss training: 2.0327,... Gradient Norm: 0.0206\n",
      "Epoch 505: avg loss training: 2.0327,... Gradient Norm: 0.0213\n",
      "Epoch 506: avg loss training: 2.0327,... Gradient Norm: 0.0224\n",
      "Epoch 507: avg loss training: 2.0327,... Gradient Norm: 0.0228\n",
      "Epoch 508: avg loss training: 2.0327,... Gradient Norm: 0.0243\n",
      "Epoch 509: avg loss training: 2.0327,... Gradient Norm: 0.0213\n",
      "Epoch 510: avg loss training: 2.0327,... Gradient Norm: 0.0235\n",
      "Epoch 511: avg loss training: 2.0327,... Gradient Norm: 0.0228\n",
      "Epoch 512: avg loss training: 2.0327,... Gradient Norm: 0.0211\n",
      "Epoch 513: avg loss training: 2.0327,... Gradient Norm: 0.0209\n",
      "Epoch 514: avg loss training: 2.0327,... Gradient Norm: 0.0205\n",
      "Epoch 515: avg loss training: 2.0327,... Gradient Norm: 0.0202\n",
      "Epoch 516: avg loss training: 2.0327,... Gradient Norm: 0.0231\n",
      "Epoch 517: avg loss training: 2.0327,... Gradient Norm: 0.0227\n",
      "Epoch 518: avg loss training: 2.0327,... Gradient Norm: 0.0216\n",
      "Epoch 519: avg loss training: 2.0327,... Gradient Norm: 0.0263\n",
      "Epoch 520: avg loss training: 2.0327,... Gradient Norm: 0.0213\n",
      "Epoch 521: avg loss training: 2.0327,... Gradient Norm: 0.0261\n",
      "Epoch 522: avg loss training: 2.0327,... Gradient Norm: 0.0260\n",
      "Epoch 523: avg loss training: 2.0327,... Gradient Norm: 0.0195\n",
      "Epoch 524: avg loss training: 2.0327,... Gradient Norm: 0.0247\n",
      "Epoch 525: avg loss training: 2.0327,... Gradient Norm: 0.0265\n",
      "Epoch 526: avg loss training: 2.0327,... Gradient Norm: 0.0228\n",
      "Epoch 527: avg loss training: 2.0327,... Gradient Norm: 0.0240\n",
      "Epoch 528: avg loss training: 2.0327,... Gradient Norm: 0.0244\n",
      "Epoch 529: avg loss training: 2.0327,... Gradient Norm: 0.0240\n",
      "Epoch 530: avg loss training: 2.0327,... Gradient Norm: 0.0234\n",
      "Epoch 531: avg loss training: 2.0327,... Gradient Norm: 0.0214\n",
      "Epoch 532: avg loss training: 2.0327,... Gradient Norm: 0.0202\n",
      "Epoch 533: avg loss training: 2.0327,... Gradient Norm: 0.0235\n",
      "Epoch 534: avg loss training: 2.0327,... Gradient Norm: 0.0238\n",
      "Epoch 535: avg loss training: 2.0327,... Gradient Norm: 0.0238\n",
      "Epoch 536: avg loss training: 2.0327,... Gradient Norm: 0.0248\n",
      "Epoch 537: avg loss training: 2.0327,... Gradient Norm: 0.0190\n",
      "Epoch 538: avg loss training: 2.0327,... Gradient Norm: 0.0249\n",
      "Epoch 539: avg loss training: 2.0327,... Gradient Norm: 0.0240\n",
      "Epoch 540: avg loss training: 2.0327,... Gradient Norm: 0.0194\n",
      "Epoch 541: avg loss training: 2.0327,... Gradient Norm: 0.0246\n",
      "Epoch 542: avg loss training: 2.0327,... Gradient Norm: 0.0244\n",
      "Epoch 543: avg loss training: 2.0327,... Gradient Norm: 0.0256\n",
      "Epoch 544: avg loss training: 2.0327,... Gradient Norm: 0.0278\n",
      "Epoch 545: avg loss training: 2.0327,... Gradient Norm: 0.0214\n",
      "Epoch 546: avg loss training: 2.0327,... Gradient Norm: 0.0215\n",
      "Epoch 547: avg loss training: 2.0327,... Gradient Norm: 0.0273\n",
      "Epoch 548: avg loss training: 2.0327,... Gradient Norm: 0.0201\n",
      "Epoch 549: avg loss training: 2.0327,... Gradient Norm: 0.0203\n",
      "Epoch 550: avg loss training: 2.0327,... Gradient Norm: 0.0189\n",
      "Epoch 551: avg loss training: 2.0327,... Gradient Norm: 0.0197\n",
      "Epoch 552: avg loss training: 2.0327,... Gradient Norm: 0.0245\n",
      "Epoch 553: avg loss training: 2.0327,... Gradient Norm: 0.0284\n",
      "Epoch 554: avg loss training: 2.0327,... Gradient Norm: 0.0217\n",
      "Epoch 555: avg loss training: 2.0327,... Gradient Norm: 0.0232\n",
      "Epoch 556: avg loss training: 2.0327,... Gradient Norm: 0.0247\n",
      "Epoch 557: avg loss training: 2.0327,... Gradient Norm: 0.0207\n",
      "Epoch 558: avg loss training: 2.0327,... Gradient Norm: 0.0253\n",
      "Epoch 559: avg loss training: 2.0327,... Gradient Norm: 0.0214\n",
      "Epoch 560: avg loss training: 2.0327,... Gradient Norm: 0.0197\n",
      "Epoch 561: avg loss training: 2.0327,... Gradient Norm: 0.0191\n",
      "Epoch 562: avg loss training: 2.0327,... Gradient Norm: 0.0192\n",
      "Epoch 563: avg loss training: 2.0327,... Gradient Norm: 0.0191\n",
      "Epoch 564: avg loss training: 2.0327,... Gradient Norm: 0.0190\n",
      "Epoch 565: avg loss training: 2.0327,... Gradient Norm: 0.0199\n",
      "Epoch 566: avg loss training: 2.0327,... Gradient Norm: 0.0212\n",
      "Epoch 567: avg loss training: 2.0327,... Gradient Norm: 0.0193\n",
      "Epoch 568: avg loss training: 2.0327,... Gradient Norm: 0.0196\n",
      "Epoch 569: avg loss training: 2.0327,... Gradient Norm: 0.0217\n",
      "Epoch 570: avg loss training: 2.0327,... Gradient Norm: 0.0227\n",
      "Epoch 571: avg loss training: 2.0327,... Gradient Norm: 0.0191\n",
      "Epoch 572: avg loss training: 2.0327,... Gradient Norm: 0.0209\n",
      "Epoch 573: avg loss training: 2.0327,... Gradient Norm: 0.0221\n",
      "Epoch 574: avg loss training: 2.0327,... Gradient Norm: 0.0190\n",
      "Epoch 575: avg loss training: 2.0327,... Gradient Norm: 0.0261\n",
      "Epoch 576: avg loss training: 2.0327,... Gradient Norm: 0.0225\n",
      "Epoch 577: avg loss training: 2.0326,... Gradient Norm: 0.0228\n",
      "Epoch 578: avg loss training: 2.0326,... Gradient Norm: 0.0229\n",
      "Epoch 579: avg loss training: 2.0326,... Gradient Norm: 0.0236\n",
      "Epoch 580: avg loss training: 2.0326,... Gradient Norm: 0.0210\n",
      "Epoch 581: avg loss training: 2.0326,... Gradient Norm: 0.0222\n",
      "Epoch 582: avg loss training: 2.0326,... Gradient Norm: 0.0254\n",
      "Epoch 583: avg loss training: 2.0326,... Gradient Norm: 0.0202\n",
      "Epoch 584: avg loss training: 2.0326,... Gradient Norm: 0.0190\n",
      "Epoch 585: avg loss training: 2.0326,... Gradient Norm: 0.0204\n",
      "Epoch 586: avg loss training: 2.0326,... Gradient Norm: 0.0239\n",
      "Epoch 587: avg loss training: 2.0326,... Gradient Norm: 0.0204\n",
      "Epoch 588: avg loss training: 2.0326,... Gradient Norm: 0.0208\n",
      "Epoch 589: avg loss training: 2.0326,... Gradient Norm: 0.0223\n",
      "Epoch 590: avg loss training: 2.0326,... Gradient Norm: 0.0234\n",
      "Epoch 591: avg loss training: 2.0326,... Gradient Norm: 0.0292\n",
      "Epoch 592: avg loss training: 2.0326,... Gradient Norm: 0.0208\n",
      "Epoch 593: avg loss training: 2.0326,... Gradient Norm: 0.0224\n",
      "Epoch 594: avg loss training: 2.0326,... Gradient Norm: 0.0209\n",
      "Epoch 595: avg loss training: 2.0326,... Gradient Norm: 0.0199\n",
      "Epoch 596: avg loss training: 2.0326,... Gradient Norm: 0.0268\n",
      "Epoch 597: avg loss training: 2.0326,... Gradient Norm: 0.0232\n",
      "Epoch 598: avg loss training: 2.0326,... Gradient Norm: 0.0207\n",
      "Epoch 599: avg loss training: 2.0326,... Gradient Norm: 0.0232\n",
      "Epoch 600: avg loss training: 2.0326,... Gradient Norm: 0.0204\n",
      "Epoch 601: avg loss training: 2.0326,... Gradient Norm: 0.0221\n",
      "Epoch 602: avg loss training: 2.0326,... Gradient Norm: 0.0219\n",
      "Epoch 603: avg loss training: 2.0326,... Gradient Norm: 0.0197\n",
      "Epoch 604: avg loss training: 2.0326,... Gradient Norm: 0.0233\n",
      "Epoch 605: avg loss training: 2.0326,... Gradient Norm: 0.0220\n",
      "Epoch 606: avg loss training: 2.0326,... Gradient Norm: 0.0300\n",
      "Epoch 607: avg loss training: 2.0326,... Gradient Norm: 0.0292\n",
      "Epoch 608: avg loss training: 2.0326,... Gradient Norm: 0.0259\n",
      "Epoch 609: avg loss training: 2.0326,... Gradient Norm: 0.0196\n",
      "Epoch 610: avg loss training: 2.0326,... Gradient Norm: 0.0306\n",
      "Epoch 611: avg loss training: 2.0326,... Gradient Norm: 0.0189\n",
      "Epoch 612: avg loss training: 2.0326,... Gradient Norm: 0.0219\n",
      "Epoch 613: avg loss training: 2.0326,... Gradient Norm: 0.0228\n",
      "Epoch 614: avg loss training: 2.0326,... Gradient Norm: 0.0202\n",
      "Epoch 615: avg loss training: 2.0326,... Gradient Norm: 0.0243\n",
      "Epoch 616: avg loss training: 2.0326,... Gradient Norm: 0.0204\n",
      "Epoch 617: avg loss training: 2.0326,... Gradient Norm: 0.0225\n",
      "Epoch 618: avg loss training: 2.0326,... Gradient Norm: 0.0216\n",
      "Epoch 619: avg loss training: 2.0326,... Gradient Norm: 0.0199\n",
      "Epoch 620: avg loss training: 2.0326,... Gradient Norm: 0.0300\n",
      "Epoch 621: avg loss training: 2.0326,... Gradient Norm: 0.0225\n",
      "Epoch 622: avg loss training: 2.0326,... Gradient Norm: 0.0198\n",
      "Epoch 623: avg loss training: 2.0326,... Gradient Norm: 0.0235\n",
      "Epoch 624: avg loss training: 2.0326,... Gradient Norm: 0.0223\n",
      "Epoch 625: avg loss training: 2.0326,... Gradient Norm: 0.0196\n",
      "Epoch 626: avg loss training: 2.0326,... Gradient Norm: 0.0208\n",
      "Epoch 627: avg loss training: 2.0326,... Gradient Norm: 0.0199\n",
      "Epoch 628: avg loss training: 2.0326,... Gradient Norm: 0.0201\n",
      "Epoch 629: avg loss training: 2.0326,... Gradient Norm: 0.0198\n",
      "Epoch 630: avg loss training: 2.0326,... Gradient Norm: 0.0193\n",
      "Epoch 631: avg loss training: 2.0326,... Gradient Norm: 0.0210\n",
      "Epoch 632: avg loss training: 2.0326,... Gradient Norm: 0.0203\n",
      "Epoch 633: avg loss training: 2.0326,... Gradient Norm: 0.0211\n",
      "Epoch 634: avg loss training: 2.0326,... Gradient Norm: 0.0218\n",
      "Epoch 635: avg loss training: 2.0326,... Gradient Norm: 0.0233\n",
      "Epoch 636: avg loss training: 2.0326,... Gradient Norm: 0.0195\n",
      "Epoch 637: avg loss training: 2.0326,... Gradient Norm: 0.0213\n",
      "Epoch 638: avg loss training: 2.0326,... Gradient Norm: 0.0209\n",
      "Epoch 639: avg loss training: 2.0326,... Gradient Norm: 0.0195\n",
      "Epoch 640: avg loss training: 2.0326,... Gradient Norm: 0.0217\n",
      "Epoch 641: avg loss training: 2.0326,... Gradient Norm: 0.0202\n",
      "Epoch 642: avg loss training: 2.0326,... Gradient Norm: 0.0203\n",
      "Epoch 643: avg loss training: 2.0326,... Gradient Norm: 0.0253\n",
      "Epoch 644: avg loss training: 2.0326,... Gradient Norm: 0.0201\n",
      "Epoch 645: avg loss training: 2.0326,... Gradient Norm: 0.0247\n",
      "Epoch 646: avg loss training: 2.0326,... Gradient Norm: 0.0187\n",
      "Epoch 647: avg loss training: 2.0326,... Gradient Norm: 0.0217\n",
      "Epoch 648: avg loss training: 2.0326,... Gradient Norm: 0.0223\n",
      "Epoch 649: avg loss training: 2.0326,... Gradient Norm: 0.0234\n",
      "Epoch 650: avg loss training: 2.0326,... Gradient Norm: 0.0207\n",
      "Epoch 651: avg loss training: 2.0326,... Gradient Norm: 0.0205\n",
      "Epoch 652: avg loss training: 2.0326,... Gradient Norm: 0.0238\n",
      "Epoch 653: avg loss training: 2.0326,... Gradient Norm: 0.0186\n",
      "Epoch 654: avg loss training: 2.0326,... Gradient Norm: 0.0226\n",
      "Epoch 655: avg loss training: 2.0326,... Gradient Norm: 0.0184\n",
      "Epoch 656: avg loss training: 2.0326,... Gradient Norm: 0.0206\n",
      "Epoch 657: avg loss training: 2.0326,... Gradient Norm: 0.0216\n",
      "Epoch 658: avg loss training: 2.0326,... Gradient Norm: 0.0194\n",
      "Epoch 659: avg loss training: 2.0326,... Gradient Norm: 0.0300\n",
      "Epoch 660: avg loss training: 2.0326,... Gradient Norm: 0.0225\n",
      "Epoch 661: avg loss training: 2.0326,... Gradient Norm: 0.0240\n",
      "Epoch 662: avg loss training: 2.0326,... Gradient Norm: 0.0255\n",
      "Epoch 663: avg loss training: 2.0326,... Gradient Norm: 0.0240\n",
      "Epoch 664: avg loss training: 2.0326,... Gradient Norm: 0.0247\n",
      "Epoch 665: avg loss training: 2.0326,... Gradient Norm: 0.0207\n",
      "Epoch 666: avg loss training: 2.0326,... Gradient Norm: 0.0211\n",
      "Epoch 667: avg loss training: 2.0326,... Gradient Norm: 0.0203\n",
      "Epoch 668: avg loss training: 2.0326,... Gradient Norm: 0.0212\n",
      "Epoch 669: avg loss training: 2.0326,... Gradient Norm: 0.0244\n",
      "Epoch 670: avg loss training: 2.0326,... Gradient Norm: 0.0185\n",
      "Epoch 671: avg loss training: 2.0326,... Gradient Norm: 0.0222\n",
      "Epoch 672: avg loss training: 2.0326,... Gradient Norm: 0.0223\n",
      "Epoch 673: avg loss training: 2.0326,... Gradient Norm: 0.0193\n",
      "Epoch 674: avg loss training: 2.0326,... Gradient Norm: 0.0210\n",
      "Epoch 675: avg loss training: 2.0326,... Gradient Norm: 0.0218\n",
      "Epoch 676: avg loss training: 2.0326,... Gradient Norm: 0.0201\n",
      "Epoch 677: avg loss training: 2.0326,... Gradient Norm: 0.0234\n",
      "Epoch 678: avg loss training: 2.0326,... Gradient Norm: 0.0255\n",
      "Epoch 679: avg loss training: 2.0326,... Gradient Norm: 0.0198\n",
      "Epoch 680: avg loss training: 2.0326,... Gradient Norm: 0.0259\n",
      "Epoch 681: avg loss training: 2.0326,... Gradient Norm: 0.0189\n",
      "Epoch 682: avg loss training: 2.0326,... Gradient Norm: 0.0214\n",
      "Epoch 683: avg loss training: 2.0326,... Gradient Norm: 0.0196\n",
      "Epoch 684: avg loss training: 2.0326,... Gradient Norm: 0.0197\n",
      "Epoch 685: avg loss training: 2.0326,... Gradient Norm: 0.0200\n",
      "Epoch 686: avg loss training: 2.0326,... Gradient Norm: 0.0181\n",
      "Epoch 687: avg loss training: 2.0326,... Gradient Norm: 0.0220\n",
      "Epoch 688: avg loss training: 2.0326,... Gradient Norm: 0.0210\n",
      "Epoch 689: avg loss training: 2.0326,... Gradient Norm: 0.0204\n",
      "Epoch 690: avg loss training: 2.0326,... Gradient Norm: 0.0211\n",
      "Epoch 691: avg loss training: 2.0326,... Gradient Norm: 0.0199\n",
      "Epoch 692: avg loss training: 2.0326,... Gradient Norm: 0.0224\n",
      "Epoch 693: avg loss training: 2.0326,... Gradient Norm: 0.0208\n",
      "Epoch 694: avg loss training: 2.0326,... Gradient Norm: 0.0219\n",
      "Epoch 695: avg loss training: 2.0326,... Gradient Norm: 0.0216\n",
      "Epoch 696: avg loss training: 2.0326,... Gradient Norm: 0.0252\n",
      "Epoch 697: avg loss training: 2.0326,... Gradient Norm: 0.0222\n",
      "Epoch 698: avg loss training: 2.0326,... Gradient Norm: 0.0191\n",
      "Epoch 699: avg loss training: 2.0326,... Gradient Norm: 0.0241\n",
      "Epoch 700: avg loss training: 2.0326,... Gradient Norm: 0.0244\n",
      "Epoch 701: avg loss training: 2.0326,... Gradient Norm: 0.0202\n",
      "Epoch 702: avg loss training: 2.0326,... Gradient Norm: 0.0213\n",
      "Epoch 703: avg loss training: 2.0326,... Gradient Norm: 0.0235\n",
      "Epoch 704: avg loss training: 2.0326,... Gradient Norm: 0.0191\n",
      "Epoch 705: avg loss training: 2.0326,... Gradient Norm: 0.0227\n",
      "Epoch 706: avg loss training: 2.0326,... Gradient Norm: 0.0214\n",
      "Epoch 707: avg loss training: 2.0326,... Gradient Norm: 0.0201\n",
      "Epoch 708: avg loss training: 2.0326,... Gradient Norm: 0.0189\n",
      "Epoch 709: avg loss training: 2.0326,... Gradient Norm: 0.0192\n",
      "Epoch 710: avg loss training: 2.0326,... Gradient Norm: 0.0190\n",
      "Epoch 711: avg loss training: 2.0326,... Gradient Norm: 0.0196\n",
      "Epoch 712: avg loss training: 2.0326,... Gradient Norm: 0.0218\n",
      "Epoch 713: avg loss training: 2.0326,... Gradient Norm: 0.0206\n",
      "Epoch 714: avg loss training: 2.0326,... Gradient Norm: 0.0196\n",
      "Epoch 715: avg loss training: 2.0326,... Gradient Norm: 0.0219\n",
      "Epoch 716: avg loss training: 2.0326,... Gradient Norm: 0.0218\n",
      "Epoch 717: avg loss training: 2.0326,... Gradient Norm: 0.0195\n",
      "Epoch 718: avg loss training: 2.0326,... Gradient Norm: 0.0205\n",
      "Epoch 719: avg loss training: 2.0326,... Gradient Norm: 0.0215\n",
      "Epoch 720: avg loss training: 2.0326,... Gradient Norm: 0.0210\n",
      "Epoch 721: avg loss training: 2.0326,... Gradient Norm: 0.0227\n",
      "Epoch 722: avg loss training: 2.0326,... Gradient Norm: 0.0210\n",
      "Epoch 723: avg loss training: 2.0326,... Gradient Norm: 0.0214\n",
      "Epoch 724: avg loss training: 2.0326,... Gradient Norm: 0.0216\n",
      "Epoch 725: avg loss training: 2.0326,... Gradient Norm: 0.0245\n",
      "Epoch 726: avg loss training: 2.0326,... Gradient Norm: 0.0227\n",
      "Epoch 727: avg loss training: 2.0326,... Gradient Norm: 0.0225\n",
      "Epoch 728: avg loss training: 2.0326,... Gradient Norm: 0.0198\n",
      "Epoch 729: avg loss training: 2.0326,... Gradient Norm: 0.0196\n",
      "Epoch 730: avg loss training: 2.0326,... Gradient Norm: 0.0202\n",
      "Epoch 731: avg loss training: 2.0326,... Gradient Norm: 0.0202\n",
      "Epoch 732: avg loss training: 2.0326,... Gradient Norm: 0.0191\n",
      "Epoch 733: avg loss training: 2.0326,... Gradient Norm: 0.0198\n",
      "Epoch 734: avg loss training: 2.0326,... Gradient Norm: 0.0193\n",
      "Epoch 735: avg loss training: 2.0326,... Gradient Norm: 0.0193\n",
      "Epoch 736: avg loss training: 2.0326,... Gradient Norm: 0.0191\n",
      "Epoch 737: avg loss training: 2.0326,... Gradient Norm: 0.0188\n",
      "Epoch 738: avg loss training: 2.0326,... Gradient Norm: 0.0209\n",
      "Epoch 739: avg loss training: 2.0326,... Gradient Norm: 0.0210\n",
      "Epoch 740: avg loss training: 2.0326,... Gradient Norm: 0.0224\n",
      "Epoch 741: avg loss training: 2.0326,... Gradient Norm: 0.0232\n",
      "Epoch 742: avg loss training: 2.0326,... Gradient Norm: 0.0198\n",
      "Epoch 743: avg loss training: 2.0326,... Gradient Norm: 0.0216\n",
      "Epoch 744: avg loss training: 2.0326,... Gradient Norm: 0.0201\n",
      "Epoch 745: avg loss training: 2.0326,... Gradient Norm: 0.0215\n",
      "Epoch 746: avg loss training: 2.0326,... Gradient Norm: 0.0209\n",
      "Epoch 747: avg loss training: 2.0326,... Gradient Norm: 0.0264\n",
      "Epoch 748: avg loss training: 2.0326,... Gradient Norm: 0.0202\n",
      "Epoch 749: avg loss training: 2.0326,... Gradient Norm: 0.0193\n",
      "Epoch 750: avg loss training: 2.0326,... Gradient Norm: 0.0207\n",
      "Epoch 751: avg loss training: 2.0326,... Gradient Norm: 0.0211\n",
      "Epoch 752: avg loss training: 2.0326,... Gradient Norm: 0.0207\n",
      "Epoch 753: avg loss training: 2.0326,... Gradient Norm: 0.0213\n",
      "Epoch 754: avg loss training: 2.0326,... Gradient Norm: 0.0254\n",
      "Epoch 755: avg loss training: 2.0326,... Gradient Norm: 0.0195\n",
      "Epoch 756: avg loss training: 2.0326,... Gradient Norm: 0.0194\n",
      "Epoch 757: avg loss training: 2.0326,... Gradient Norm: 0.0208\n",
      "Epoch 758: avg loss training: 2.0326,... Gradient Norm: 0.0212\n",
      "Epoch 759: avg loss training: 2.0326,... Gradient Norm: 0.0192\n",
      "Epoch 760: avg loss training: 2.0326,... Gradient Norm: 0.0217\n",
      "Epoch 761: avg loss training: 2.0326,... Gradient Norm: 0.0205\n",
      "Epoch 762: avg loss training: 2.0326,... Gradient Norm: 0.0198\n",
      "Epoch 763: avg loss training: 2.0326,... Gradient Norm: 0.0230\n",
      "Epoch 764: avg loss training: 2.0326,... Gradient Norm: 0.0194\n",
      "Epoch 765: avg loss training: 2.0326,... Gradient Norm: 0.0208\n",
      "Epoch 766: avg loss training: 2.0326,... Gradient Norm: 0.0201\n",
      "Epoch 767: avg loss training: 2.0326,... Gradient Norm: 0.0201\n",
      "Epoch 768: avg loss training: 2.0326,... Gradient Norm: 0.0234\n",
      "Epoch 769: avg loss training: 2.0326,... Gradient Norm: 0.0213\n",
      "Epoch 770: avg loss training: 2.0326,... Gradient Norm: 0.0201\n",
      "Epoch 771: avg loss training: 2.0326,... Gradient Norm: 0.0202\n",
      "Epoch 772: avg loss training: 2.0326,... Gradient Norm: 0.0188\n",
      "Epoch 773: avg loss training: 2.0326,... Gradient Norm: 0.0199\n",
      "Epoch 774: avg loss training: 2.0326,... Gradient Norm: 0.0192\n",
      "Epoch 775: avg loss training: 2.0326,... Gradient Norm: 0.0188\n",
      "Epoch 776: avg loss training: 2.0326,... Gradient Norm: 0.0195\n",
      "Epoch 777: avg loss training: 2.0326,... Gradient Norm: 0.0205\n",
      "Epoch 778: avg loss training: 2.0326,... Gradient Norm: 0.0234\n",
      "Epoch 779: avg loss training: 2.0326,... Gradient Norm: 0.0256\n",
      "Epoch 780: avg loss training: 2.0326,... Gradient Norm: 0.0223\n",
      "Epoch 781: avg loss training: 2.0326,... Gradient Norm: 0.0225\n",
      "Epoch 782: avg loss training: 2.0326,... Gradient Norm: 0.0218\n",
      "Epoch 783: avg loss training: 2.0326,... Gradient Norm: 0.0204\n",
      "Epoch 784: avg loss training: 2.0326,... Gradient Norm: 0.0211\n",
      "Epoch 785: avg loss training: 2.0326,... Gradient Norm: 0.0207\n",
      "Epoch 786: avg loss training: 2.0326,... Gradient Norm: 0.0216\n",
      "Epoch 787: avg loss training: 2.0326,... Gradient Norm: 0.0201\n",
      "Epoch 788: avg loss training: 2.0326,... Gradient Norm: 0.0212\n",
      "Epoch 789: avg loss training: 2.0326,... Gradient Norm: 0.0210\n",
      "Epoch 790: avg loss training: 2.0326,... Gradient Norm: 0.0224\n",
      "Epoch 791: avg loss training: 2.0326,... Gradient Norm: 0.0231\n",
      "Epoch 792: avg loss training: 2.0326,... Gradient Norm: 0.0192\n",
      "Epoch 793: avg loss training: 2.0326,... Gradient Norm: 0.0198\n",
      "Epoch 794: avg loss training: 2.0326,... Gradient Norm: 0.0197\n",
      "Epoch 795: avg loss training: 2.0326,... Gradient Norm: 0.0197\n",
      "Epoch 796: avg loss training: 2.0326,... Gradient Norm: 0.0187\n",
      "Epoch 797: avg loss training: 2.0326,... Gradient Norm: 0.0199\n",
      "Epoch 798: avg loss training: 2.0326,... Gradient Norm: 0.0225\n",
      "Epoch 799: avg loss training: 2.0326,... Gradient Norm: 0.0193\n",
      "Epoch 800: avg loss training: 2.0326,... Gradient Norm: 0.0213\n",
      "Epoch 801: avg loss training: 2.0326,... Gradient Norm: 0.0192\n",
      "Epoch 802: avg loss training: 2.0326,... Gradient Norm: 0.0226\n",
      "Epoch 803: avg loss training: 2.0326,... Gradient Norm: 0.0236\n",
      "Epoch 804: avg loss training: 2.0326,... Gradient Norm: 0.0254\n",
      "Epoch 805: avg loss training: 2.0326,... Gradient Norm: 0.0210\n",
      "Epoch 806: avg loss training: 2.0326,... Gradient Norm: 0.0230\n",
      "Epoch 807: avg loss training: 2.0326,... Gradient Norm: 0.0222\n",
      "Epoch 808: avg loss training: 2.0326,... Gradient Norm: 0.0189\n",
      "Epoch 809: avg loss training: 2.0326,... Gradient Norm: 0.0211\n",
      "Epoch 810: avg loss training: 2.0326,... Gradient Norm: 0.0223\n",
      "Epoch 811: avg loss training: 2.0326,... Gradient Norm: 0.0206\n",
      "Epoch 812: avg loss training: 2.0326,... Gradient Norm: 0.0242\n",
      "Epoch 813: avg loss training: 2.0326,... Gradient Norm: 0.0205\n",
      "Epoch 814: avg loss training: 2.0326,... Gradient Norm: 0.0189\n",
      "Epoch 815: avg loss training: 2.0326,... Gradient Norm: 0.0214\n",
      "Epoch 816: avg loss training: 2.0326,... Gradient Norm: 0.0195\n",
      "Epoch 817: avg loss training: 2.0326,... Gradient Norm: 0.0189\n",
      "Epoch 818: avg loss training: 2.0326,... Gradient Norm: 0.0189\n",
      "Epoch 819: avg loss training: 2.0326,... Gradient Norm: 0.0225\n",
      "Epoch 820: avg loss training: 2.0326,... Gradient Norm: 0.0199\n",
      "Epoch 821: avg loss training: 2.0326,... Gradient Norm: 0.0190\n",
      "Epoch 822: avg loss training: 2.0326,... Gradient Norm: 0.0189\n",
      "Epoch 823: avg loss training: 2.0326,... Gradient Norm: 0.0204\n",
      "Epoch 824: avg loss training: 2.0326,... Gradient Norm: 0.0204\n",
      "Epoch 825: avg loss training: 2.0326,... Gradient Norm: 0.0231\n",
      "Epoch 826: avg loss training: 2.0326,... Gradient Norm: 0.0203\n",
      "Epoch 827: avg loss training: 2.0326,... Gradient Norm: 0.0198\n",
      "Epoch 828: avg loss training: 2.0326,... Gradient Norm: 0.0198\n",
      "Epoch 829: avg loss training: 2.0326,... Gradient Norm: 0.0206\n",
      "Epoch 830: avg loss training: 2.0326,... Gradient Norm: 0.0210\n",
      "Epoch 831: avg loss training: 2.0326,... Gradient Norm: 0.0206\n",
      "Epoch 832: avg loss training: 2.0326,... Gradient Norm: 0.0188\n",
      "Epoch 833: avg loss training: 2.0326,... Gradient Norm: 0.0207\n",
      "Epoch 834: avg loss training: 2.0326,... Gradient Norm: 0.0219\n",
      "Epoch 835: avg loss training: 2.0326,... Gradient Norm: 0.0227\n",
      "Epoch 836: avg loss training: 2.0326,... Gradient Norm: 0.0201\n",
      "Epoch 837: avg loss training: 2.0326,... Gradient Norm: 0.0187\n",
      "Epoch 838: avg loss training: 2.0326,... Gradient Norm: 0.0212\n",
      "Epoch 839: avg loss training: 2.0326,... Gradient Norm: 0.0213\n",
      "Epoch 840: avg loss training: 2.0326,... Gradient Norm: 0.0200\n",
      "Epoch 841: avg loss training: 2.0326,... Gradient Norm: 0.0214\n",
      "Epoch 842: avg loss training: 2.0326,... Gradient Norm: 0.0207\n",
      "Epoch 843: avg loss training: 2.0326,... Gradient Norm: 0.0190\n",
      "Epoch 844: avg loss training: 2.0326,... Gradient Norm: 0.0211\n",
      "Epoch 845: avg loss training: 2.0326,... Gradient Norm: 0.0208\n",
      "Epoch 846: avg loss training: 2.0326,... Gradient Norm: 0.0195\n",
      "Epoch 847: avg loss training: 2.0326,... Gradient Norm: 0.0235\n",
      "Epoch 848: avg loss training: 2.0326,... Gradient Norm: 0.0219\n",
      "Epoch 849: avg loss training: 2.0326,... Gradient Norm: 0.0213\n",
      "Epoch 850: avg loss training: 2.0326,... Gradient Norm: 0.0210\n",
      "Epoch 851: avg loss training: 2.0326,... Gradient Norm: 0.0220\n",
      "Epoch 852: avg loss training: 2.0326,... Gradient Norm: 0.0209\n",
      "Epoch 853: avg loss training: 2.0326,... Gradient Norm: 0.0221\n",
      "Epoch 854: avg loss training: 2.0326,... Gradient Norm: 0.0226\n",
      "Epoch 855: avg loss training: 2.0326,... Gradient Norm: 0.0195\n",
      "Epoch 856: avg loss training: 2.0326,... Gradient Norm: 0.0200\n",
      "Epoch 857: avg loss training: 2.0326,... Gradient Norm: 0.0198\n",
      "Epoch 858: avg loss training: 2.0326,... Gradient Norm: 0.0188\n",
      "Epoch 859: avg loss training: 2.0326,... Gradient Norm: 0.0246\n",
      "Epoch 860: avg loss training: 2.0326,... Gradient Norm: 0.0198\n",
      "Epoch 861: avg loss training: 2.0326,... Gradient Norm: 0.0208\n",
      "Epoch 862: avg loss training: 2.0326,... Gradient Norm: 0.0206\n",
      "Epoch 863: avg loss training: 2.0326,... Gradient Norm: 0.0195\n",
      "Epoch 864: avg loss training: 2.0326,... Gradient Norm: 0.0217\n",
      "Epoch 865: avg loss training: 2.0326,... Gradient Norm: 0.0212\n",
      "Epoch 866: avg loss training: 2.0326,... Gradient Norm: 0.0251\n",
      "Epoch 867: avg loss training: 2.0326,... Gradient Norm: 0.0255\n",
      "Epoch 868: avg loss training: 2.0326,... Gradient Norm: 0.0236\n",
      "Epoch 869: avg loss training: 2.0326,... Gradient Norm: 0.0252\n",
      "Epoch 870: avg loss training: 2.0326,... Gradient Norm: 0.0217\n",
      "Epoch 871: avg loss training: 2.0326,... Gradient Norm: 0.0219\n",
      "Epoch 872: avg loss training: 2.0326,... Gradient Norm: 0.0273\n",
      "Epoch 873: avg loss training: 2.0326,... Gradient Norm: 0.0192\n",
      "Epoch 874: avg loss training: 2.0326,... Gradient Norm: 0.0206\n",
      "Epoch 875: avg loss training: 2.0326,... Gradient Norm: 0.0229\n",
      "Epoch 876: avg loss training: 2.0326,... Gradient Norm: 0.0210\n",
      "Epoch 877: avg loss training: 2.0326,... Gradient Norm: 0.0189\n",
      "Epoch 878: avg loss training: 2.0326,... Gradient Norm: 0.0200\n",
      "Epoch 879: avg loss training: 2.0326,... Gradient Norm: 0.0209\n",
      "Epoch 880: avg loss training: 2.0326,... Gradient Norm: 0.0185\n",
      "Epoch 881: avg loss training: 2.0326,... Gradient Norm: 0.0186\n",
      "Epoch 882: avg loss training: 2.0326,... Gradient Norm: 0.0255\n",
      "Epoch 883: avg loss training: 2.0326,... Gradient Norm: 0.0185\n",
      "Epoch 884: avg loss training: 2.0326,... Gradient Norm: 0.0194\n",
      "Epoch 885: avg loss training: 2.0326,... Gradient Norm: 0.0219\n",
      "Epoch 886: avg loss training: 2.0326,... Gradient Norm: 0.0195\n",
      "Epoch 887: avg loss training: 2.0326,... Gradient Norm: 0.0205\n",
      "Epoch 888: avg loss training: 2.0326,... Gradient Norm: 0.0201\n",
      "Epoch 889: avg loss training: 2.0326,... Gradient Norm: 0.0193\n",
      "Epoch 890: avg loss training: 2.0326,... Gradient Norm: 0.0260\n",
      "Epoch 891: avg loss training: 2.0326,... Gradient Norm: 0.0201\n",
      "Epoch 892: avg loss training: 2.0326,... Gradient Norm: 0.0206\n",
      "Epoch 893: avg loss training: 2.0326,... Gradient Norm: 0.0212\n",
      "Epoch 894: avg loss training: 2.0326,... Gradient Norm: 0.0199\n",
      "Epoch 895: avg loss training: 2.0326,... Gradient Norm: 0.0213\n",
      "Epoch 896: avg loss training: 2.0326,... Gradient Norm: 0.0222\n",
      "Epoch 897: avg loss training: 2.0326,... Gradient Norm: 0.0212\n",
      "Epoch 898: avg loss training: 2.0326,... Gradient Norm: 0.0194\n",
      "Epoch 899: avg loss training: 2.0326,... Gradient Norm: 0.0221\n",
      "Epoch 900: avg loss training: 2.0326,... Gradient Norm: 0.0196\n",
      "Epoch 901: avg loss training: 2.0326,... Gradient Norm: 0.0208\n",
      "Epoch 902: avg loss training: 2.0326,... Gradient Norm: 0.0187\n",
      "Epoch 903: avg loss training: 2.0326,... Gradient Norm: 0.0219\n",
      "Epoch 904: avg loss training: 2.0326,... Gradient Norm: 0.0220\n",
      "Epoch 905: avg loss training: 2.0326,... Gradient Norm: 0.0251\n",
      "Epoch 906: avg loss training: 2.0326,... Gradient Norm: 0.0208\n",
      "Epoch 907: avg loss training: 2.0326,... Gradient Norm: 0.0194\n",
      "Epoch 908: avg loss training: 2.0326,... Gradient Norm: 0.0223\n",
      "Epoch 909: avg loss training: 2.0326,... Gradient Norm: 0.0221\n",
      "Epoch 910: avg loss training: 2.0326,... Gradient Norm: 0.0195\n",
      "Epoch 911: avg loss training: 2.0326,... Gradient Norm: 0.0268\n",
      "Epoch 912: avg loss training: 2.0326,... Gradient Norm: 0.0203\n",
      "Epoch 913: avg loss training: 2.0326,... Gradient Norm: 0.0231\n",
      "Epoch 914: avg loss training: 2.0326,... Gradient Norm: 0.0215\n",
      "Epoch 915: avg loss training: 2.0326,... Gradient Norm: 0.0188\n",
      "Epoch 916: avg loss training: 2.0326,... Gradient Norm: 0.0193\n",
      "Epoch 917: avg loss training: 2.0326,... Gradient Norm: 0.0206\n",
      "Epoch 918: avg loss training: 2.0326,... Gradient Norm: 0.0194\n",
      "Epoch 919: avg loss training: 2.0326,... Gradient Norm: 0.0233\n",
      "Epoch 920: avg loss training: 2.0326,... Gradient Norm: 0.0220\n",
      "Epoch 921: avg loss training: 2.0326,... Gradient Norm: 0.0185\n",
      "Epoch 922: avg loss training: 2.0326,... Gradient Norm: 0.0188\n",
      "Epoch 923: avg loss training: 2.0326,... Gradient Norm: 0.0203\n",
      "Epoch 924: avg loss training: 2.0326,... Gradient Norm: 0.0200\n",
      "Epoch 925: avg loss training: 2.0326,... Gradient Norm: 0.0229\n",
      "Epoch 926: avg loss training: 2.0326,... Gradient Norm: 0.0227\n",
      "Epoch 927: avg loss training: 2.0326,... Gradient Norm: 0.0186\n",
      "Epoch 928: avg loss training: 2.0326,... Gradient Norm: 0.0259\n",
      "Epoch 929: avg loss training: 2.0326,... Gradient Norm: 0.0200\n",
      "Epoch 930: avg loss training: 2.0326,... Gradient Norm: 0.0215\n",
      "Epoch 931: avg loss training: 2.0326,... Gradient Norm: 0.0215\n",
      "Epoch 932: avg loss training: 2.0326,... Gradient Norm: 0.0202\n",
      "Epoch 933: avg loss training: 2.0326,... Gradient Norm: 0.0194\n",
      "Epoch 934: avg loss training: 2.0326,... Gradient Norm: 0.0206\n",
      "Epoch 935: avg loss training: 2.0326,... Gradient Norm: 0.0187\n",
      "Epoch 936: avg loss training: 2.0326,... Gradient Norm: 0.0188\n",
      "Epoch 937: avg loss training: 2.0326,... Gradient Norm: 0.0210\n",
      "Epoch 938: avg loss training: 2.0326,... Gradient Norm: 0.0200\n",
      "Epoch 939: avg loss training: 2.0326,... Gradient Norm: 0.0200\n",
      "Epoch 940: avg loss training: 2.0326,... Gradient Norm: 0.0263\n",
      "Epoch 941: avg loss training: 2.0326,... Gradient Norm: 0.0230\n",
      "Epoch 942: avg loss training: 2.0326,... Gradient Norm: 0.0216\n",
      "Epoch 943: avg loss training: 2.0326,... Gradient Norm: 0.0237\n",
      "Epoch 944: avg loss training: 2.0326,... Gradient Norm: 0.0209\n",
      "Epoch 945: avg loss training: 2.0326,... Gradient Norm: 0.0215\n",
      "Epoch 946: avg loss training: 2.0326,... Gradient Norm: 0.0231\n",
      "Epoch 947: avg loss training: 2.0326,... Gradient Norm: 0.0202\n",
      "Epoch 948: avg loss training: 2.0326,... Gradient Norm: 0.0217\n",
      "Epoch 949: avg loss training: 2.0326,... Gradient Norm: 0.0216\n",
      "Epoch 950: avg loss training: 2.0326,... Gradient Norm: 0.0192\n",
      "Epoch 951: avg loss training: 2.0326,... Gradient Norm: 0.0211\n",
      "Epoch 952: avg loss training: 2.0326,... Gradient Norm: 0.0246\n",
      "Epoch 953: avg loss training: 2.0326,... Gradient Norm: 0.0186\n",
      "Epoch 954: avg loss training: 2.0326,... Gradient Norm: 0.0215\n",
      "Epoch 955: avg loss training: 2.0326,... Gradient Norm: 0.0206\n",
      "Epoch 956: avg loss training: 2.0326,... Gradient Norm: 0.0189\n",
      "Epoch 957: avg loss training: 2.0326,... Gradient Norm: 0.0210\n",
      "Epoch 958: avg loss training: 2.0326,... Gradient Norm: 0.0206\n",
      "Epoch 959: avg loss training: 2.0326,... Gradient Norm: 0.0222\n",
      "Epoch 960: avg loss training: 2.0326,... Gradient Norm: 0.0195\n",
      "Epoch 961: avg loss training: 2.0326,... Gradient Norm: 0.0217\n",
      "Epoch 962: avg loss training: 2.0326,... Gradient Norm: 0.0218\n",
      "Epoch 963: avg loss training: 2.0326,... Gradient Norm: 0.0191\n",
      "Epoch 964: avg loss training: 2.0326,... Gradient Norm: 0.0265\n",
      "Epoch 965: avg loss training: 2.0326,... Gradient Norm: 0.0206\n",
      "Epoch 966: avg loss training: 2.0326,... Gradient Norm: 0.0196\n",
      "Epoch 967: avg loss training: 2.0326,... Gradient Norm: 0.0247\n",
      "Epoch 968: avg loss training: 2.0326,... Gradient Norm: 0.0234\n",
      "Epoch 969: avg loss training: 2.0326,... Gradient Norm: 0.0207\n",
      "Epoch 970: avg loss training: 2.0326,... Gradient Norm: 0.0196\n",
      "Epoch 971: avg loss training: 2.0326,... Gradient Norm: 0.0275\n",
      "Epoch 972: avg loss training: 2.0326,... Gradient Norm: 0.0212\n",
      "Epoch 973: avg loss training: 2.0326,... Gradient Norm: 0.0189\n",
      "Epoch 974: avg loss training: 2.0326,... Gradient Norm: 0.0221\n",
      "Epoch 975: avg loss training: 2.0326,... Gradient Norm: 0.0267\n",
      "Epoch 976: avg loss training: 2.0326,... Gradient Norm: 0.0239\n",
      "Epoch 977: avg loss training: 2.0326,... Gradient Norm: 0.0210\n",
      "Epoch 978: avg loss training: 2.0326,... Gradient Norm: 0.0204\n",
      "Epoch 979: avg loss training: 2.0326,... Gradient Norm: 0.0261\n",
      "Epoch 980: avg loss training: 2.0326,... Gradient Norm: 0.0256\n",
      "Epoch 981: avg loss training: 2.0326,... Gradient Norm: 0.0191\n",
      "Epoch 982: avg loss training: 2.0326,... Gradient Norm: 0.0226\n",
      "Epoch 983: avg loss training: 2.0326,... Gradient Norm: 0.0217\n",
      "Epoch 984: avg loss training: 2.0326,... Gradient Norm: 0.0240\n",
      "Epoch 985: avg loss training: 2.0326,... Gradient Norm: 0.0192\n",
      "Epoch 986: avg loss training: 2.0326,... Gradient Norm: 0.0192\n",
      "Epoch 987: avg loss training: 2.0326,... Gradient Norm: 0.0196\n",
      "Epoch 988: avg loss training: 2.0326,... Gradient Norm: 0.0198\n",
      "Epoch 989: avg loss training: 2.0326,... Gradient Norm: 0.0199\n",
      "Epoch 990: avg loss training: 2.0326,... Gradient Norm: 0.0200\n",
      "Epoch 991: avg loss training: 2.0326,... Gradient Norm: 0.0182\n",
      "Epoch 992: avg loss training: 2.0326,... Gradient Norm: 0.0213\n",
      "Epoch 993: avg loss training: 2.0326,... Gradient Norm: 0.0212\n",
      "Epoch 994: avg loss training: 2.0326,... Gradient Norm: 0.0192\n",
      "Epoch 995: avg loss training: 2.0326,... Gradient Norm: 0.0191\n",
      "Epoch 996: avg loss training: 2.0326,... Gradient Norm: 0.0189\n",
      "Epoch 997: avg loss training: 2.0326,... Gradient Norm: 0.0204\n",
      "Epoch 998: avg loss training: 2.0326,... Gradient Norm: 0.0236\n",
      "Epoch 999: avg loss training: 2.0326,... Gradient Norm: 0.0227\n",
      "Epoch 1000: avg loss training: 2.0326,... Gradient Norm: 0.0206\n",
      "final result: (0.9770695267705247, 1.9764839707402513) ********************\n"
     ]
    }
   ],
   "source": [
    "score_new, score_orig, model_x, model_y, new_x_f, new_x_r, f_forward, f_reverse= loci_w_marginal(\n",
    "    x, y, independence_test=False, neural_network=True, \n",
    "    return_function=True, n_steps=1000, marginal_loglik = True\n",
    ")\n",
    "\n",
    "print(f\"final result: {score_new, score_orig} ********************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928a7bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: avg loss training: 3.7508,... Gradient Norm: 11.8301\n",
      "Epoch 2: avg loss training: 22.2824,... Gradient Norm: 239.9918\n",
      "Epoch 3: avg loss training: 89.0588,... Gradient Norm: 1446.8469\n",
      "Epoch 4: avg loss training: 7.9198,... Gradient Norm: 49.1776\n",
      "Epoch 5: avg loss training: 8.2388,... Gradient Norm: 27.6370\n",
      "Epoch 6: avg loss training: 9.1583,... Gradient Norm: 31.8784\n",
      "Epoch 7: avg loss training: 7.9223,... Gradient Norm: 18.6161\n",
      "Epoch 8: avg loss training: 7.5675,... Gradient Norm: 24.4812\n",
      "Epoch 9: avg loss training: 6.8327,... Gradient Norm: 20.0497\n",
      "Epoch 10: avg loss training: 5.9662,... Gradient Norm: 12.8380\n",
      "Epoch 11: avg loss training: 5.3454,... Gradient Norm: 10.2864\n",
      "Epoch 12: avg loss training: 4.9028,... Gradient Norm: 11.0880\n",
      "Epoch 13: avg loss training: 4.5266,... Gradient Norm: 8.4915\n",
      "Epoch 14: avg loss training: 4.4930,... Gradient Norm: 15.0251\n",
      "Epoch 15: avg loss training: 4.6255,... Gradient Norm: 33.1632\n",
      "Epoch 16: avg loss training: 4.2109,... Gradient Norm: 13.0217\n",
      "Epoch 17: avg loss training: 4.0118,... Gradient Norm: 10.6397\n",
      "Epoch 18: avg loss training: 4.0249,... Gradient Norm: 18.6273\n",
      "Epoch 19: avg loss training: 3.8687,... Gradient Norm: 12.4597\n",
      "Epoch 20: avg loss training: 3.7247,... Gradient Norm: 7.1893\n",
      "Epoch 21: avg loss training: 3.6129,... Gradient Norm: 4.6363\n",
      "Epoch 22: avg loss training: 3.5985,... Gradient Norm: 10.8972\n",
      "Epoch 23: avg loss training: 3.5832,... Gradient Norm: 14.6228\n",
      "Epoch 24: avg loss training: 3.4629,... Gradient Norm: 7.4244\n",
      "Epoch 25: avg loss training: 3.4030,... Gradient Norm: 7.8808\n",
      "Epoch 26: avg loss training: 3.3597,... Gradient Norm: 11.5277\n",
      "Epoch 27: avg loss training: 3.2528,... Gradient Norm: 2.8687\n",
      "Epoch 28: avg loss training: 3.2189,... Gradient Norm: 7.6818\n",
      "Epoch 29: avg loss training: 3.1401,... Gradient Norm: 2.2853\n",
      "Epoch 30: avg loss training: 3.1143,... Gradient Norm: 6.7704\n",
      "Epoch 31: avg loss training: 3.0526,... Gradient Norm: 2.7172\n",
      "Epoch 32: avg loss training: 3.0155,... Gradient Norm: 3.3518\n",
      "Epoch 33: avg loss training: 2.9803,... Gradient Norm: 2.5695\n",
      "Epoch 34: avg loss training: 2.9607,... Gradient Norm: 4.7628\n",
      "Epoch 35: avg loss training: 2.9211,... Gradient Norm: 3.3599\n",
      "Epoch 36: avg loss training: 2.8930,... Gradient Norm: 2.6866\n",
      "Epoch 37: avg loss training: 2.8828,... Gradient Norm: 4.4230\n",
      "Epoch 38: avg loss training: 2.8617,... Gradient Norm: 1.3255\n",
      "Epoch 39: avg loss training: 2.8651,... Gradient Norm: 4.4115\n",
      "Epoch 40: avg loss training: 2.8495,... Gradient Norm: 1.6151\n",
      "Epoch 41: avg loss training: 2.8490,... Gradient Norm: 3.9713\n",
      "Epoch 42: avg loss training: 2.8434,... Gradient Norm: 2.0220\n",
      "Epoch 43: avg loss training: 2.8363,... Gradient Norm: 2.9073\n",
      "Epoch 44: avg loss training: 2.8270,... Gradient Norm: 1.7383\n",
      "Epoch 45: avg loss training: 2.8253,... Gradient Norm: 2.5486\n",
      "Epoch 46: avg loss training: 2.8219,... Gradient Norm: 2.4891\n",
      "Epoch 47: avg loss training: 2.8182,... Gradient Norm: 2.1112\n",
      "Epoch 48: avg loss training: 2.8188,... Gradient Norm: 2.0717\n",
      "Epoch 49: avg loss training: 2.8164,... Gradient Norm: 1.7812\n",
      "Epoch 50: avg loss training: 2.8141,... Gradient Norm: 2.1619\n",
      "Epoch 51: avg loss training: 2.8092,... Gradient Norm: 1.7487\n",
      "Epoch 52: avg loss training: 2.8082,... Gradient Norm: 2.1217\n",
      "Epoch 53: avg loss training: 2.8041,... Gradient Norm: 1.6188\n",
      "Epoch 54: avg loss training: 2.8046,... Gradient Norm: 1.8782\n",
      "Epoch 55: avg loss training: 2.8033,... Gradient Norm: 1.9009\n",
      "Epoch 56: avg loss training: 2.8022,... Gradient Norm: 1.7331\n",
      "Epoch 57: avg loss training: 2.8003,... Gradient Norm: 1.7157\n",
      "Epoch 58: avg loss training: 2.7986,... Gradient Norm: 1.3992\n",
      "Epoch 59: avg loss training: 2.7966,... Gradient Norm: 1.7227\n",
      "Epoch 60: avg loss training: 2.7943,... Gradient Norm: 1.0371\n",
      "Epoch 61: avg loss training: 2.7946,... Gradient Norm: 1.6707\n",
      "Epoch 62: avg loss training: 2.7927,... Gradient Norm: 0.5842\n",
      "Epoch 63: avg loss training: 2.7930,... Gradient Norm: 1.6876\n",
      "Epoch 64: avg loss training: 2.7912,... Gradient Norm: 0.4435\n",
      "Epoch 65: avg loss training: 2.7912,... Gradient Norm: 1.4640\n",
      "Epoch 66: avg loss training: 2.7892,... Gradient Norm: 0.4180\n",
      "Epoch 67: avg loss training: 2.7889,... Gradient Norm: 1.1207\n",
      "Epoch 68: avg loss training: 2.7880,... Gradient Norm: 0.5297\n",
      "Epoch 69: avg loss training: 2.7877,... Gradient Norm: 0.8566\n",
      "Epoch 70: avg loss training: 2.7873,... Gradient Norm: 0.6133\n",
      "Epoch 71: avg loss training: 2.7868,... Gradient Norm: 0.6155\n",
      "Epoch 72: avg loss training: 2.7863,... Gradient Norm: 0.7830\n",
      "Epoch 73: avg loss training: 2.7853,... Gradient Norm: 0.4033\n",
      "Epoch 74: avg loss training: 2.7850,... Gradient Norm: 0.7144\n",
      "Epoch 75: avg loss training: 2.7841,... Gradient Norm: 0.3290\n",
      "Epoch 76: avg loss training: 2.7838,... Gradient Norm: 0.7287\n",
      "Epoch 77: avg loss training: 2.7832,... Gradient Norm: 0.3170\n",
      "Epoch 78: avg loss training: 2.7828,... Gradient Norm: 0.5143\n",
      "Epoch 79: avg loss training: 2.7821,... Gradient Norm: 0.3468\n",
      "Epoch 80: avg loss training: 2.7817,... Gradient Norm: 0.4861\n",
      "Epoch 81: avg loss training: 2.7812,... Gradient Norm: 0.3607\n",
      "Epoch 82: avg loss training: 2.7809,... Gradient Norm: 0.5244\n",
      "Epoch 83: avg loss training: 2.7804,... Gradient Norm: 0.3399\n",
      "Epoch 84: avg loss training: 2.7802,... Gradient Norm: 0.4937\n",
      "Epoch 85: avg loss training: 2.7797,... Gradient Norm: 0.3566\n",
      "Epoch 86: avg loss training: 2.7793,... Gradient Norm: 0.4915\n",
      "Epoch 87: avg loss training: 2.7788,... Gradient Norm: 0.1760\n",
      "Epoch 88: avg loss training: 2.7785,... Gradient Norm: 0.2336\n",
      "Epoch 89: avg loss training: 2.7781,... Gradient Norm: 0.2454\n",
      "Epoch 90: avg loss training: 2.7777,... Gradient Norm: 0.2030\n",
      "Epoch 91: avg loss training: 2.7773,... Gradient Norm: 0.2517\n",
      "Epoch 92: avg loss training: 2.7768,... Gradient Norm: 0.3606\n",
      "Epoch 93: avg loss training: 2.7764,... Gradient Norm: 0.2086\n",
      "Epoch 94: avg loss training: 2.7760,... Gradient Norm: 0.2233\n",
      "Epoch 95: avg loss training: 2.7757,... Gradient Norm: 0.2689\n",
      "Epoch 96: avg loss training: 2.7753,... Gradient Norm: 0.1420\n",
      "Epoch 97: avg loss training: 2.7750,... Gradient Norm: 0.3557\n",
      "Epoch 98: avg loss training: 2.7747,... Gradient Norm: 0.5619\n",
      "Epoch 99: avg loss training: 2.7744,... Gradient Norm: 0.2608\n",
      "Epoch 100: avg loss training: 2.7740,... Gradient Norm: 0.3699\n",
      "Epoch 101: avg loss training: 2.7738,... Gradient Norm: 0.5768\n",
      "Epoch 102: avg loss training: 2.7734,... Gradient Norm: 0.3262\n",
      "Epoch 103: avg loss training: 2.7730,... Gradient Norm: 0.2067\n",
      "Epoch 104: avg loss training: 2.7727,... Gradient Norm: 0.4080\n",
      "Epoch 105: avg loss training: 2.7725,... Gradient Norm: 0.5257\n",
      "Epoch 106: avg loss training: 2.7721,... Gradient Norm: 0.1066\n",
      "Epoch 107: avg loss training: 2.7718,... Gradient Norm: 0.5073\n",
      "Epoch 108: avg loss training: 2.7716,... Gradient Norm: 0.4599\n",
      "Epoch 109: avg loss training: 2.7712,... Gradient Norm: 0.1777\n",
      "Epoch 110: avg loss training: 2.7709,... Gradient Norm: 0.2890\n",
      "Epoch 111: avg loss training: 2.7706,... Gradient Norm: 0.3565\n",
      "Epoch 112: avg loss training: 2.7703,... Gradient Norm: 0.1433\n",
      "Epoch 113: avg loss training: 2.7701,... Gradient Norm: 0.2115\n",
      "Epoch 114: avg loss training: 2.7698,... Gradient Norm: 0.3121\n",
      "Epoch 115: avg loss training: 2.7695,... Gradient Norm: 0.2560\n",
      "Epoch 116: avg loss training: 2.7692,... Gradient Norm: 0.0941\n",
      "Epoch 117: avg loss training: 2.7690,... Gradient Norm: 0.3044\n",
      "Epoch 118: avg loss training: 2.7688,... Gradient Norm: 0.2291\n",
      "Epoch 119: avg loss training: 2.7685,... Gradient Norm: 0.1291\n",
      "Epoch 120: avg loss training: 2.7683,... Gradient Norm: 0.2210\n",
      "Epoch 121: avg loss training: 2.7681,... Gradient Norm: 0.2664\n",
      "Epoch 122: avg loss training: 2.7678,... Gradient Norm: 0.1325\n",
      "Epoch 123: avg loss training: 2.7676,... Gradient Norm: 0.3056\n",
      "Epoch 124: avg loss training: 2.7674,... Gradient Norm: 0.3153\n",
      "Epoch 125: avg loss training: 2.7671,... Gradient Norm: 0.1251\n",
      "Epoch 126: avg loss training: 2.7669,... Gradient Norm: 0.2237\n",
      "Epoch 127: avg loss training: 2.7667,... Gradient Norm: 0.2834\n",
      "Epoch 128: avg loss training: 2.7665,... Gradient Norm: 0.1747\n",
      "Epoch 129: avg loss training: 2.7663,... Gradient Norm: 0.1633\n",
      "Epoch 130: avg loss training: 2.7661,... Gradient Norm: 0.2828\n",
      "Epoch 131: avg loss training: 2.7659,... Gradient Norm: 0.1830\n",
      "Epoch 132: avg loss training: 2.7657,... Gradient Norm: 0.1330\n",
      "Epoch 133: avg loss training: 2.7655,... Gradient Norm: 0.2601\n",
      "Epoch 134: avg loss training: 2.7653,... Gradient Norm: 0.1278\n",
      "Epoch 135: avg loss training: 2.7652,... Gradient Norm: 0.1762\n",
      "Epoch 136: avg loss training: 2.7650,... Gradient Norm: 0.2338\n",
      "Epoch 137: avg loss training: 2.7648,... Gradient Norm: 0.2580\n",
      "Epoch 138: avg loss training: 2.7646,... Gradient Norm: 0.1019\n",
      "Epoch 139: avg loss training: 2.7645,... Gradient Norm: 0.1993\n",
      "Epoch 140: avg loss training: 2.7643,... Gradient Norm: 0.2130\n",
      "Epoch 141: avg loss training: 2.7641,... Gradient Norm: 0.1260\n",
      "Epoch 142: avg loss training: 2.7640,... Gradient Norm: 0.1595\n",
      "Epoch 143: avg loss training: 2.7638,... Gradient Norm: 0.1649\n",
      "Epoch 144: avg loss training: 2.7637,... Gradient Norm: 0.1584\n",
      "Epoch 145: avg loss training: 2.7635,... Gradient Norm: 0.0874\n",
      "Epoch 146: avg loss training: 2.7633,... Gradient Norm: 0.1100\n",
      "Epoch 147: avg loss training: 2.7632,... Gradient Norm: 0.1041\n",
      "Epoch 148: avg loss training: 2.7630,... Gradient Norm: 0.1571\n",
      "Epoch 149: avg loss training: 2.7629,... Gradient Norm: 0.1140\n",
      "Epoch 150: avg loss training: 2.7627,... Gradient Norm: 0.2161\n",
      "Epoch 151: avg loss training: 2.7626,... Gradient Norm: 0.1975\n",
      "Epoch 152: avg loss training: 2.7624,... Gradient Norm: 0.1399\n",
      "Epoch 153: avg loss training: 2.7623,... Gradient Norm: 0.1276\n",
      "Epoch 154: avg loss training: 2.7621,... Gradient Norm: 0.0946\n",
      "Epoch 155: avg loss training: 2.7620,... Gradient Norm: 0.0901\n",
      "Epoch 156: avg loss training: 2.7618,... Gradient Norm: 0.1026\n",
      "Epoch 157: avg loss training: 2.7617,... Gradient Norm: 0.1961\n",
      "Epoch 158: avg loss training: 2.7616,... Gradient Norm: 0.2273\n",
      "Epoch 159: avg loss training: 2.7614,... Gradient Norm: 0.1438\n",
      "Epoch 160: avg loss training: 2.7613,... Gradient Norm: 0.1079\n",
      "Epoch 161: avg loss training: 2.7612,... Gradient Norm: 0.0696\n",
      "Epoch 162: avg loss training: 2.7611,... Gradient Norm: 0.0796\n",
      "Epoch 163: avg loss training: 2.7609,... Gradient Norm: 0.1059\n",
      "Epoch 164: avg loss training: 2.7608,... Gradient Norm: 0.0873\n",
      "Epoch 165: avg loss training: 2.7607,... Gradient Norm: 0.0581\n",
      "Epoch 166: avg loss training: 2.7606,... Gradient Norm: 0.0565\n",
      "Epoch 167: avg loss training: 2.7605,... Gradient Norm: 0.0936\n",
      "Epoch 168: avg loss training: 2.7604,... Gradient Norm: 0.1263\n",
      "Epoch 169: avg loss training: 2.7603,... Gradient Norm: 0.1141\n",
      "Epoch 170: avg loss training: 2.7601,... Gradient Norm: 0.1375\n",
      "Epoch 171: avg loss training: 2.7600,... Gradient Norm: 0.0647\n",
      "Epoch 172: avg loss training: 2.7600,... Gradient Norm: 0.2338\n",
      "Epoch 173: avg loss training: 2.7599,... Gradient Norm: 0.2560\n",
      "Epoch 174: avg loss training: 2.7597,... Gradient Norm: 0.0950\n",
      "Epoch 175: avg loss training: 2.7596,... Gradient Norm: 0.0802\n",
      "Epoch 176: avg loss training: 2.7596,... Gradient Norm: 0.1746\n",
      "Epoch 177: avg loss training: 2.7595,... Gradient Norm: 0.3269\n",
      "Epoch 178: avg loss training: 2.7594,... Gradient Norm: 0.3122\n",
      "Epoch 179: avg loss training: 2.7593,... Gradient Norm: 0.0962\n",
      "Epoch 180: avg loss training: 2.7592,... Gradient Norm: 0.0623\n",
      "Epoch 181: avg loss training: 2.7591,... Gradient Norm: 0.1468\n",
      "Epoch 182: avg loss training: 2.7590,... Gradient Norm: 0.2343\n",
      "Epoch 183: avg loss training: 2.7589,... Gradient Norm: 0.2648\n",
      "Epoch 184: avg loss training: 2.7588,... Gradient Norm: 0.2058\n",
      "Epoch 185: avg loss training: 2.7587,... Gradient Norm: 0.2582\n",
      "Epoch 186: avg loss training: 2.7587,... Gradient Norm: 0.2326\n",
      "Epoch 187: avg loss training: 2.7586,... Gradient Norm: 0.0973\n",
      "Epoch 188: avg loss training: 2.7585,... Gradient Norm: 0.1044\n",
      "Epoch 189: avg loss training: 2.7584,... Gradient Norm: 0.1249\n",
      "Epoch 190: avg loss training: 2.7583,... Gradient Norm: 0.1086\n",
      "Epoch 191: avg loss training: 2.7582,... Gradient Norm: 0.0914\n",
      "Epoch 192: avg loss training: 2.7582,... Gradient Norm: 0.1718\n",
      "Epoch 193: avg loss training: 2.7581,... Gradient Norm: 0.1761\n",
      "Epoch 194: avg loss training: 2.7580,... Gradient Norm: 0.2254\n",
      "Epoch 195: avg loss training: 2.7580,... Gradient Norm: 0.3207\n",
      "Epoch 196: avg loss training: 2.7579,... Gradient Norm: 0.3873\n",
      "Epoch 197: avg loss training: 2.7578,... Gradient Norm: 0.2859\n",
      "Epoch 198: avg loss training: 2.7577,... Gradient Norm: 0.0699\n",
      "Epoch 199: avg loss training: 2.7577,... Gradient Norm: 0.1801\n",
      "Epoch 200: avg loss training: 2.7576,... Gradient Norm: 0.3541\n",
      "Epoch 201: avg loss training: 2.7576,... Gradient Norm: 0.4417\n",
      "Epoch 202: avg loss training: 2.7575,... Gradient Norm: 0.3042\n",
      "Epoch 203: avg loss training: 2.7574,... Gradient Norm: 0.0730\n",
      "Epoch 204: avg loss training: 2.7573,... Gradient Norm: 0.2081\n",
      "Epoch 205: avg loss training: 2.7573,... Gradient Norm: 0.2988\n",
      "Epoch 206: avg loss training: 2.7572,... Gradient Norm: 0.2441\n",
      "Epoch 207: avg loss training: 2.7572,... Gradient Norm: 0.1150\n",
      "Epoch 208: avg loss training: 2.7571,... Gradient Norm: 0.2151\n",
      "Epoch 209: avg loss training: 2.7571,... Gradient Norm: 0.3253\n",
      "Epoch 210: avg loss training: 2.7570,... Gradient Norm: 0.2729\n",
      "Epoch 211: avg loss training: 2.7569,... Gradient Norm: 0.1667\n",
      "Epoch 212: avg loss training: 2.7569,... Gradient Norm: 0.1930\n",
      "Epoch 213: avg loss training: 2.7568,... Gradient Norm: 0.1380\n",
      "Epoch 214: avg loss training: 2.7567,... Gradient Norm: 0.1630\n",
      "Epoch 215: avg loss training: 2.7567,... Gradient Norm: 0.1131\n",
      "Epoch 216: avg loss training: 2.7566,... Gradient Norm: 0.0925\n",
      "Epoch 217: avg loss training: 2.7566,... Gradient Norm: 0.1063\n",
      "Epoch 218: avg loss training: 2.7565,... Gradient Norm: 0.0552\n",
      "Epoch 219: avg loss training: 2.7565,... Gradient Norm: 0.0913\n",
      "Epoch 220: avg loss training: 2.7564,... Gradient Norm: 0.0896\n",
      "Epoch 221: avg loss training: 2.7564,... Gradient Norm: 0.0958\n",
      "Epoch 222: avg loss training: 2.7563,... Gradient Norm: 0.1172\n",
      "Epoch 223: avg loss training: 2.7563,... Gradient Norm: 0.1110\n",
      "Epoch 224: avg loss training: 2.7562,... Gradient Norm: 0.0930\n",
      "Epoch 225: avg loss training: 2.7562,... Gradient Norm: 0.1122\n",
      "Epoch 226: avg loss training: 2.7561,... Gradient Norm: 0.2121\n",
      "Epoch 227: avg loss training: 2.7561,... Gradient Norm: 0.1534\n",
      "Epoch 228: avg loss training: 2.7560,... Gradient Norm: 0.1450\n",
      "Epoch 229: avg loss training: 2.7560,... Gradient Norm: 0.0693\n",
      "Epoch 230: avg loss training: 2.7559,... Gradient Norm: 0.1134\n",
      "Epoch 231: avg loss training: 2.7558,... Gradient Norm: 0.1593\n",
      "Epoch 232: avg loss training: 2.7558,... Gradient Norm: 0.1358\n",
      "Epoch 233: avg loss training: 2.7557,... Gradient Norm: 0.1790\n",
      "Epoch 234: avg loss training: 2.7557,... Gradient Norm: 0.1720\n",
      "Epoch 235: avg loss training: 2.7556,... Gradient Norm: 0.1240\n",
      "Epoch 236: avg loss training: 2.7556,... Gradient Norm: 0.1188\n",
      "Epoch 237: avg loss training: 2.7555,... Gradient Norm: 0.0630\n",
      "Epoch 238: avg loss training: 2.7555,... Gradient Norm: 0.0735\n",
      "Epoch 239: avg loss training: 2.7554,... Gradient Norm: 0.0618\n",
      "Epoch 240: avg loss training: 2.7554,... Gradient Norm: 0.0708\n",
      "Epoch 241: avg loss training: 2.7553,... Gradient Norm: 0.0568\n",
      "Epoch 242: avg loss training: 2.7553,... Gradient Norm: 0.0825\n",
      "Epoch 243: avg loss training: 2.7552,... Gradient Norm: 0.0705\n",
      "Epoch 244: avg loss training: 2.7552,... Gradient Norm: 0.1161\n",
      "Epoch 245: avg loss training: 2.7552,... Gradient Norm: 0.2033\n",
      "Epoch 246: avg loss training: 2.7551,... Gradient Norm: 0.2090\n",
      "Epoch 247: avg loss training: 2.7551,... Gradient Norm: 0.1827\n",
      "Epoch 248: avg loss training: 2.7550,... Gradient Norm: 0.0594\n",
      "Epoch 249: avg loss training: 2.7550,... Gradient Norm: 0.2176\n",
      "Epoch 250: avg loss training: 2.7549,... Gradient Norm: 0.1512\n",
      "Epoch 251: avg loss training: 2.7549,... Gradient Norm: 0.0976\n",
      "Epoch 252: avg loss training: 2.7548,... Gradient Norm: 0.3511\n",
      "Epoch 253: avg loss training: 2.7548,... Gradient Norm: 0.3226\n",
      "Epoch 254: avg loss training: 2.7547,... Gradient Norm: 0.1317\n",
      "Epoch 255: avg loss training: 2.7547,... Gradient Norm: 0.2470\n",
      "Epoch 256: avg loss training: 2.7546,... Gradient Norm: 0.1506\n",
      "Epoch 257: avg loss training: 2.7546,... Gradient Norm: 0.1137\n",
      "Epoch 258: avg loss training: 2.7545,... Gradient Norm: 0.2161\n",
      "Epoch 259: avg loss training: 2.7544,... Gradient Norm: 0.0810\n",
      "Epoch 260: avg loss training: 2.7544,... Gradient Norm: 0.1804\n",
      "Epoch 261: avg loss training: 2.7543,... Gradient Norm: 0.0725\n",
      "Epoch 262: avg loss training: 2.7543,... Gradient Norm: 0.2548\n",
      "Epoch 263: avg loss training: 2.7542,... Gradient Norm: 0.0919\n",
      "Epoch 264: avg loss training: 2.7542,... Gradient Norm: 0.1571\n",
      "Epoch 265: avg loss training: 2.7542,... Gradient Norm: 0.1049\n",
      "Epoch 266: avg loss training: 2.7541,... Gradient Norm: 0.2261\n",
      "Epoch 267: avg loss training: 2.7541,... Gradient Norm: 0.1055\n",
      "Epoch 268: avg loss training: 2.7540,... Gradient Norm: 0.1173\n",
      "Epoch 269: avg loss training: 2.7540,... Gradient Norm: 0.1599\n",
      "Epoch 270: avg loss training: 2.7540,... Gradient Norm: 0.0876\n",
      "Epoch 271: avg loss training: 2.7539,... Gradient Norm: 0.1477\n",
      "Epoch 272: avg loss training: 2.7539,... Gradient Norm: 0.0987\n",
      "Epoch 273: avg loss training: 2.7538,... Gradient Norm: 0.0845\n",
      "Epoch 274: avg loss training: 2.7538,... Gradient Norm: 0.1112\n",
      "Epoch 275: avg loss training: 2.7538,... Gradient Norm: 0.0568\n",
      "Epoch 276: avg loss training: 2.7537,... Gradient Norm: 0.0718\n",
      "Epoch 277: avg loss training: 2.7537,... Gradient Norm: 0.0625\n",
      "Epoch 278: avg loss training: 2.7536,... Gradient Norm: 0.0530\n",
      "Epoch 279: avg loss training: 2.7536,... Gradient Norm: 0.0843\n",
      "Epoch 280: avg loss training: 2.7536,... Gradient Norm: 0.0898\n",
      "Epoch 281: avg loss training: 2.7535,... Gradient Norm: 0.1398\n",
      "Epoch 282: avg loss training: 2.7535,... Gradient Norm: 0.0949\n",
      "Epoch 283: avg loss training: 2.7534,... Gradient Norm: 0.0740\n",
      "Epoch 284: avg loss training: 2.7534,... Gradient Norm: 0.0767\n",
      "Epoch 285: avg loss training: 2.7534,... Gradient Norm: 0.1077\n",
      "Epoch 286: avg loss training: 2.7533,... Gradient Norm: 0.0846\n",
      "Epoch 287: avg loss training: 2.7533,... Gradient Norm: 0.0484\n",
      "Epoch 288: avg loss training: 2.7533,... Gradient Norm: 0.0490\n",
      "Epoch 289: avg loss training: 2.7532,... Gradient Norm: 0.0466\n",
      "Epoch 290: avg loss training: 2.7532,... Gradient Norm: 0.0470\n",
      "Epoch 291: avg loss training: 2.7532,... Gradient Norm: 0.0556\n",
      "Epoch 292: avg loss training: 2.7531,... Gradient Norm: 0.0742\n",
      "Epoch 293: avg loss training: 2.7531,... Gradient Norm: 0.0825\n",
      "Epoch 294: avg loss training: 2.7531,... Gradient Norm: 0.1087\n",
      "Epoch 295: avg loss training: 2.7530,... Gradient Norm: 0.1243\n",
      "Epoch 296: avg loss training: 2.7530,... Gradient Norm: 0.1432\n",
      "Epoch 297: avg loss training: 2.7529,... Gradient Norm: 0.0988\n",
      "Epoch 298: avg loss training: 2.7529,... Gradient Norm: 0.1210\n",
      "Epoch 299: avg loss training: 2.7529,... Gradient Norm: 0.0791\n",
      "Epoch 300: avg loss training: 2.7529,... Gradient Norm: 0.0941\n",
      "Epoch 301: avg loss training: 2.7528,... Gradient Norm: 0.0689\n",
      "Epoch 302: avg loss training: 2.7528,... Gradient Norm: 0.0678\n",
      "Epoch 303: avg loss training: 2.7528,... Gradient Norm: 0.0637\n",
      "Epoch 304: avg loss training: 2.7527,... Gradient Norm: 0.0526\n",
      "Epoch 305: avg loss training: 2.7527,... Gradient Norm: 0.0767\n",
      "Epoch 306: avg loss training: 2.7527,... Gradient Norm: 0.0873\n",
      "Epoch 307: avg loss training: 2.7526,... Gradient Norm: 0.0876\n",
      "Epoch 308: avg loss training: 2.7526,... Gradient Norm: 0.0549\n",
      "Epoch 309: avg loss training: 2.7526,... Gradient Norm: 0.0775\n",
      "Epoch 310: avg loss training: 2.7526,... Gradient Norm: 0.1085\n",
      "Epoch 311: avg loss training: 2.7525,... Gradient Norm: 0.1508\n",
      "Epoch 312: avg loss training: 2.7525,... Gradient Norm: 0.1734\n",
      "Epoch 313: avg loss training: 2.7525,... Gradient Norm: 0.0662\n",
      "Epoch 314: avg loss training: 2.7524,... Gradient Norm: 0.1496\n",
      "Epoch 315: avg loss training: 2.7524,... Gradient Norm: 0.0933\n",
      "Epoch 316: avg loss training: 2.7524,... Gradient Norm: 0.0867\n",
      "Epoch 317: avg loss training: 2.7523,... Gradient Norm: 0.1283\n",
      "Epoch 318: avg loss training: 2.7523,... Gradient Norm: 0.1763\n",
      "Epoch 319: avg loss training: 2.7523,... Gradient Norm: 0.0885\n",
      "Epoch 320: avg loss training: 2.7522,... Gradient Norm: 0.1268\n",
      "Epoch 321: avg loss training: 2.7522,... Gradient Norm: 0.1791\n",
      "Epoch 322: avg loss training: 2.7522,... Gradient Norm: 0.0680\n",
      "Epoch 323: avg loss training: 2.7521,... Gradient Norm: 0.1017\n",
      "Epoch 324: avg loss training: 2.7521,... Gradient Norm: 0.0775\n",
      "Epoch 325: avg loss training: 2.7521,... Gradient Norm: 0.0815\n",
      "Epoch 326: avg loss training: 2.7520,... Gradient Norm: 0.1432\n",
      "Epoch 327: avg loss training: 2.7520,... Gradient Norm: 0.0669\n",
      "Epoch 328: avg loss training: 2.7520,... Gradient Norm: 0.2052\n",
      "Epoch 329: avg loss training: 2.7520,... Gradient Norm: 0.0778\n",
      "Epoch 330: avg loss training: 2.7519,... Gradient Norm: 0.1310\n",
      "Epoch 331: avg loss training: 2.7519,... Gradient Norm: 0.1183\n",
      "Epoch 332: avg loss training: 2.7519,... Gradient Norm: 0.1459\n",
      "Epoch 333: avg loss training: 2.7519,... Gradient Norm: 0.0502\n",
      "Epoch 334: avg loss training: 2.7518,... Gradient Norm: 0.1279\n",
      "Epoch 335: avg loss training: 2.7518,... Gradient Norm: 0.1355\n",
      "Epoch 336: avg loss training: 2.7518,... Gradient Norm: 0.0826\n",
      "Epoch 337: avg loss training: 2.7518,... Gradient Norm: 0.1654\n",
      "Epoch 338: avg loss training: 2.7517,... Gradient Norm: 0.0609\n",
      "Epoch 339: avg loss training: 2.7517,... Gradient Norm: 0.2009\n",
      "Epoch 340: avg loss training: 2.7517,... Gradient Norm: 0.0611\n",
      "Epoch 341: avg loss training: 2.7517,... Gradient Norm: 0.1205\n",
      "Epoch 342: avg loss training: 2.7516,... Gradient Norm: 0.0839\n",
      "Epoch 343: avg loss training: 2.7516,... Gradient Norm: 0.1697\n",
      "Epoch 344: avg loss training: 2.7516,... Gradient Norm: 0.0524\n",
      "Epoch 345: avg loss training: 2.7516,... Gradient Norm: 0.1602\n",
      "Epoch 346: avg loss training: 2.7516,... Gradient Norm: 0.0859\n",
      "Epoch 347: avg loss training: 2.7515,... Gradient Norm: 0.1284\n",
      "Epoch 348: avg loss training: 2.7515,... Gradient Norm: 0.1482\n",
      "Epoch 349: avg loss training: 2.7515,... Gradient Norm: 0.1017\n",
      "Epoch 350: avg loss training: 2.7515,... Gradient Norm: 0.1890\n",
      "Epoch 351: avg loss training: 2.7515,... Gradient Norm: 0.0621\n",
      "Epoch 352: avg loss training: 2.7514,... Gradient Norm: 0.1442\n",
      "Epoch 353: avg loss training: 2.7514,... Gradient Norm: 0.1080\n",
      "Epoch 354: avg loss training: 2.7514,... Gradient Norm: 0.1345\n",
      "Epoch 355: avg loss training: 2.7514,... Gradient Norm: 0.1008\n",
      "Epoch 356: avg loss training: 2.7513,... Gradient Norm: 0.0596\n",
      "Epoch 357: avg loss training: 2.7513,... Gradient Norm: 0.1411\n",
      "Epoch 358: avg loss training: 2.7513,... Gradient Norm: 0.0688\n",
      "Epoch 359: avg loss training: 2.7513,... Gradient Norm: 0.1164\n",
      "Epoch 360: avg loss training: 2.7513,... Gradient Norm: 0.0681\n",
      "Epoch 361: avg loss training: 2.7512,... Gradient Norm: 0.1086\n",
      "Epoch 362: avg loss training: 2.7512,... Gradient Norm: 0.0770\n",
      "Epoch 363: avg loss training: 2.7512,... Gradient Norm: 0.0630\n",
      "Epoch 364: avg loss training: 2.7512,... Gradient Norm: 0.1037\n",
      "Epoch 365: avg loss training: 2.7512,... Gradient Norm: 0.0672\n",
      "Epoch 366: avg loss training: 2.7511,... Gradient Norm: 0.1057\n",
      "Epoch 367: avg loss training: 2.7511,... Gradient Norm: 0.0886\n",
      "Epoch 368: avg loss training: 2.7511,... Gradient Norm: 0.0660\n",
      "Epoch 369: avg loss training: 2.7511,... Gradient Norm: 0.1144\n",
      "Epoch 370: avg loss training: 2.7511,... Gradient Norm: 0.0871\n",
      "Epoch 371: avg loss training: 2.7511,... Gradient Norm: 0.0571\n",
      "Epoch 372: avg loss training: 2.7510,... Gradient Norm: 0.0579\n",
      "Epoch 373: avg loss training: 2.7510,... Gradient Norm: 0.0457\n",
      "Epoch 374: avg loss training: 2.7510,... Gradient Norm: 0.1070\n",
      "Epoch 375: avg loss training: 2.7510,... Gradient Norm: 0.0932\n",
      "Epoch 376: avg loss training: 2.7510,... Gradient Norm: 0.0482\n",
      "Epoch 377: avg loss training: 2.7510,... Gradient Norm: 0.0451\n",
      "Epoch 378: avg loss training: 2.7509,... Gradient Norm: 0.0541\n",
      "Epoch 379: avg loss training: 2.7509,... Gradient Norm: 0.0630\n",
      "Epoch 380: avg loss training: 2.7509,... Gradient Norm: 0.0715\n",
      "Epoch 381: avg loss training: 2.7509,... Gradient Norm: 0.0558\n",
      "Epoch 382: avg loss training: 2.7509,... Gradient Norm: 0.0726\n",
      "Epoch 383: avg loss training: 2.7509,... Gradient Norm: 0.0726\n",
      "Epoch 384: avg loss training: 2.7509,... Gradient Norm: 0.0436\n",
      "Epoch 385: avg loss training: 2.7508,... Gradient Norm: 0.0592\n",
      "Epoch 386: avg loss training: 2.7508,... Gradient Norm: 0.0514\n",
      "Epoch 387: avg loss training: 2.7508,... Gradient Norm: 0.0428\n",
      "Epoch 388: avg loss training: 2.7508,... Gradient Norm: 0.0377\n",
      "Epoch 389: avg loss training: 2.7508,... Gradient Norm: 0.0578\n",
      "Epoch 390: avg loss training: 2.7508,... Gradient Norm: 0.0388\n",
      "Epoch 391: avg loss training: 2.7508,... Gradient Norm: 0.0721\n",
      "Epoch 392: avg loss training: 2.7507,... Gradient Norm: 0.0800\n",
      "Epoch 393: avg loss training: 2.7507,... Gradient Norm: 0.0454\n",
      "Epoch 394: avg loss training: 2.7507,... Gradient Norm: 0.1039\n",
      "Epoch 395: avg loss training: 2.7507,... Gradient Norm: 0.0410\n",
      "Epoch 396: avg loss training: 2.7507,... Gradient Norm: 0.1231\n",
      "Epoch 397: avg loss training: 2.7507,... Gradient Norm: 0.0542\n",
      "Epoch 398: avg loss training: 2.7507,... Gradient Norm: 0.0659\n",
      "Epoch 399: avg loss training: 2.7506,... Gradient Norm: 0.0407\n",
      "Epoch 400: avg loss training: 2.7506,... Gradient Norm: 0.0591\n",
      "Epoch 401: avg loss training: 2.7506,... Gradient Norm: 0.0651\n",
      "Epoch 402: avg loss training: 2.7506,... Gradient Norm: 0.0391\n",
      "Epoch 403: avg loss training: 2.7506,... Gradient Norm: 0.0426\n",
      "Epoch 404: avg loss training: 2.7506,... Gradient Norm: 0.0427\n",
      "Epoch 405: avg loss training: 2.7506,... Gradient Norm: 0.0452\n",
      "Epoch 406: avg loss training: 2.7506,... Gradient Norm: 0.0509\n",
      "Epoch 407: avg loss training: 2.7505,... Gradient Norm: 0.0465\n",
      "Epoch 408: avg loss training: 2.7505,... Gradient Norm: 0.2120\n",
      "Epoch 409: avg loss training: 2.7505,... Gradient Norm: 0.0982\n",
      "Epoch 410: avg loss training: 2.7505,... Gradient Norm: 0.0695\n",
      "Epoch 411: avg loss training: 2.7505,... Gradient Norm: 0.0836\n",
      "Epoch 412: avg loss training: 2.7505,... Gradient Norm: 0.0715\n",
      "Epoch 413: avg loss training: 2.7504,... Gradient Norm: 0.0774\n",
      "Epoch 414: avg loss training: 2.7504,... Gradient Norm: 0.0756\n",
      "Epoch 415: avg loss training: 2.7504,... Gradient Norm: 0.1290\n",
      "Epoch 416: avg loss training: 2.7504,... Gradient Norm: 0.1059\n",
      "Epoch 417: avg loss training: 2.7504,... Gradient Norm: 0.0784\n",
      "Epoch 418: avg loss training: 2.7503,... Gradient Norm: 0.0857\n",
      "Epoch 419: avg loss training: 2.7503,... Gradient Norm: 0.1379\n",
      "Epoch 420: avg loss training: 2.7503,... Gradient Norm: 0.1159\n",
      "Epoch 421: avg loss training: 2.7503,... Gradient Norm: 0.0760\n",
      "Epoch 422: avg loss training: 2.7502,... Gradient Norm: 0.1208\n",
      "Epoch 423: avg loss training: 2.7502,... Gradient Norm: 0.1137\n",
      "Epoch 424: avg loss training: 2.7502,... Gradient Norm: 0.0680\n",
      "Epoch 425: avg loss training: 2.7502,... Gradient Norm: 0.0603\n",
      "Epoch 426: avg loss training: 2.7502,... Gradient Norm: 0.0650\n",
      "Epoch 427: avg loss training: 2.7501,... Gradient Norm: 0.0684\n",
      "Epoch 428: avg loss training: 2.7501,... Gradient Norm: 0.0665\n",
      "Epoch 429: avg loss training: 2.7501,... Gradient Norm: 0.0716\n",
      "Epoch 430: avg loss training: 2.7501,... Gradient Norm: 0.0664\n",
      "Epoch 431: avg loss training: 2.7501,... Gradient Norm: 0.0644\n",
      "Epoch 432: avg loss training: 2.7500,... Gradient Norm: 0.0582\n",
      "Epoch 433: avg loss training: 2.7500,... Gradient Norm: 0.0664\n",
      "Epoch 434: avg loss training: 2.7500,... Gradient Norm: 0.0590\n",
      "Epoch 435: avg loss training: 2.7500,... Gradient Norm: 0.0558\n",
      "Epoch 436: avg loss training: 2.7500,... Gradient Norm: 0.0508\n",
      "Epoch 437: avg loss training: 2.7500,... Gradient Norm: 0.0576\n",
      "Epoch 438: avg loss training: 2.7499,... Gradient Norm: 0.0532\n",
      "Epoch 439: avg loss training: 2.7499,... Gradient Norm: 0.0526\n",
      "Epoch 440: avg loss training: 2.7499,... Gradient Norm: 0.0481\n",
      "Epoch 441: avg loss training: 2.7499,... Gradient Norm: 0.0527\n",
      "Epoch 442: avg loss training: 2.7499,... Gradient Norm: 0.0605\n",
      "Epoch 443: avg loss training: 2.7499,... Gradient Norm: 0.0520\n",
      "Epoch 444: avg loss training: 2.7499,... Gradient Norm: 0.0468\n",
      "Epoch 445: avg loss training: 2.7499,... Gradient Norm: 0.0521\n",
      "Epoch 446: avg loss training: 2.7498,... Gradient Norm: 0.0601\n",
      "Epoch 447: avg loss training: 2.7498,... Gradient Norm: 0.0478\n",
      "Epoch 448: avg loss training: 2.7498,... Gradient Norm: 0.0469\n",
      "Epoch 449: avg loss training: 2.7498,... Gradient Norm: 0.0528\n",
      "Epoch 450: avg loss training: 2.7498,... Gradient Norm: 0.0505\n",
      "Epoch 451: avg loss training: 2.7498,... Gradient Norm: 0.0469\n",
      "Epoch 452: avg loss training: 2.7498,... Gradient Norm: 0.0560\n",
      "Epoch 453: avg loss training: 2.7498,... Gradient Norm: 0.0738\n",
      "Epoch 454: avg loss training: 2.7497,... Gradient Norm: 0.0580\n",
      "Epoch 455: avg loss training: 2.7497,... Gradient Norm: 0.0484\n",
      "Epoch 456: avg loss training: 2.7497,... Gradient Norm: 0.0524\n",
      "Epoch 457: avg loss training: 2.7497,... Gradient Norm: 0.0473\n",
      "Epoch 458: avg loss training: 2.7497,... Gradient Norm: 0.0430\n",
      "Epoch 459: avg loss training: 2.7497,... Gradient Norm: 0.0446\n",
      "Epoch 460: avg loss training: 2.7497,... Gradient Norm: 0.0422\n",
      "Epoch 461: avg loss training: 2.7497,... Gradient Norm: 0.0522\n",
      "Epoch 462: avg loss training: 2.7497,... Gradient Norm: 0.0493\n",
      "Epoch 463: avg loss training: 2.7496,... Gradient Norm: 0.0528\n",
      "Epoch 464: avg loss training: 2.7496,... Gradient Norm: 0.0671\n",
      "Epoch 465: avg loss training: 2.7496,... Gradient Norm: 0.0482\n",
      "Epoch 466: avg loss training: 2.7496,... Gradient Norm: 0.0798\n",
      "Epoch 467: avg loss training: 2.7496,... Gradient Norm: 0.0628\n",
      "Epoch 468: avg loss training: 2.7496,... Gradient Norm: 0.0442\n",
      "Epoch 469: avg loss training: 2.7496,... Gradient Norm: 0.0436\n",
      "Epoch 470: avg loss training: 2.7496,... Gradient Norm: 0.0426\n",
      "Epoch 471: avg loss training: 2.7496,... Gradient Norm: 0.0685\n",
      "Epoch 472: avg loss training: 2.7495,... Gradient Norm: 0.0739\n",
      "Epoch 473: avg loss training: 2.7495,... Gradient Norm: 0.0532\n",
      "Epoch 474: avg loss training: 2.7495,... Gradient Norm: 0.0740\n",
      "Epoch 475: avg loss training: 2.7495,... Gradient Norm: 0.0479\n",
      "Epoch 476: avg loss training: 2.7495,... Gradient Norm: 0.0679\n",
      "Epoch 477: avg loss training: 2.7495,... Gradient Norm: 0.0547\n",
      "Epoch 478: avg loss training: 2.7495,... Gradient Norm: 0.0672\n",
      "Epoch 479: avg loss training: 2.7495,... Gradient Norm: 0.0397\n",
      "Epoch 480: avg loss training: 2.7495,... Gradient Norm: 0.0708\n",
      "Epoch 481: avg loss training: 2.7495,... Gradient Norm: 0.0401\n",
      "Epoch 482: avg loss training: 2.7495,... Gradient Norm: 0.0497\n",
      "Epoch 483: avg loss training: 2.7494,... Gradient Norm: 0.0624\n",
      "Epoch 484: avg loss training: 2.7494,... Gradient Norm: 0.0440\n",
      "Epoch 485: avg loss training: 2.7494,... Gradient Norm: 0.0479\n",
      "Epoch 486: avg loss training: 2.7494,... Gradient Norm: 0.0502\n",
      "Epoch 487: avg loss training: 2.7494,... Gradient Norm: 0.0947\n",
      "Epoch 488: avg loss training: 2.7494,... Gradient Norm: 0.0639\n",
      "Epoch 489: avg loss training: 2.7494,... Gradient Norm: 0.0582\n",
      "Epoch 490: avg loss training: 2.7494,... Gradient Norm: 0.0625\n",
      "Epoch 491: avg loss training: 2.7494,... Gradient Norm: 0.0474\n",
      "Epoch 492: avg loss training: 2.7494,... Gradient Norm: 0.0747\n",
      "Epoch 493: avg loss training: 2.7494,... Gradient Norm: 0.0404\n",
      "Epoch 494: avg loss training: 2.7494,... Gradient Norm: 0.0448\n",
      "Epoch 495: avg loss training: 2.7493,... Gradient Norm: 0.0600\n",
      "Epoch 496: avg loss training: 2.7493,... Gradient Norm: 0.0476\n",
      "Epoch 497: avg loss training: 2.7493,... Gradient Norm: 0.0575\n",
      "Epoch 498: avg loss training: 2.7493,... Gradient Norm: 0.0682\n",
      "Epoch 499: avg loss training: 2.7493,... Gradient Norm: 0.0406\n",
      "Epoch 500: avg loss training: 2.7493,... Gradient Norm: 0.0634\n",
      "Epoch 501: avg loss training: 2.7493,... Gradient Norm: 0.0588\n",
      "Epoch 502: avg loss training: 2.7493,... Gradient Norm: 0.0683\n",
      "Epoch 503: avg loss training: 2.7493,... Gradient Norm: 0.0488\n",
      "Epoch 504: avg loss training: 2.7493,... Gradient Norm: 0.0567\n",
      "Epoch 505: avg loss training: 2.7493,... Gradient Norm: 0.0722\n",
      "Epoch 506: avg loss training: 2.7493,... Gradient Norm: 0.0791\n",
      "Epoch 507: avg loss training: 2.7493,... Gradient Norm: 0.0393\n",
      "Epoch 508: avg loss training: 2.7492,... Gradient Norm: 0.0732\n",
      "Epoch 509: avg loss training: 2.7492,... Gradient Norm: 0.0545\n",
      "Epoch 510: avg loss training: 2.7492,... Gradient Norm: 0.0849\n",
      "Epoch 511: avg loss training: 2.7492,... Gradient Norm: 0.0770\n",
      "Epoch 512: avg loss training: 2.7492,... Gradient Norm: 0.0632\n",
      "Epoch 513: avg loss training: 2.7492,... Gradient Norm: 0.0759\n",
      "Epoch 514: avg loss training: 2.7492,... Gradient Norm: 0.0712\n",
      "Epoch 515: avg loss training: 2.7492,... Gradient Norm: 0.0643\n",
      "Epoch 516: avg loss training: 2.7492,... Gradient Norm: 0.0635\n",
      "Epoch 517: avg loss training: 2.7492,... Gradient Norm: 0.0745\n",
      "Epoch 518: avg loss training: 2.7492,... Gradient Norm: 0.0520\n",
      "Epoch 519: avg loss training: 2.7492,... Gradient Norm: 0.0697\n",
      "Epoch 520: avg loss training: 2.7492,... Gradient Norm: 0.0477\n",
      "Epoch 521: avg loss training: 2.7492,... Gradient Norm: 0.0670\n",
      "Epoch 522: avg loss training: 2.7492,... Gradient Norm: 0.0412\n",
      "Epoch 523: avg loss training: 2.7492,... Gradient Norm: 0.0648\n",
      "Epoch 524: avg loss training: 2.7491,... Gradient Norm: 0.0447\n",
      "Epoch 525: avg loss training: 2.7491,... Gradient Norm: 0.0819\n",
      "Epoch 526: avg loss training: 2.7491,... Gradient Norm: 0.0821\n",
      "Epoch 527: avg loss training: 2.7491,... Gradient Norm: 0.0454\n",
      "Epoch 528: avg loss training: 2.7491,... Gradient Norm: 0.0545\n",
      "Epoch 529: avg loss training: 2.7491,... Gradient Norm: 0.0750\n",
      "Epoch 530: avg loss training: 2.7491,... Gradient Norm: 0.0599\n",
      "Epoch 531: avg loss training: 2.7491,... Gradient Norm: 0.0794\n",
      "Epoch 532: avg loss training: 2.7491,... Gradient Norm: 0.0715\n",
      "Epoch 533: avg loss training: 2.7491,... Gradient Norm: 0.0661\n",
      "Epoch 534: avg loss training: 2.7491,... Gradient Norm: 0.0549\n",
      "Epoch 535: avg loss training: 2.7491,... Gradient Norm: 0.0554\n",
      "Epoch 536: avg loss training: 2.7491,... Gradient Norm: 0.0451\n",
      "Epoch 537: avg loss training: 2.7491,... Gradient Norm: 0.0483\n",
      "Epoch 538: avg loss training: 2.7491,... Gradient Norm: 0.0561\n",
      "Epoch 539: avg loss training: 2.7491,... Gradient Norm: 0.0542\n",
      "Epoch 540: avg loss training: 2.7491,... Gradient Norm: 0.0546\n",
      "Epoch 541: avg loss training: 2.7491,... Gradient Norm: 0.0578\n",
      "Epoch 542: avg loss training: 2.7491,... Gradient Norm: 0.0547\n",
      "Epoch 543: avg loss training: 2.7491,... Gradient Norm: 0.0571\n",
      "Epoch 544: avg loss training: 2.7491,... Gradient Norm: 0.0482\n",
      "Epoch 545: avg loss training: 2.7490,... Gradient Norm: 0.0444\n",
      "Epoch 546: avg loss training: 2.7490,... Gradient Norm: 0.0451\n",
      "Epoch 547: avg loss training: 2.7490,... Gradient Norm: 0.0607\n",
      "Epoch 548: avg loss training: 2.7490,... Gradient Norm: 0.0484\n",
      "Epoch 549: avg loss training: 2.7490,... Gradient Norm: 0.0600\n",
      "Epoch 550: avg loss training: 2.7490,... Gradient Norm: 0.0658\n",
      "Epoch 551: avg loss training: 2.7490,... Gradient Norm: 0.0620\n",
      "Epoch 552: avg loss training: 2.7490,... Gradient Norm: 0.0476\n",
      "Epoch 553: avg loss training: 2.7490,... Gradient Norm: 0.0562\n",
      "Epoch 554: avg loss training: 2.7490,... Gradient Norm: 0.0769\n",
      "Epoch 555: avg loss training: 2.7490,... Gradient Norm: 0.0741\n",
      "Epoch 556: avg loss training: 2.7490,... Gradient Norm: 0.0646\n",
      "Epoch 557: avg loss training: 2.7490,... Gradient Norm: 0.0631\n",
      "Epoch 558: avg loss training: 2.7490,... Gradient Norm: 0.0469\n",
      "Epoch 559: avg loss training: 2.7490,... Gradient Norm: 0.0385\n",
      "Epoch 560: avg loss training: 2.7490,... Gradient Norm: 0.0556\n",
      "Epoch 561: avg loss training: 2.7490,... Gradient Norm: 0.0496\n",
      "Epoch 562: avg loss training: 2.7490,... Gradient Norm: 0.0871\n",
      "Epoch 563: avg loss training: 2.7490,... Gradient Norm: 0.0531\n",
      "Epoch 564: avg loss training: 2.7490,... Gradient Norm: 0.0543\n",
      "Epoch 565: avg loss training: 2.7490,... Gradient Norm: 0.0579\n",
      "Epoch 566: avg loss training: 2.7490,... Gradient Norm: 0.0487\n",
      "Epoch 567: avg loss training: 2.7490,... Gradient Norm: 0.0627\n",
      "Epoch 568: avg loss training: 2.7490,... Gradient Norm: 0.0523\n",
      "Epoch 569: avg loss training: 2.7490,... Gradient Norm: 0.0781\n",
      "Epoch 570: avg loss training: 2.7490,... Gradient Norm: 0.0454\n",
      "Epoch 571: avg loss training: 2.7489,... Gradient Norm: 0.0758\n",
      "Epoch 572: avg loss training: 2.7489,... Gradient Norm: 0.1020\n",
      "Epoch 573: avg loss training: 2.7489,... Gradient Norm: 0.0413\n",
      "Epoch 574: avg loss training: 2.7489,... Gradient Norm: 0.1088\n",
      "Epoch 575: avg loss training: 2.7489,... Gradient Norm: 0.0420\n",
      "Epoch 576: avg loss training: 2.7489,... Gradient Norm: 0.0732\n",
      "Epoch 577: avg loss training: 2.7489,... Gradient Norm: 0.0448\n",
      "Epoch 578: avg loss training: 2.7489,... Gradient Norm: 0.0607\n",
      "Epoch 579: avg loss training: 2.7489,... Gradient Norm: 0.0481\n",
      "Epoch 580: avg loss training: 2.7489,... Gradient Norm: 0.1186\n",
      "Epoch 581: avg loss training: 2.7489,... Gradient Norm: 0.1105\n",
      "Epoch 582: avg loss training: 2.7489,... Gradient Norm: 0.0998\n",
      "Epoch 583: avg loss training: 2.7489,... Gradient Norm: 0.0876\n",
      "Epoch 584: avg loss training: 2.7489,... Gradient Norm: 0.1063\n",
      "Epoch 585: avg loss training: 2.7489,... Gradient Norm: 0.0675\n",
      "Epoch 586: avg loss training: 2.7489,... Gradient Norm: 0.0847\n",
      "Epoch 587: avg loss training: 2.7489,... Gradient Norm: 0.0599\n",
      "Epoch 588: avg loss training: 2.7489,... Gradient Norm: 0.0821\n",
      "Epoch 589: avg loss training: 2.7489,... Gradient Norm: 0.0717\n",
      "Epoch 590: avg loss training: 2.7489,... Gradient Norm: 0.0693\n",
      "Epoch 591: avg loss training: 2.7489,... Gradient Norm: 0.1120\n",
      "Epoch 592: avg loss training: 2.7489,... Gradient Norm: 0.0612\n",
      "Epoch 593: avg loss training: 2.7489,... Gradient Norm: 0.1272\n",
      "Epoch 594: avg loss training: 2.7489,... Gradient Norm: 0.0816\n",
      "Epoch 595: avg loss training: 2.7489,... Gradient Norm: 0.0687\n",
      "Epoch 596: avg loss training: 2.7489,... Gradient Norm: 0.0957\n",
      "Epoch 597: avg loss training: 2.7489,... Gradient Norm: 0.0481\n",
      "Epoch 598: avg loss training: 2.7489,... Gradient Norm: 0.0618\n",
      "Epoch 599: avg loss training: 2.7489,... Gradient Norm: 0.0496\n",
      "Epoch 600: avg loss training: 2.7489,... Gradient Norm: 0.0665\n",
      "Epoch 601: avg loss training: 2.7489,... Gradient Norm: 0.0903\n",
      "Epoch 602: avg loss training: 2.7489,... Gradient Norm: 0.0464\n",
      "Epoch 603: avg loss training: 2.7489,... Gradient Norm: 0.1054\n",
      "Epoch 604: avg loss training: 2.7489,... Gradient Norm: 0.0582\n",
      "Epoch 605: avg loss training: 2.7489,... Gradient Norm: 0.0724\n",
      "Epoch 606: avg loss training: 2.7489,... Gradient Norm: 0.0705\n",
      "Epoch 607: avg loss training: 2.7488,... Gradient Norm: 0.0850\n",
      "Epoch 608: avg loss training: 2.7488,... Gradient Norm: 0.1090\n",
      "Epoch 609: avg loss training: 2.7488,... Gradient Norm: 0.0728\n",
      "Epoch 610: avg loss training: 2.7488,... Gradient Norm: 0.0762\n",
      "Epoch 611: avg loss training: 2.7488,... Gradient Norm: 0.0738\n",
      "Epoch 612: avg loss training: 2.7488,... Gradient Norm: 0.0547\n",
      "Epoch 613: avg loss training: 2.7488,... Gradient Norm: 0.0840\n",
      "Epoch 614: avg loss training: 2.7488,... Gradient Norm: 0.0945\n",
      "Epoch 615: avg loss training: 2.7488,... Gradient Norm: 0.0746\n",
      "Epoch 616: avg loss training: 2.7488,... Gradient Norm: 0.0595\n",
      "Epoch 617: avg loss training: 2.7488,... Gradient Norm: 0.0891\n",
      "Epoch 618: avg loss training: 2.7488,... Gradient Norm: 0.1052\n",
      "Epoch 619: avg loss training: 2.7488,... Gradient Norm: 0.0614\n",
      "Epoch 620: avg loss training: 2.7488,... Gradient Norm: 0.0500\n",
      "Epoch 621: avg loss training: 2.7488,... Gradient Norm: 0.0658\n",
      "Epoch 622: avg loss training: 2.7488,... Gradient Norm: 0.0648\n",
      "Epoch 623: avg loss training: 2.7488,... Gradient Norm: 0.1123\n",
      "Epoch 624: avg loss training: 2.7488,... Gradient Norm: 0.0420\n",
      "Epoch 625: avg loss training: 2.7488,... Gradient Norm: 0.1091\n",
      "Epoch 626: avg loss training: 2.7488,... Gradient Norm: 0.0923\n",
      "Epoch 627: avg loss training: 2.7488,... Gradient Norm: 0.0539\n",
      "Epoch 628: avg loss training: 2.7488,... Gradient Norm: 0.0756\n",
      "Epoch 629: avg loss training: 2.7488,... Gradient Norm: 0.0702\n",
      "Epoch 630: avg loss training: 2.7488,... Gradient Norm: 0.0705\n",
      "Epoch 631: avg loss training: 2.7488,... Gradient Norm: 0.0715\n",
      "Epoch 632: avg loss training: 2.7488,... Gradient Norm: 0.0958\n",
      "Epoch 633: avg loss training: 2.7488,... Gradient Norm: 0.1357\n",
      "Epoch 634: avg loss training: 2.7488,... Gradient Norm: 0.0820\n",
      "Epoch 635: avg loss training: 2.7488,... Gradient Norm: 0.0828\n",
      "Epoch 636: avg loss training: 2.7488,... Gradient Norm: 0.1442\n",
      "Epoch 637: avg loss training: 2.7488,... Gradient Norm: 0.0848\n",
      "Epoch 638: avg loss training: 2.7488,... Gradient Norm: 0.1001\n",
      "Epoch 639: avg loss training: 2.7488,... Gradient Norm: 0.0732\n",
      "Epoch 640: avg loss training: 2.7488,... Gradient Norm: 0.1071\n",
      "Epoch 641: avg loss training: 2.7488,... Gradient Norm: 0.0981\n",
      "Epoch 642: avg loss training: 2.7488,... Gradient Norm: 0.0552\n",
      "Epoch 643: avg loss training: 2.7488,... Gradient Norm: 0.0764\n",
      "Epoch 644: avg loss training: 2.7488,... Gradient Norm: 0.0610\n",
      "Epoch 645: avg loss training: 2.7488,... Gradient Norm: 0.0867\n",
      "Epoch 646: avg loss training: 2.7488,... Gradient Norm: 0.0761\n",
      "Epoch 647: avg loss training: 2.7488,... Gradient Norm: 0.0489\n",
      "Epoch 648: avg loss training: 2.7488,... Gradient Norm: 0.1062\n",
      "Epoch 649: avg loss training: 2.7488,... Gradient Norm: 0.0850\n",
      "Epoch 650: avg loss training: 2.7488,... Gradient Norm: 0.0506\n",
      "Epoch 651: avg loss training: 2.7488,... Gradient Norm: 0.1110\n",
      "Epoch 652: avg loss training: 2.7488,... Gradient Norm: 0.0868\n",
      "Epoch 653: avg loss training: 2.7488,... Gradient Norm: 0.1018\n",
      "Epoch 654: avg loss training: 2.7488,... Gradient Norm: 0.0889\n",
      "Epoch 655: avg loss training: 2.7488,... Gradient Norm: 0.0521\n",
      "Epoch 656: avg loss training: 2.7488,... Gradient Norm: 0.0494\n",
      "Epoch 657: avg loss training: 2.7487,... Gradient Norm: 0.1015\n",
      "Epoch 658: avg loss training: 2.7487,... Gradient Norm: 0.0894\n",
      "Epoch 659: avg loss training: 2.7487,... Gradient Norm: 0.0546\n",
      "Epoch 660: avg loss training: 2.7487,... Gradient Norm: 0.0444\n",
      "Epoch 661: avg loss training: 2.7487,... Gradient Norm: 0.1280\n",
      "Epoch 662: avg loss training: 2.7487,... Gradient Norm: 0.0892\n",
      "Epoch 663: avg loss training: 2.7487,... Gradient Norm: 0.0953\n",
      "Epoch 664: avg loss training: 2.7487,... Gradient Norm: 0.1418\n",
      "Epoch 665: avg loss training: 2.7487,... Gradient Norm: 0.0624\n",
      "Epoch 666: avg loss training: 2.7487,... Gradient Norm: 0.0965\n",
      "Epoch 667: avg loss training: 2.7487,... Gradient Norm: 0.0916\n",
      "Epoch 668: avg loss training: 2.7487,... Gradient Norm: 0.1332\n",
      "Epoch 669: avg loss training: 2.7487,... Gradient Norm: 0.0646\n",
      "Epoch 670: avg loss training: 2.7487,... Gradient Norm: 0.1119\n",
      "Epoch 671: avg loss training: 2.7487,... Gradient Norm: 0.0915\n",
      "Epoch 672: avg loss training: 2.7487,... Gradient Norm: 0.1079\n",
      "Epoch 673: avg loss training: 2.7487,... Gradient Norm: 0.1488\n",
      "Epoch 674: avg loss training: 2.7487,... Gradient Norm: 0.1113\n",
      "Epoch 675: avg loss training: 2.7487,... Gradient Norm: 0.1151\n",
      "Epoch 676: avg loss training: 2.7487,... Gradient Norm: 0.0893\n",
      "Epoch 677: avg loss training: 2.7487,... Gradient Norm: 0.1749\n",
      "Epoch 678: avg loss training: 2.7487,... Gradient Norm: 0.1612\n",
      "Epoch 679: avg loss training: 2.7487,... Gradient Norm: 0.1179\n",
      "Epoch 680: avg loss training: 2.7487,... Gradient Norm: 0.1253\n",
      "Epoch 681: avg loss training: 2.7487,... Gradient Norm: 0.1203\n",
      "Epoch 682: avg loss training: 2.7487,... Gradient Norm: 0.1185\n",
      "Epoch 683: avg loss training: 2.7487,... Gradient Norm: 0.1439\n",
      "Epoch 684: avg loss training: 2.7487,... Gradient Norm: 0.0603\n",
      "Epoch 685: avg loss training: 2.7487,... Gradient Norm: 0.0797\n",
      "Epoch 686: avg loss training: 2.7487,... Gradient Norm: 0.0670\n",
      "Epoch 687: avg loss training: 2.7487,... Gradient Norm: 0.0399\n",
      "Epoch 688: avg loss training: 2.7487,... Gradient Norm: 0.0825\n",
      "Epoch 689: avg loss training: 2.7487,... Gradient Norm: 0.0971\n",
      "Epoch 690: avg loss training: 2.7487,... Gradient Norm: 0.0730\n",
      "Epoch 691: avg loss training: 2.7487,... Gradient Norm: 0.1098\n",
      "Epoch 692: avg loss training: 2.7487,... Gradient Norm: 0.0535\n",
      "Epoch 693: avg loss training: 2.7487,... Gradient Norm: 0.0572\n",
      "Epoch 694: avg loss training: 2.7487,... Gradient Norm: 0.0519\n",
      "Epoch 695: avg loss training: 2.7487,... Gradient Norm: 0.0693\n",
      "Epoch 696: avg loss training: 2.7487,... Gradient Norm: 0.0554\n",
      "Epoch 697: avg loss training: 2.7487,... Gradient Norm: 0.0645\n",
      "Epoch 698: avg loss training: 2.7487,... Gradient Norm: 0.0638\n",
      "Epoch 699: avg loss training: 2.7487,... Gradient Norm: 0.0550\n",
      "Epoch 700: avg loss training: 2.7487,... Gradient Norm: 0.0704\n",
      "Epoch 701: avg loss training: 2.7487,... Gradient Norm: 0.1102\n",
      "Epoch 702: avg loss training: 2.7487,... Gradient Norm: 0.1015\n",
      "Epoch 703: avg loss training: 2.7487,... Gradient Norm: 0.0949\n",
      "Epoch 704: avg loss training: 2.7487,... Gradient Norm: 0.0818\n",
      "Epoch 705: avg loss training: 2.7487,... Gradient Norm: 0.1126\n",
      "Epoch 706: avg loss training: 2.7487,... Gradient Norm: 0.1014\n",
      "Epoch 707: avg loss training: 2.7487,... Gradient Norm: 0.1127\n",
      "Epoch 708: avg loss training: 2.7487,... Gradient Norm: 0.1139\n",
      "Epoch 709: avg loss training: 2.7487,... Gradient Norm: 0.0465\n",
      "Epoch 710: avg loss training: 2.7487,... Gradient Norm: 0.1499\n",
      "Epoch 711: avg loss training: 2.7487,... Gradient Norm: 0.0656\n",
      "Epoch 712: avg loss training: 2.7487,... Gradient Norm: 0.0940\n",
      "Epoch 713: avg loss training: 2.7487,... Gradient Norm: 0.0933\n",
      "Epoch 714: avg loss training: 2.7487,... Gradient Norm: 0.1282\n",
      "Epoch 715: avg loss training: 2.7487,... Gradient Norm: 0.0460\n",
      "Epoch 716: avg loss training: 2.7487,... Gradient Norm: 0.0489\n",
      "Epoch 717: avg loss training: 2.7487,... Gradient Norm: 0.0871\n",
      "Epoch 718: avg loss training: 2.7487,... Gradient Norm: 0.1244\n",
      "Epoch 719: avg loss training: 2.7487,... Gradient Norm: 0.0476\n",
      "Epoch 720: avg loss training: 2.7487,... Gradient Norm: 0.0943\n",
      "Epoch 721: avg loss training: 2.7487,... Gradient Norm: 0.0729\n",
      "Epoch 722: avg loss training: 2.7487,... Gradient Norm: 0.1220\n",
      "Epoch 723: avg loss training: 2.7487,... Gradient Norm: 0.1079\n",
      "Epoch 724: avg loss training: 2.7487,... Gradient Norm: 0.1004\n",
      "Epoch 725: avg loss training: 2.7487,... Gradient Norm: 0.0442\n",
      "Epoch 726: avg loss training: 2.7487,... Gradient Norm: 0.1664\n",
      "Epoch 727: avg loss training: 2.7487,... Gradient Norm: 0.0871\n",
      "Epoch 728: avg loss training: 2.7487,... Gradient Norm: 0.0901\n",
      "Epoch 729: avg loss training: 2.7487,... Gradient Norm: 0.1041\n",
      "Epoch 730: avg loss training: 2.7487,... Gradient Norm: 0.0608\n",
      "Epoch 731: avg loss training: 2.7487,... Gradient Norm: 0.0847\n",
      "Epoch 732: avg loss training: 2.7487,... Gradient Norm: 0.0721\n",
      "Epoch 733: avg loss training: 2.7487,... Gradient Norm: 0.0881\n",
      "Epoch 734: avg loss training: 2.7487,... Gradient Norm: 0.1248\n",
      "Epoch 735: avg loss training: 2.7487,... Gradient Norm: 0.0932\n",
      "Epoch 736: avg loss training: 2.7487,... Gradient Norm: 0.0611\n",
      "Epoch 737: avg loss training: 2.7487,... Gradient Norm: 0.1499\n",
      "Epoch 738: avg loss training: 2.7487,... Gradient Norm: 0.0949\n",
      "Epoch 739: avg loss training: 2.7487,... Gradient Norm: 0.1013\n",
      "Epoch 740: avg loss training: 2.7487,... Gradient Norm: 0.0632\n",
      "Epoch 741: avg loss training: 2.7487,... Gradient Norm: 0.1358\n",
      "Epoch 742: avg loss training: 2.7487,... Gradient Norm: 0.0925\n",
      "Epoch 743: avg loss training: 2.7487,... Gradient Norm: 0.0943\n",
      "Epoch 744: avg loss training: 2.7487,... Gradient Norm: 0.0483\n",
      "Epoch 745: avg loss training: 2.7487,... Gradient Norm: 0.1502\n",
      "Epoch 746: avg loss training: 2.7487,... Gradient Norm: 0.0519\n",
      "Epoch 747: avg loss training: 2.7487,... Gradient Norm: 0.0950\n",
      "Epoch 748: avg loss training: 2.7487,... Gradient Norm: 0.0628\n",
      "Epoch 749: avg loss training: 2.7486,... Gradient Norm: 0.1191\n",
      "Epoch 750: avg loss training: 2.7486,... Gradient Norm: 0.0944\n",
      "Epoch 751: avg loss training: 2.7486,... Gradient Norm: 0.0797\n",
      "Epoch 752: avg loss training: 2.7486,... Gradient Norm: 0.0779\n",
      "Epoch 753: avg loss training: 2.7486,... Gradient Norm: 0.0851\n",
      "Epoch 754: avg loss training: 2.7486,... Gradient Norm: 0.0561\n",
      "Epoch 755: avg loss training: 2.7486,... Gradient Norm: 0.0545\n",
      "Epoch 756: avg loss training: 2.7486,... Gradient Norm: 0.0641\n",
      "Epoch 757: avg loss training: 2.7486,... Gradient Norm: 0.0701\n",
      "Epoch 758: avg loss training: 2.7486,... Gradient Norm: 0.0639\n",
      "Epoch 759: avg loss training: 2.7486,... Gradient Norm: 0.0715\n",
      "Epoch 760: avg loss training: 2.7486,... Gradient Norm: 0.0983\n",
      "Epoch 761: avg loss training: 2.7486,... Gradient Norm: 0.0535\n",
      "Epoch 762: avg loss training: 2.7486,... Gradient Norm: 0.0928\n",
      "Epoch 763: avg loss training: 2.7486,... Gradient Norm: 0.0807\n",
      "Epoch 764: avg loss training: 2.7486,... Gradient Norm: 0.1281\n",
      "Epoch 765: avg loss training: 2.7486,... Gradient Norm: 0.0911\n",
      "Epoch 766: avg loss training: 2.7486,... Gradient Norm: 0.0894\n",
      "Epoch 767: avg loss training: 2.7486,... Gradient Norm: 0.0794\n",
      "Epoch 768: avg loss training: 2.7486,... Gradient Norm: 0.1677\n",
      "Epoch 769: avg loss training: 2.7486,... Gradient Norm: 0.1378\n",
      "Epoch 770: avg loss training: 2.7486,... Gradient Norm: 0.0957\n",
      "Epoch 771: avg loss training: 2.7486,... Gradient Norm: 0.1157\n",
      "Epoch 772: avg loss training: 2.7486,... Gradient Norm: 0.1062\n",
      "Epoch 773: avg loss training: 2.7486,... Gradient Norm: 0.0733\n",
      "Epoch 774: avg loss training: 2.7486,... Gradient Norm: 0.1708\n",
      "Epoch 775: avg loss training: 2.7486,... Gradient Norm: 0.1222\n",
      "Epoch 776: avg loss training: 2.7486,... Gradient Norm: 0.0915\n",
      "Epoch 777: avg loss training: 2.7486,... Gradient Norm: 0.1182\n",
      "Epoch 778: avg loss training: 2.7486,... Gradient Norm: 0.1078\n",
      "Epoch 779: avg loss training: 2.7486,... Gradient Norm: 0.0408\n",
      "Epoch 780: avg loss training: 2.7486,... Gradient Norm: 0.1739\n",
      "Epoch 781: avg loss training: 2.7486,... Gradient Norm: 0.1639\n",
      "Epoch 782: avg loss training: 2.7486,... Gradient Norm: 0.0941\n",
      "Epoch 783: avg loss training: 2.7486,... Gradient Norm: 0.1156\n",
      "Epoch 784: avg loss training: 2.7486,... Gradient Norm: 0.1196\n",
      "Epoch 785: avg loss training: 2.7486,... Gradient Norm: 0.0771\n",
      "Epoch 786: avg loss training: 2.7486,... Gradient Norm: 0.1661\n",
      "Epoch 787: avg loss training: 2.7486,... Gradient Norm: 0.1734\n",
      "Epoch 788: avg loss training: 2.7486,... Gradient Norm: 0.0585\n",
      "Epoch 789: avg loss training: 2.7486,... Gradient Norm: 0.0887\n",
      "Epoch 790: avg loss training: 2.7486,... Gradient Norm: 0.1055\n",
      "Epoch 791: avg loss training: 2.7486,... Gradient Norm: 0.0895\n",
      "Epoch 792: avg loss training: 2.7486,... Gradient Norm: 0.1081\n",
      "Epoch 793: avg loss training: 2.7486,... Gradient Norm: 0.1352\n",
      "Epoch 794: avg loss training: 2.7486,... Gradient Norm: 0.0757\n",
      "Epoch 795: avg loss training: 2.7486,... Gradient Norm: 0.1026\n",
      "Epoch 796: avg loss training: 2.7486,... Gradient Norm: 0.0510\n",
      "Epoch 797: avg loss training: 2.7486,... Gradient Norm: 0.0678\n",
      "Epoch 798: avg loss training: 2.7486,... Gradient Norm: 0.0872\n",
      "Epoch 799: avg loss training: 2.7486,... Gradient Norm: 0.0602\n",
      "Epoch 800: avg loss training: 2.7486,... Gradient Norm: 0.0825\n",
      "Epoch 801: avg loss training: 2.7486,... Gradient Norm: 0.0775\n",
      "Epoch 802: avg loss training: 2.7486,... Gradient Norm: 0.1366\n",
      "Epoch 803: avg loss training: 2.7486,... Gradient Norm: 0.0523\n",
      "Epoch 804: avg loss training: 2.7486,... Gradient Norm: 0.0802\n",
      "Epoch 805: avg loss training: 2.7486,... Gradient Norm: 0.0694\n",
      "Epoch 806: avg loss training: 2.7486,... Gradient Norm: 0.0692\n",
      "Epoch 807: avg loss training: 2.7486,... Gradient Norm: 0.0770\n",
      "Epoch 808: avg loss training: 2.7486,... Gradient Norm: 0.0567\n",
      "Epoch 809: avg loss training: 2.7486,... Gradient Norm: 0.0483\n",
      "Epoch 810: avg loss training: 2.7486,... Gradient Norm: 0.0780\n",
      "Epoch 811: avg loss training: 2.7486,... Gradient Norm: 0.0888\n",
      "Epoch 812: avg loss training: 2.7486,... Gradient Norm: 0.0787\n",
      "Epoch 813: avg loss training: 2.7486,... Gradient Norm: 0.0802\n",
      "Epoch 814: avg loss training: 2.7486,... Gradient Norm: 0.0586\n",
      "Epoch 815: avg loss training: 2.7486,... Gradient Norm: 0.0461\n",
      "Epoch 816: avg loss training: 2.7486,... Gradient Norm: 0.0948\n",
      "Epoch 817: avg loss training: 2.7486,... Gradient Norm: 0.0546\n",
      "Epoch 818: avg loss training: 2.7486,... Gradient Norm: 0.0838\n",
      "Epoch 819: avg loss training: 2.7486,... Gradient Norm: 0.1426\n",
      "Epoch 820: avg loss training: 2.7486,... Gradient Norm: 0.0708\n",
      "Epoch 821: avg loss training: 2.7486,... Gradient Norm: 0.0964\n",
      "Epoch 822: avg loss training: 2.7486,... Gradient Norm: 0.0745\n",
      "Epoch 823: avg loss training: 2.7486,... Gradient Norm: 0.0909\n",
      "Epoch 824: avg loss training: 2.7486,... Gradient Norm: 0.0639\n",
      "Epoch 825: avg loss training: 2.7486,... Gradient Norm: 0.0831\n",
      "Epoch 826: avg loss training: 2.7486,... Gradient Norm: 0.0788\n",
      "Epoch 827: avg loss training: 2.7486,... Gradient Norm: 0.1118\n",
      "Epoch 828: avg loss training: 2.7486,... Gradient Norm: 0.0836\n",
      "Epoch 829: avg loss training: 2.7486,... Gradient Norm: 0.0527\n",
      "Epoch 830: avg loss training: 2.7486,... Gradient Norm: 0.0504\n",
      "Epoch 831: avg loss training: 2.7486,... Gradient Norm: 0.0566\n",
      "Epoch 832: avg loss training: 2.7486,... Gradient Norm: 0.1125\n",
      "Epoch 833: avg loss training: 2.7486,... Gradient Norm: 0.0496\n",
      "Epoch 834: avg loss training: 2.7486,... Gradient Norm: 0.0755\n",
      "Epoch 835: avg loss training: 2.7486,... Gradient Norm: 0.0751\n",
      "Epoch 836: avg loss training: 2.7486,... Gradient Norm: 0.0683\n",
      "Epoch 837: avg loss training: 2.7486,... Gradient Norm: 0.0519\n",
      "Epoch 838: avg loss training: 2.7486,... Gradient Norm: 0.1207\n",
      "Epoch 839: avg loss training: 2.7486,... Gradient Norm: 0.0849\n",
      "Epoch 840: avg loss training: 2.7486,... Gradient Norm: 0.0827\n",
      "Epoch 841: avg loss training: 2.7486,... Gradient Norm: 0.1605\n",
      "Epoch 842: avg loss training: 2.7486,... Gradient Norm: 0.0872\n",
      "Epoch 843: avg loss training: 2.7486,... Gradient Norm: 0.0609\n",
      "Epoch 844: avg loss training: 2.7486,... Gradient Norm: 0.1469\n",
      "Epoch 845: avg loss training: 2.7486,... Gradient Norm: 0.1018\n",
      "Epoch 846: avg loss training: 2.7486,... Gradient Norm: 0.0615\n",
      "Epoch 847: avg loss training: 2.7486,... Gradient Norm: 0.0968\n",
      "Epoch 848: avg loss training: 2.7486,... Gradient Norm: 0.0680\n",
      "Epoch 849: avg loss training: 2.7486,... Gradient Norm: 0.0594\n",
      "Epoch 850: avg loss training: 2.7486,... Gradient Norm: 0.0541\n",
      "Epoch 851: avg loss training: 2.7486,... Gradient Norm: 0.0845\n",
      "Epoch 852: avg loss training: 2.7486,... Gradient Norm: 0.1319\n",
      "Epoch 853: avg loss training: 2.7486,... Gradient Norm: 0.0960\n",
      "Epoch 854: avg loss training: 2.7486,... Gradient Norm: 0.0593\n",
      "Epoch 855: avg loss training: 2.7486,... Gradient Norm: 0.1181\n",
      "Epoch 856: avg loss training: 2.7486,... Gradient Norm: 0.0845\n",
      "Epoch 857: avg loss training: 2.7486,... Gradient Norm: 0.0469\n",
      "Epoch 858: avg loss training: 2.7486,... Gradient Norm: 0.0883\n",
      "Epoch 859: avg loss training: 2.7486,... Gradient Norm: 0.0394\n",
      "Epoch 860: avg loss training: 2.7486,... Gradient Norm: 0.0839\n",
      "Epoch 861: avg loss training: 2.7486,... Gradient Norm: 0.0522\n",
      "Epoch 862: avg loss training: 2.7486,... Gradient Norm: 0.1149\n",
      "Epoch 863: avg loss training: 2.7486,... Gradient Norm: 0.0901\n",
      "Epoch 864: avg loss training: 2.7486,... Gradient Norm: 0.0540\n",
      "Epoch 865: avg loss training: 2.7486,... Gradient Norm: 0.0580\n",
      "Epoch 866: avg loss training: 2.7486,... Gradient Norm: 0.0621\n",
      "Epoch 867: avg loss training: 2.7486,... Gradient Norm: 0.0359\n",
      "Epoch 868: avg loss training: 2.7486,... Gradient Norm: 0.0564\n",
      "Epoch 869: avg loss training: 2.7486,... Gradient Norm: 0.0571\n",
      "Epoch 870: avg loss training: 2.7486,... Gradient Norm: 0.0924\n",
      "Epoch 871: avg loss training: 2.7486,... Gradient Norm: 0.0955\n",
      "Epoch 872: avg loss training: 2.7486,... Gradient Norm: 0.0635\n",
      "Epoch 873: avg loss training: 2.7486,... Gradient Norm: 0.0587\n",
      "Epoch 874: avg loss training: 2.7486,... Gradient Norm: 0.0597\n",
      "Epoch 875: avg loss training: 2.7486,... Gradient Norm: 0.0597\n",
      "Epoch 876: avg loss training: 2.7486,... Gradient Norm: 0.0521\n",
      "Epoch 877: avg loss training: 2.7486,... Gradient Norm: 0.0655\n",
      "Epoch 878: avg loss training: 2.7486,... Gradient Norm: 0.0678\n",
      "Epoch 879: avg loss training: 2.7486,... Gradient Norm: 0.0543\n",
      "Epoch 880: avg loss training: 2.7486,... Gradient Norm: 0.0497\n",
      "Epoch 881: avg loss training: 2.7486,... Gradient Norm: 0.0757\n",
      "Epoch 882: avg loss training: 2.7486,... Gradient Norm: 0.0677\n",
      "Epoch 883: avg loss training: 2.7486,... Gradient Norm: 0.0599\n",
      "Epoch 884: avg loss training: 2.7486,... Gradient Norm: 0.0682\n",
      "Epoch 885: avg loss training: 2.7486,... Gradient Norm: 0.0606\n",
      "Epoch 886: avg loss training: 2.7486,... Gradient Norm: 0.0824\n",
      "Epoch 887: avg loss training: 2.7486,... Gradient Norm: 0.0484\n",
      "Epoch 888: avg loss training: 2.7486,... Gradient Norm: 0.1462\n",
      "Epoch 889: avg loss training: 2.7486,... Gradient Norm: 0.0940\n",
      "Epoch 890: avg loss training: 2.7486,... Gradient Norm: 0.0835\n",
      "Epoch 891: avg loss training: 2.7486,... Gradient Norm: 0.1227\n",
      "Epoch 892: avg loss training: 2.7486,... Gradient Norm: 0.0562\n",
      "Epoch 893: avg loss training: 2.7486,... Gradient Norm: 0.0896\n",
      "Epoch 894: avg loss training: 2.7486,... Gradient Norm: 0.0580\n",
      "Epoch 895: avg loss training: 2.7486,... Gradient Norm: 0.0900\n",
      "Epoch 896: avg loss training: 2.7486,... Gradient Norm: 0.0893\n",
      "Epoch 897: avg loss training: 2.7486,... Gradient Norm: 0.0881\n",
      "Epoch 898: avg loss training: 2.7486,... Gradient Norm: 0.1140\n",
      "Epoch 899: avg loss training: 2.7486,... Gradient Norm: 0.1135\n",
      "Epoch 900: avg loss training: 2.7486,... Gradient Norm: 0.0862\n",
      "Epoch 901: avg loss training: 2.7486,... Gradient Norm: 0.0892\n",
      "Epoch 902: avg loss training: 2.7486,... Gradient Norm: 0.0525\n",
      "Epoch 903: avg loss training: 2.7486,... Gradient Norm: 0.1059\n",
      "Epoch 904: avg loss training: 2.7486,... Gradient Norm: 0.0465\n",
      "Epoch 905: avg loss training: 2.7486,... Gradient Norm: 0.0891\n",
      "Epoch 906: avg loss training: 2.7486,... Gradient Norm: 0.0547\n",
      "Epoch 907: avg loss training: 2.7486,... Gradient Norm: 0.1434\n",
      "Epoch 908: avg loss training: 2.7486,... Gradient Norm: 0.0486\n",
      "Epoch 909: avg loss training: 2.7486,... Gradient Norm: 0.0850\n",
      "Epoch 910: avg loss training: 2.7486,... Gradient Norm: 0.0808\n",
      "Epoch 911: avg loss training: 2.7486,... Gradient Norm: 0.1698\n",
      "Epoch 912: avg loss training: 2.7486,... Gradient Norm: 0.0489\n",
      "Epoch 913: avg loss training: 2.7486,... Gradient Norm: 0.0656\n",
      "Epoch 914: avg loss training: 2.7486,... Gradient Norm: 0.0500\n",
      "Epoch 915: avg loss training: 2.7486,... Gradient Norm: 0.0597\n",
      "Epoch 916: avg loss training: 2.7486,... Gradient Norm: 0.0823\n",
      "Epoch 917: avg loss training: 2.7486,... Gradient Norm: 0.0914\n",
      "Epoch 918: avg loss training: 2.7486,... Gradient Norm: 0.0840\n",
      "Epoch 919: avg loss training: 2.7486,... Gradient Norm: 0.1467\n",
      "Epoch 920: avg loss training: 2.7486,... Gradient Norm: 0.0648\n",
      "Epoch 921: avg loss training: 2.7486,... Gradient Norm: 0.0725\n",
      "Epoch 922: avg loss training: 2.7486,... Gradient Norm: 0.0687\n",
      "Epoch 923: avg loss training: 2.7486,... Gradient Norm: 0.0855\n",
      "Epoch 924: avg loss training: 2.7486,... Gradient Norm: 0.0811\n",
      "Epoch 925: avg loss training: 2.7486,... Gradient Norm: 0.0753\n",
      "Epoch 926: avg loss training: 2.7486,... Gradient Norm: 0.0914\n",
      "Epoch 927: avg loss training: 2.7486,... Gradient Norm: 0.0947\n",
      "Epoch 928: avg loss training: 2.7486,... Gradient Norm: 0.0954\n",
      "Epoch 929: avg loss training: 2.7486,... Gradient Norm: 0.0948\n",
      "Epoch 930: avg loss training: 2.7486,... Gradient Norm: 0.0598\n",
      "Epoch 931: avg loss training: 2.7486,... Gradient Norm: 0.0868\n",
      "Epoch 932: avg loss training: 2.7486,... Gradient Norm: 0.1168\n",
      "Epoch 933: avg loss training: 2.7486,... Gradient Norm: 0.0884\n",
      "Epoch 934: avg loss training: 2.7486,... Gradient Norm: 0.0930\n",
      "Epoch 935: avg loss training: 2.7486,... Gradient Norm: 0.0589\n",
      "Epoch 936: avg loss training: 2.7486,... Gradient Norm: 0.0847\n",
      "Epoch 937: avg loss training: 2.7486,... Gradient Norm: 0.0510\n",
      "Epoch 938: avg loss training: 2.7486,... Gradient Norm: 0.0981\n",
      "Epoch 939: avg loss training: 2.7486,... Gradient Norm: 0.0562\n",
      "Epoch 940: avg loss training: 2.7486,... Gradient Norm: 0.0834\n",
      "Epoch 941: avg loss training: 2.7486,... Gradient Norm: 0.0807\n",
      "Epoch 942: avg loss training: 2.7486,... Gradient Norm: 0.0539\n",
      "Epoch 943: avg loss training: 2.7486,... Gradient Norm: 0.0504\n",
      "Epoch 944: avg loss training: 2.7486,... Gradient Norm: 0.0526\n",
      "Epoch 945: avg loss training: 2.7486,... Gradient Norm: 0.0540\n",
      "Epoch 946: avg loss training: 2.7486,... Gradient Norm: 0.0520\n",
      "Epoch 947: avg loss training: 2.7486,... Gradient Norm: 0.0532\n",
      "Epoch 948: avg loss training: 2.7486,... Gradient Norm: 0.0515\n",
      "Epoch 949: avg loss training: 2.7486,... Gradient Norm: 0.0532\n",
      "Epoch 950: avg loss training: 2.7486,... Gradient Norm: 0.0654\n",
      "Epoch 951: avg loss training: 2.7486,... Gradient Norm: 0.0535\n",
      "Epoch 952: avg loss training: 2.7486,... Gradient Norm: 0.0869\n",
      "Epoch 953: avg loss training: 2.7486,... Gradient Norm: 0.0525\n",
      "Epoch 954: avg loss training: 2.7486,... Gradient Norm: 0.0699\n",
      "Epoch 955: avg loss training: 2.7486,... Gradient Norm: 0.0953\n",
      "Epoch 956: avg loss training: 2.7486,... Gradient Norm: 0.0576\n",
      "Epoch 957: avg loss training: 2.7486,... Gradient Norm: 0.0676\n",
      "Epoch 958: avg loss training: 2.7486,... Gradient Norm: 0.0852\n",
      "Epoch 959: avg loss training: 2.7486,... Gradient Norm: 0.0955\n",
      "Epoch 960: avg loss training: 2.7486,... Gradient Norm: 0.0570\n",
      "Epoch 961: avg loss training: 2.7486,... Gradient Norm: 0.0586\n",
      "Epoch 962: avg loss training: 2.7486,... Gradient Norm: 0.0712\n",
      "Epoch 963: avg loss training: 2.7486,... Gradient Norm: 0.0800\n",
      "Epoch 964: avg loss training: 2.7486,... Gradient Norm: 0.0627\n",
      "Epoch 965: avg loss training: 2.7486,... Gradient Norm: 0.1244\n",
      "Epoch 966: avg loss training: 2.7486,... Gradient Norm: 0.0864\n",
      "Epoch 967: avg loss training: 2.7486,... Gradient Norm: 0.0899\n",
      "Epoch 968: avg loss training: 2.7486,... Gradient Norm: 0.0598\n",
      "Epoch 969: avg loss training: 2.7486,... Gradient Norm: 0.1331\n",
      "Epoch 970: avg loss training: 2.7486,... Gradient Norm: 0.0931\n",
      "Epoch 971: avg loss training: 2.7486,... Gradient Norm: 0.0871\n",
      "Epoch 972: avg loss training: 2.7486,... Gradient Norm: 0.0800\n",
      "Epoch 973: avg loss training: 2.7486,... Gradient Norm: 0.1271\n",
      "Epoch 974: avg loss training: 2.7486,... Gradient Norm: 0.0912\n",
      "Epoch 975: avg loss training: 2.7486,... Gradient Norm: 0.0891\n",
      "Epoch 976: avg loss training: 2.7486,... Gradient Norm: 0.0832\n",
      "Epoch 977: avg loss training: 2.7486,... Gradient Norm: 0.0821\n",
      "Epoch 978: avg loss training: 2.7486,... Gradient Norm: 0.0605\n",
      "Epoch 979: avg loss training: 2.7486,... Gradient Norm: 0.0872\n",
      "Epoch 980: avg loss training: 2.7486,... Gradient Norm: 0.0493\n",
      "Epoch 981: avg loss training: 2.7486,... Gradient Norm: 0.0610\n",
      "Epoch 982: avg loss training: 2.7486,... Gradient Norm: 0.1163\n",
      "Epoch 983: avg loss training: 2.7486,... Gradient Norm: 0.0508\n",
      "Epoch 984: avg loss training: 2.7486,... Gradient Norm: 0.0741\n",
      "Epoch 985: avg loss training: 2.7486,... Gradient Norm: 0.0521\n",
      "Epoch 986: avg loss training: 2.7486,... Gradient Norm: 0.0576\n",
      "Epoch 987: avg loss training: 2.7486,... Gradient Norm: 0.0566\n",
      "Epoch 988: avg loss training: 2.7486,... Gradient Norm: 0.0942\n",
      "Epoch 989: avg loss training: 2.7486,... Gradient Norm: 0.0506\n",
      "Epoch 990: avg loss training: 2.7486,... Gradient Norm: 0.1099\n",
      "Epoch 991: avg loss training: 2.7486,... Gradient Norm: 0.0597\n",
      "Epoch 992: avg loss training: 2.7486,... Gradient Norm: 0.0549\n",
      "Epoch 993: avg loss training: 2.7486,... Gradient Norm: 0.0549\n",
      "Epoch 994: avg loss training: 2.7486,... Gradient Norm: 0.0657\n",
      "Epoch 995: avg loss training: 2.7486,... Gradient Norm: 0.0829\n",
      "Epoch 996: avg loss training: 2.7486,... Gradient Norm: 0.0874\n",
      "Epoch 997: avg loss training: 2.7486,... Gradient Norm: 0.0488\n",
      "Epoch 998: avg loss training: 2.7486,... Gradient Norm: 0.0541\n",
      "Epoch 999: avg loss training: 2.7486,... Gradient Norm: 0.0613\n",
      "Epoch 1000: avg loss training: 2.7486,... Gradient Norm: 0.0766\n",
      "Epoch 1: avg loss training: 3.8354,... Gradient Norm: 13.1194\n",
      "Epoch 2: avg loss training: 17.9692,... Gradient Norm: 185.3369\n",
      "Epoch 3: avg loss training: 103.7134,... Gradient Norm: 1628.4672\n",
      "Epoch 4: avg loss training: 10.8042,... Gradient Norm: 81.4047\n",
      "Epoch 5: avg loss training: 8.6628,... Gradient Norm: 30.1585\n",
      "Epoch 6: avg loss training: 9.7040,... Gradient Norm: 43.3611\n",
      "Epoch 7: avg loss training: 8.8964,... Gradient Norm: 51.3356\n",
      "Epoch 8: avg loss training: 7.4955,... Gradient Norm: 37.2112\n",
      "Epoch 9: avg loss training: 6.0144,... Gradient Norm: 18.0081\n",
      "Epoch 10: avg loss training: 5.4218,... Gradient Norm: 16.5992\n",
      "Epoch 11: avg loss training: 5.2099,... Gradient Norm: 21.0022\n",
      "Epoch 12: avg loss training: 4.7138,... Gradient Norm: 11.1304\n",
      "Epoch 13: avg loss training: 4.5671,... Gradient Norm: 10.9526\n",
      "Epoch 14: avg loss training: 4.7383,... Gradient Norm: 18.3342\n",
      "Epoch 15: avg loss training: 4.7954,... Gradient Norm: 21.3306\n",
      "Epoch 16: avg loss training: 4.5226,... Gradient Norm: 18.5505\n",
      "Epoch 17: avg loss training: 4.1130,... Gradient Norm: 10.4746\n",
      "Epoch 18: avg loss training: 3.8799,... Gradient Norm: 4.6108\n",
      "Epoch 19: avg loss training: 3.8900,... Gradient Norm: 12.8719\n",
      "Epoch 20: avg loss training: 3.8627,... Gradient Norm: 15.0910\n",
      "Epoch 21: avg loss training: 3.7231,... Gradient Norm: 8.6086\n",
      "Epoch 22: avg loss training: 3.6529,... Gradient Norm: 4.4784\n",
      "Epoch 23: avg loss training: 3.6329,... Gradient Norm: 5.7979\n",
      "Epoch 24: avg loss training: 3.6098,... Gradient Norm: 7.9707\n",
      "Epoch 25: avg loss training: 3.5048,... Gradient Norm: 7.0300\n",
      "Epoch 26: avg loss training: 3.3882,... Gradient Norm: 2.9542\n",
      "Epoch 27: avg loss training: 3.3890,... Gradient Norm: 10.7399\n",
      "Epoch 28: avg loss training: 3.2966,... Gradient Norm: 6.6957\n",
      "Epoch 29: avg loss training: 3.2243,... Gradient Norm: 2.3412\n",
      "Epoch 30: avg loss training: 3.2156,... Gradient Norm: 5.1259\n",
      "Epoch 31: avg loss training: 3.1775,... Gradient Norm: 5.3430\n",
      "Epoch 32: avg loss training: 3.1061,... Gradient Norm: 3.3311\n",
      "Epoch 33: avg loss training: 3.0585,... Gradient Norm: 2.4990\n",
      "Epoch 34: avg loss training: 3.0390,... Gradient Norm: 4.7050\n",
      "Epoch 35: avg loss training: 3.0035,... Gradient Norm: 4.2299\n",
      "Epoch 36: avg loss training: 2.9655,... Gradient Norm: 2.7937\n",
      "Epoch 37: avg loss training: 2.9511,... Gradient Norm: 3.6213\n",
      "Epoch 38: avg loss training: 2.9360,... Gradient Norm: 3.8006\n",
      "Epoch 39: avg loss training: 2.9229,... Gradient Norm: 3.2170\n",
      "Epoch 40: avg loss training: 2.9130,... Gradient Norm: 2.6351\n",
      "Epoch 41: avg loss training: 2.9038,... Gradient Norm: 3.5281\n",
      "Epoch 42: avg loss training: 2.8953,... Gradient Norm: 2.4314\n",
      "Epoch 43: avg loss training: 2.8938,... Gradient Norm: 2.5568\n",
      "Epoch 44: avg loss training: 2.8801,... Gradient Norm: 2.0259\n",
      "Epoch 45: avg loss training: 2.8739,... Gradient Norm: 1.6792\n",
      "Epoch 46: avg loss training: 2.8754,... Gradient Norm: 2.4812\n",
      "Epoch 47: avg loss training: 2.8647,... Gradient Norm: 1.0982\n",
      "Epoch 48: avg loss training: 2.8655,... Gradient Norm: 2.0571\n",
      "Epoch 49: avg loss training: 2.8651,... Gradient Norm: 2.3238\n",
      "Epoch 50: avg loss training: 2.8572,... Gradient Norm: 1.3415\n",
      "Epoch 51: avg loss training: 2.8591,... Gradient Norm: 2.0140\n",
      "Epoch 52: avg loss training: 2.8570,... Gradient Norm: 1.8598\n",
      "Epoch 53: avg loss training: 2.8551,... Gradient Norm: 2.5357\n",
      "Epoch 54: avg loss training: 2.8518,... Gradient Norm: 1.0090\n",
      "Epoch 55: avg loss training: 2.8535,... Gradient Norm: 2.7607\n",
      "Epoch 56: avg loss training: 2.8503,... Gradient Norm: 1.6290\n",
      "Epoch 57: avg loss training: 2.8488,... Gradient Norm: 1.3540\n",
      "Epoch 58: avg loss training: 2.8495,... Gradient Norm: 2.3243\n",
      "Epoch 59: avg loss training: 2.8461,... Gradient Norm: 0.7743\n",
      "Epoch 60: avg loss training: 2.8459,... Gradient Norm: 1.8093\n",
      "Epoch 61: avg loss training: 2.8450,... Gradient Norm: 1.4477\n",
      "Epoch 62: avg loss training: 2.8442,... Gradient Norm: 1.2553\n",
      "Epoch 63: avg loss training: 2.8432,... Gradient Norm: 0.9328\n",
      "Epoch 64: avg loss training: 2.8430,... Gradient Norm: 1.0675\n",
      "Epoch 65: avg loss training: 2.8412,... Gradient Norm: 0.8001\n",
      "Epoch 66: avg loss training: 2.8411,... Gradient Norm: 1.0338\n",
      "Epoch 67: avg loss training: 2.8404,... Gradient Norm: 0.7191\n",
      "Epoch 68: avg loss training: 2.8398,... Gradient Norm: 1.3570\n",
      "Epoch 69: avg loss training: 2.8393,... Gradient Norm: 0.7694\n",
      "Epoch 70: avg loss training: 2.8390,... Gradient Norm: 1.2053\n",
      "Epoch 71: avg loss training: 2.8380,... Gradient Norm: 0.4136\n",
      "Epoch 72: avg loss training: 2.8382,... Gradient Norm: 1.4864\n",
      "Epoch 73: avg loss training: 2.8374,... Gradient Norm: 0.7926\n",
      "Epoch 74: avg loss training: 2.8372,... Gradient Norm: 0.9009\n",
      "Epoch 75: avg loss training: 2.8368,... Gradient Norm: 0.8755\n",
      "Epoch 76: avg loss training: 2.8360,... Gradient Norm: 0.2346\n",
      "Epoch 77: avg loss training: 2.8362,... Gradient Norm: 1.0398\n",
      "Epoch 78: avg loss training: 2.8355,... Gradient Norm: 0.9283\n",
      "Epoch 79: avg loss training: 2.8351,... Gradient Norm: 0.3452\n",
      "Epoch 80: avg loss training: 2.8350,... Gradient Norm: 0.8768\n",
      "Epoch 81: avg loss training: 2.8345,... Gradient Norm: 0.2525\n",
      "Epoch 82: avg loss training: 2.8344,... Gradient Norm: 1.0920\n",
      "Epoch 83: avg loss training: 2.8341,... Gradient Norm: 0.6248\n",
      "Epoch 84: avg loss training: 2.8337,... Gradient Norm: 0.4187\n",
      "Epoch 85: avg loss training: 2.8337,... Gradient Norm: 0.7519\n",
      "Epoch 86: avg loss training: 2.8332,... Gradient Norm: 0.4348\n",
      "Epoch 87: avg loss training: 2.8331,... Gradient Norm: 0.5626\n",
      "Epoch 88: avg loss training: 2.8329,... Gradient Norm: 0.9478\n",
      "Epoch 89: avg loss training: 2.8324,... Gradient Norm: 0.2417\n",
      "Epoch 90: avg loss training: 2.8323,... Gradient Norm: 0.5843\n",
      "Epoch 91: avg loss training: 2.8320,... Gradient Norm: 0.4007\n",
      "Epoch 92: avg loss training: 2.8317,... Gradient Norm: 0.4619\n",
      "Epoch 93: avg loss training: 2.8318,... Gradient Norm: 0.8699\n",
      "Epoch 94: avg loss training: 2.8314,... Gradient Norm: 0.5541\n",
      "Epoch 95: avg loss training: 2.8311,... Gradient Norm: 0.3123\n",
      "Epoch 96: avg loss training: 2.8311,... Gradient Norm: 0.7870\n",
      "Epoch 97: avg loss training: 2.8308,... Gradient Norm: 0.5476\n",
      "Epoch 98: avg loss training: 2.8305,... Gradient Norm: 0.3005\n",
      "Epoch 99: avg loss training: 2.8304,... Gradient Norm: 0.5562\n",
      "Epoch 100: avg loss training: 2.8302,... Gradient Norm: 0.1942\n",
      "Epoch 101: avg loss training: 2.8300,... Gradient Norm: 0.4792\n",
      "Epoch 102: avg loss training: 2.8299,... Gradient Norm: 0.4677\n",
      "Epoch 103: avg loss training: 2.8297,... Gradient Norm: 0.2764\n",
      "Epoch 104: avg loss training: 2.8295,... Gradient Norm: 0.1660\n",
      "Epoch 105: avg loss training: 2.8294,... Gradient Norm: 0.2453\n",
      "Epoch 106: avg loss training: 2.8292,... Gradient Norm: 0.1883\n",
      "Epoch 107: avg loss training: 2.8291,... Gradient Norm: 0.4070\n",
      "Epoch 108: avg loss training: 2.8291,... Gradient Norm: 0.7983\n",
      "Epoch 109: avg loss training: 2.8289,... Gradient Norm: 0.5950\n",
      "Epoch 110: avg loss training: 2.8287,... Gradient Norm: 0.1436\n",
      "Epoch 111: avg loss training: 2.8286,... Gradient Norm: 0.4336\n",
      "Epoch 112: avg loss training: 2.8285,... Gradient Norm: 0.8233\n",
      "Epoch 113: avg loss training: 2.8284,... Gradient Norm: 0.9518\n",
      "Epoch 114: avg loss training: 2.8282,... Gradient Norm: 0.6300\n",
      "Epoch 115: avg loss training: 2.8280,... Gradient Norm: 0.0865\n",
      "Epoch 116: avg loss training: 2.8279,... Gradient Norm: 0.4658\n",
      "Epoch 117: avg loss training: 2.8278,... Gradient Norm: 0.6207\n",
      "Epoch 118: avg loss training: 2.8277,... Gradient Norm: 0.5042\n",
      "Epoch 119: avg loss training: 2.8276,... Gradient Norm: 0.2243\n",
      "Epoch 120: avg loss training: 2.8275,... Gradient Norm: 0.3548\n",
      "Epoch 121: avg loss training: 2.8274,... Gradient Norm: 0.5395\n",
      "Epoch 122: avg loss training: 2.8273,... Gradient Norm: 0.2792\n",
      "Epoch 123: avg loss training: 2.8272,... Gradient Norm: 0.2200\n",
      "Epoch 124: avg loss training: 2.8271,... Gradient Norm: 0.4323\n",
      "Epoch 125: avg loss training: 2.8270,... Gradient Norm: 0.2406\n",
      "Epoch 126: avg loss training: 2.8269,... Gradient Norm: 0.1958\n",
      "Epoch 127: avg loss training: 2.8268,... Gradient Norm: 0.4097\n",
      "Epoch 128: avg loss training: 2.8267,... Gradient Norm: 0.2825\n",
      "Epoch 129: avg loss training: 2.8265,... Gradient Norm: 0.1195\n",
      "Epoch 130: avg loss training: 2.8265,... Gradient Norm: 0.3270\n",
      "Epoch 131: avg loss training: 2.8264,... Gradient Norm: 0.3737\n",
      "Epoch 132: avg loss training: 2.8263,... Gradient Norm: 0.3377\n",
      "Epoch 133: avg loss training: 2.8262,... Gradient Norm: 0.3371\n",
      "Epoch 134: avg loss training: 2.8261,... Gradient Norm: 0.3975\n",
      "Epoch 135: avg loss training: 2.8260,... Gradient Norm: 0.6245\n",
      "Epoch 136: avg loss training: 2.8259,... Gradient Norm: 0.5568\n",
      "Epoch 137: avg loss training: 2.8258,... Gradient Norm: 0.1999\n",
      "Epoch 138: avg loss training: 2.8257,... Gradient Norm: 0.3397\n",
      "Epoch 139: avg loss training: 2.8256,... Gradient Norm: 0.4603\n",
      "Epoch 140: avg loss training: 2.8256,... Gradient Norm: 0.5988\n",
      "Epoch 141: avg loss training: 2.8256,... Gradient Norm: 0.6820\n",
      "Epoch 142: avg loss training: 2.8254,... Gradient Norm: 0.6509\n",
      "Epoch 143: avg loss training: 2.8253,... Gradient Norm: 0.5328\n",
      "Epoch 144: avg loss training: 2.8252,... Gradient Norm: 0.3255\n",
      "Epoch 145: avg loss training: 2.8252,... Gradient Norm: 0.5318\n",
      "Epoch 146: avg loss training: 2.8252,... Gradient Norm: 1.0803\n",
      "Epoch 147: avg loss training: 2.8251,... Gradient Norm: 1.0520\n",
      "Epoch 148: avg loss training: 2.8250,... Gradient Norm: 0.6545\n",
      "Epoch 149: avg loss training: 2.8249,... Gradient Norm: 0.3937\n",
      "Epoch 150: avg loss training: 2.8247,... Gradient Norm: 0.1544\n",
      "Epoch 151: avg loss training: 2.8248,... Gradient Norm: 0.5040\n",
      "Epoch 152: avg loss training: 2.8247,... Gradient Norm: 0.8017\n",
      "Epoch 153: avg loss training: 2.8246,... Gradient Norm: 0.8930\n",
      "Epoch 154: avg loss training: 2.8245,... Gradient Norm: 0.5032\n",
      "Epoch 155: avg loss training: 2.8244,... Gradient Norm: 0.2021\n",
      "Epoch 156: avg loss training: 2.8244,... Gradient Norm: 0.6821\n",
      "Epoch 157: avg loss training: 2.8244,... Gradient Norm: 0.7983\n",
      "Epoch 158: avg loss training: 2.8242,... Gradient Norm: 0.4949\n",
      "Epoch 159: avg loss training: 2.8241,... Gradient Norm: 0.1321\n",
      "Epoch 160: avg loss training: 2.8240,... Gradient Norm: 0.6044\n",
      "Epoch 161: avg loss training: 2.8241,... Gradient Norm: 0.8428\n",
      "Epoch 162: avg loss training: 2.8240,... Gradient Norm: 0.7779\n",
      "Epoch 163: avg loss training: 2.8238,... Gradient Norm: 0.4103\n",
      "Epoch 164: avg loss training: 2.8238,... Gradient Norm: 0.5235\n",
      "Epoch 165: avg loss training: 2.8238,... Gradient Norm: 1.1496\n",
      "Epoch 166: avg loss training: 2.8238,... Gradient Norm: 1.1019\n",
      "Epoch 167: avg loss training: 2.8235,... Gradient Norm: 0.2772\n",
      "Epoch 168: avg loss training: 2.8237,... Gradient Norm: 1.2830\n",
      "Epoch 169: avg loss training: 2.8237,... Gradient Norm: 1.2935\n",
      "Epoch 170: avg loss training: 2.8235,... Gradient Norm: 0.6642\n",
      "Epoch 171: avg loss training: 2.8234,... Gradient Norm: 0.6134\n",
      "Epoch 172: avg loss training: 2.8236,... Gradient Norm: 1.2990\n",
      "Epoch 173: avg loss training: 2.8233,... Gradient Norm: 1.1244\n",
      "Epoch 174: avg loss training: 2.8232,... Gradient Norm: 0.4778\n",
      "Epoch 175: avg loss training: 2.8232,... Gradient Norm: 0.7746\n",
      "Epoch 176: avg loss training: 2.8231,... Gradient Norm: 0.7219\n",
      "Epoch 177: avg loss training: 2.8229,... Gradient Norm: 0.4654\n",
      "Epoch 178: avg loss training: 2.8230,... Gradient Norm: 0.6566\n",
      "Epoch 179: avg loss training: 2.8229,... Gradient Norm: 0.4015\n",
      "Epoch 180: avg loss training: 2.8228,... Gradient Norm: 0.6668\n",
      "Epoch 181: avg loss training: 2.8228,... Gradient Norm: 0.4408\n",
      "Epoch 182: avg loss training: 2.8227,... Gradient Norm: 0.3214\n",
      "Epoch 183: avg loss training: 2.8227,... Gradient Norm: 0.3931\n",
      "Epoch 184: avg loss training: 2.8226,... Gradient Norm: 0.2679\n",
      "Epoch 185: avg loss training: 2.8225,... Gradient Norm: 0.4774\n",
      "Epoch 186: avg loss training: 2.8224,... Gradient Norm: 0.2466\n",
      "Epoch 187: avg loss training: 2.8224,... Gradient Norm: 0.4564\n",
      "Epoch 188: avg loss training: 2.8223,... Gradient Norm: 0.2481\n",
      "Epoch 189: avg loss training: 2.8223,... Gradient Norm: 0.4531\n",
      "Epoch 190: avg loss training: 2.8222,... Gradient Norm: 0.4239\n",
      "Epoch 191: avg loss training: 2.8222,... Gradient Norm: 0.3640\n",
      "Epoch 192: avg loss training: 2.8221,... Gradient Norm: 0.2253\n",
      "Epoch 193: avg loss training: 2.8221,... Gradient Norm: 0.3075\n",
      "Epoch 194: avg loss training: 2.8220,... Gradient Norm: 0.2429\n",
      "Epoch 195: avg loss training: 2.8220,... Gradient Norm: 0.3703\n",
      "Epoch 196: avg loss training: 2.8219,... Gradient Norm: 0.2158\n",
      "Epoch 197: avg loss training: 2.8219,... Gradient Norm: 0.4615\n",
      "Epoch 198: avg loss training: 2.8218,... Gradient Norm: 0.3732\n",
      "Epoch 199: avg loss training: 2.8218,... Gradient Norm: 0.4020\n",
      "Epoch 200: avg loss training: 2.8218,... Gradient Norm: 0.7114\n",
      "Epoch 201: avg loss training: 2.8217,... Gradient Norm: 0.2558\n",
      "Epoch 202: avg loss training: 2.8217,... Gradient Norm: 0.3938\n",
      "Epoch 203: avg loss training: 2.8216,... Gradient Norm: 0.2856\n",
      "Epoch 204: avg loss training: 2.8215,... Gradient Norm: 0.2154\n",
      "Epoch 205: avg loss training: 2.8215,... Gradient Norm: 0.3184\n",
      "Epoch 206: avg loss training: 2.8214,... Gradient Norm: 0.1646\n",
      "Epoch 207: avg loss training: 2.8214,... Gradient Norm: 0.1678\n",
      "Epoch 208: avg loss training: 2.8214,... Gradient Norm: 0.1883\n",
      "Epoch 209: avg loss training: 2.8213,... Gradient Norm: 0.2727\n",
      "Epoch 210: avg loss training: 2.8213,... Gradient Norm: 0.2062\n",
      "Epoch 211: avg loss training: 2.8212,... Gradient Norm: 0.0873\n",
      "Epoch 212: avg loss training: 2.8212,... Gradient Norm: 0.2160\n",
      "Epoch 213: avg loss training: 2.8211,... Gradient Norm: 0.3912\n",
      "Epoch 214: avg loss training: 2.8211,... Gradient Norm: 0.4432\n",
      "Epoch 215: avg loss training: 2.8210,... Gradient Norm: 0.4552\n",
      "Epoch 216: avg loss training: 2.8210,... Gradient Norm: 0.4046\n",
      "Epoch 217: avg loss training: 2.8210,... Gradient Norm: 0.1524\n",
      "Epoch 218: avg loss training: 2.8209,... Gradient Norm: 0.3826\n",
      "Epoch 219: avg loss training: 2.8209,... Gradient Norm: 0.5170\n",
      "Epoch 220: avg loss training: 2.8208,... Gradient Norm: 0.2112\n",
      "Epoch 221: avg loss training: 2.8208,... Gradient Norm: 0.1877\n",
      "Epoch 222: avg loss training: 2.8207,... Gradient Norm: 0.2689\n",
      "Epoch 223: avg loss training: 2.8207,... Gradient Norm: 0.3073\n",
      "Epoch 224: avg loss training: 2.8207,... Gradient Norm: 0.2102\n",
      "Epoch 225: avg loss training: 2.8206,... Gradient Norm: 0.1860\n",
      "Epoch 226: avg loss training: 2.8206,... Gradient Norm: 0.3107\n",
      "Epoch 227: avg loss training: 2.8206,... Gradient Norm: 0.5972\n",
      "Epoch 228: avg loss training: 2.8205,... Gradient Norm: 0.4336\n",
      "Epoch 229: avg loss training: 2.8205,... Gradient Norm: 0.3375\n",
      "Epoch 230: avg loss training: 2.8205,... Gradient Norm: 0.5981\n",
      "Epoch 231: avg loss training: 2.8204,... Gradient Norm: 0.0954\n",
      "Epoch 232: avg loss training: 2.8204,... Gradient Norm: 0.6594\n",
      "Epoch 233: avg loss training: 2.8203,... Gradient Norm: 0.3893\n",
      "Epoch 234: avg loss training: 2.8203,... Gradient Norm: 0.2629\n",
      "Epoch 235: avg loss training: 2.8202,... Gradient Norm: 0.4653\n",
      "Epoch 236: avg loss training: 2.8202,... Gradient Norm: 0.1353\n",
      "Epoch 237: avg loss training: 2.8202,... Gradient Norm: 0.5492\n",
      "Epoch 238: avg loss training: 2.8202,... Gradient Norm: 0.5415\n",
      "Epoch 239: avg loss training: 2.8201,... Gradient Norm: 0.1663\n",
      "Epoch 240: avg loss training: 2.8201,... Gradient Norm: 0.5517\n",
      "Epoch 241: avg loss training: 2.8201,... Gradient Norm: 0.3925\n",
      "Epoch 242: avg loss training: 2.8200,... Gradient Norm: 0.2986\n",
      "Epoch 243: avg loss training: 2.8200,... Gradient Norm: 0.6747\n",
      "Epoch 244: avg loss training: 2.8199,... Gradient Norm: 0.3203\n",
      "Epoch 245: avg loss training: 2.8199,... Gradient Norm: 0.4654\n",
      "Epoch 246: avg loss training: 2.8199,... Gradient Norm: 0.6222\n",
      "Epoch 247: avg loss training: 2.8198,... Gradient Norm: 0.2458\n",
      "Epoch 248: avg loss training: 2.8198,... Gradient Norm: 0.6933\n",
      "Epoch 249: avg loss training: 2.8198,... Gradient Norm: 0.4732\n",
      "Epoch 250: avg loss training: 2.8197,... Gradient Norm: 0.4049\n",
      "Epoch 251: avg loss training: 2.8197,... Gradient Norm: 0.6023\n",
      "Epoch 252: avg loss training: 2.8196,... Gradient Norm: 0.2418\n",
      "Epoch 253: avg loss training: 2.8196,... Gradient Norm: 0.2853\n",
      "Epoch 254: avg loss training: 2.8196,... Gradient Norm: 0.4258\n",
      "Epoch 255: avg loss training: 2.8195,... Gradient Norm: 0.1505\n",
      "Epoch 256: avg loss training: 2.8196,... Gradient Norm: 0.6967\n",
      "Epoch 257: avg loss training: 2.8195,... Gradient Norm: 0.6371\n",
      "Epoch 258: avg loss training: 2.8195,... Gradient Norm: 0.3245\n",
      "Epoch 259: avg loss training: 2.8195,... Gradient Norm: 0.8580\n",
      "Epoch 260: avg loss training: 2.8194,... Gradient Norm: 0.1930\n",
      "Epoch 261: avg loss training: 2.8194,... Gradient Norm: 0.7339\n",
      "Epoch 262: avg loss training: 2.8193,... Gradient Norm: 0.5195\n",
      "Epoch 263: avg loss training: 2.8193,... Gradient Norm: 0.3245\n",
      "Epoch 264: avg loss training: 2.8193,... Gradient Norm: 0.4853\n",
      "Epoch 265: avg loss training: 2.8192,... Gradient Norm: 0.1528\n",
      "Epoch 266: avg loss training: 2.8192,... Gradient Norm: 0.3537\n",
      "Epoch 267: avg loss training: 2.8192,... Gradient Norm: 0.2952\n",
      "Epoch 268: avg loss training: 2.8191,... Gradient Norm: 0.1911\n",
      "Epoch 269: avg loss training: 2.8191,... Gradient Norm: 0.5282\n",
      "Epoch 270: avg loss training: 2.8191,... Gradient Norm: 0.4491\n",
      "Epoch 271: avg loss training: 2.8190,... Gradient Norm: 0.2028\n",
      "Epoch 272: avg loss training: 2.8190,... Gradient Norm: 0.4648\n",
      "Epoch 273: avg loss training: 2.8190,... Gradient Norm: 0.5067\n",
      "Epoch 274: avg loss training: 2.8189,... Gradient Norm: 0.2145\n",
      "Epoch 275: avg loss training: 2.8189,... Gradient Norm: 0.6842\n",
      "Epoch 276: avg loss training: 2.8189,... Gradient Norm: 0.1282\n",
      "Epoch 277: avg loss training: 2.8189,... Gradient Norm: 0.6317\n",
      "Epoch 278: avg loss training: 2.8188,... Gradient Norm: 0.4225\n",
      "Epoch 279: avg loss training: 2.8188,... Gradient Norm: 0.2098\n",
      "Epoch 280: avg loss training: 2.8188,... Gradient Norm: 0.2552\n",
      "Epoch 281: avg loss training: 2.8187,... Gradient Norm: 0.2151\n",
      "Epoch 282: avg loss training: 2.8187,... Gradient Norm: 0.1972\n",
      "Epoch 283: avg loss training: 2.8187,... Gradient Norm: 0.3879\n",
      "Epoch 284: avg loss training: 2.8187,... Gradient Norm: 0.5530\n",
      "Epoch 285: avg loss training: 2.8186,... Gradient Norm: 0.1593\n",
      "Epoch 286: avg loss training: 2.8186,... Gradient Norm: 0.7425\n",
      "Epoch 287: avg loss training: 2.8186,... Gradient Norm: 0.3363\n",
      "Epoch 288: avg loss training: 2.8185,... Gradient Norm: 0.4331\n",
      "Epoch 289: avg loss training: 2.8185,... Gradient Norm: 0.4705\n",
      "Epoch 290: avg loss training: 2.8185,... Gradient Norm: 0.1859\n",
      "Epoch 291: avg loss training: 2.8184,... Gradient Norm: 0.2612\n",
      "Epoch 292: avg loss training: 2.8184,... Gradient Norm: 0.3531\n",
      "Epoch 293: avg loss training: 2.8184,... Gradient Norm: 0.2670\n",
      "Epoch 294: avg loss training: 2.8184,... Gradient Norm: 0.2188\n",
      "Epoch 295: avg loss training: 2.8184,... Gradient Norm: 0.5378\n",
      "Epoch 296: avg loss training: 2.8183,... Gradient Norm: 0.2613\n",
      "Epoch 297: avg loss training: 2.8183,... Gradient Norm: 0.4603\n",
      "Epoch 298: avg loss training: 2.8183,... Gradient Norm: 0.4094\n",
      "Epoch 299: avg loss training: 2.8182,... Gradient Norm: 0.2001\n",
      "Epoch 300: avg loss training: 2.8182,... Gradient Norm: 0.2854\n",
      "Epoch 301: avg loss training: 2.8182,... Gradient Norm: 0.2289\n",
      "Epoch 302: avg loss training: 2.8182,... Gradient Norm: 0.4574\n",
      "Epoch 303: avg loss training: 2.8182,... Gradient Norm: 0.2357\n",
      "Epoch 304: avg loss training: 2.8181,... Gradient Norm: 0.2436\n",
      "Epoch 305: avg loss training: 2.8181,... Gradient Norm: 0.2701\n",
      "Epoch 306: avg loss training: 2.8181,... Gradient Norm: 0.1082\n",
      "Epoch 307: avg loss training: 2.8181,... Gradient Norm: 0.3832\n",
      "Epoch 308: avg loss training: 2.8181,... Gradient Norm: 0.4418\n",
      "Epoch 309: avg loss training: 2.8180,... Gradient Norm: 0.2330\n",
      "Epoch 310: avg loss training: 2.8180,... Gradient Norm: 0.3433\n",
      "Epoch 311: avg loss training: 2.8180,... Gradient Norm: 0.4867\n",
      "Epoch 312: avg loss training: 2.8180,... Gradient Norm: 0.1727\n",
      "Epoch 313: avg loss training: 2.8179,... Gradient Norm: 0.4874\n",
      "Epoch 314: avg loss training: 2.8179,... Gradient Norm: 0.1790\n",
      "Epoch 315: avg loss training: 2.8179,... Gradient Norm: 0.4211\n",
      "Epoch 316: avg loss training: 2.8179,... Gradient Norm: 0.2650\n",
      "Epoch 317: avg loss training: 2.8179,... Gradient Norm: 0.2698\n",
      "Epoch 318: avg loss training: 2.8178,... Gradient Norm: 0.1668\n",
      "Epoch 319: avg loss training: 2.8178,... Gradient Norm: 0.4203\n",
      "Epoch 320: avg loss training: 2.8178,... Gradient Norm: 0.3233\n",
      "Epoch 321: avg loss training: 2.8178,... Gradient Norm: 0.2406\n",
      "Epoch 322: avg loss training: 2.8177,... Gradient Norm: 0.5254\n",
      "Epoch 323: avg loss training: 2.8177,... Gradient Norm: 0.4211\n",
      "Epoch 324: avg loss training: 2.8177,... Gradient Norm: 0.1618\n",
      "Epoch 325: avg loss training: 2.8177,... Gradient Norm: 0.3623\n",
      "Epoch 326: avg loss training: 2.8177,... Gradient Norm: 0.3247\n",
      "Epoch 327: avg loss training: 2.8176,... Gradient Norm: 0.2842\n",
      "Epoch 328: avg loss training: 2.8176,... Gradient Norm: 0.4584\n",
      "Epoch 329: avg loss training: 2.8176,... Gradient Norm: 0.3225\n",
      "Epoch 330: avg loss training: 2.8176,... Gradient Norm: 0.4364\n",
      "Epoch 331: avg loss training: 2.8176,... Gradient Norm: 0.6234\n",
      "Epoch 332: avg loss training: 2.8175,... Gradient Norm: 0.1938\n",
      "Epoch 333: avg loss training: 2.8175,... Gradient Norm: 0.4404\n",
      "Epoch 334: avg loss training: 2.8175,... Gradient Norm: 0.3324\n",
      "Epoch 335: avg loss training: 2.8175,... Gradient Norm: 0.1587\n",
      "Epoch 336: avg loss training: 2.8175,... Gradient Norm: 0.4299\n",
      "Epoch 337: avg loss training: 2.8174,... Gradient Norm: 0.1681\n",
      "Epoch 338: avg loss training: 2.8174,... Gradient Norm: 0.4151\n",
      "Epoch 339: avg loss training: 2.8174,... Gradient Norm: 0.4391\n",
      "Epoch 340: avg loss training: 2.8174,... Gradient Norm: 0.3243\n",
      "Epoch 341: avg loss training: 2.8174,... Gradient Norm: 0.9051\n",
      "Epoch 342: avg loss training: 2.8174,... Gradient Norm: 0.4140\n",
      "Epoch 343: avg loss training: 2.8174,... Gradient Norm: 0.5247\n",
      "Epoch 344: avg loss training: 2.8173,... Gradient Norm: 0.1135\n",
      "Epoch 345: avg loss training: 2.8173,... Gradient Norm: 0.8459\n",
      "Epoch 346: avg loss training: 2.8173,... Gradient Norm: 0.4694\n",
      "Epoch 347: avg loss training: 2.8173,... Gradient Norm: 0.5069\n",
      "Epoch 348: avg loss training: 2.8173,... Gradient Norm: 0.2018\n",
      "Epoch 349: avg loss training: 2.8173,... Gradient Norm: 0.8881\n",
      "Epoch 350: avg loss training: 2.8173,... Gradient Norm: 0.4622\n",
      "Epoch 351: avg loss training: 2.8172,... Gradient Norm: 0.6138\n",
      "Epoch 352: avg loss training: 2.8172,... Gradient Norm: 0.2621\n",
      "Epoch 353: avg loss training: 2.8172,... Gradient Norm: 0.8901\n",
      "Epoch 354: avg loss training: 2.8172,... Gradient Norm: 0.4100\n",
      "Epoch 355: avg loss training: 2.8172,... Gradient Norm: 0.6447\n",
      "Epoch 356: avg loss training: 2.8171,... Gradient Norm: 0.3345\n",
      "Epoch 357: avg loss training: 2.8171,... Gradient Norm: 0.4678\n",
      "Epoch 358: avg loss training: 2.8171,... Gradient Norm: 0.2464\n",
      "Epoch 359: avg loss training: 2.8171,... Gradient Norm: 0.4655\n",
      "Epoch 360: avg loss training: 2.8171,... Gradient Norm: 0.3904\n",
      "Epoch 361: avg loss training: 2.8171,... Gradient Norm: 0.9976\n",
      "Epoch 362: avg loss training: 2.8170,... Gradient Norm: 0.1666\n",
      "Epoch 363: avg loss training: 2.8170,... Gradient Norm: 0.5002\n",
      "Epoch 364: avg loss training: 2.8170,... Gradient Norm: 0.2801\n",
      "Epoch 365: avg loss training: 2.8170,... Gradient Norm: 0.3855\n",
      "Epoch 366: avg loss training: 2.8170,... Gradient Norm: 0.4222\n",
      "Epoch 367: avg loss training: 2.8170,... Gradient Norm: 0.3508\n",
      "Epoch 368: avg loss training: 2.8169,... Gradient Norm: 0.3471\n",
      "Epoch 369: avg loss training: 2.8169,... Gradient Norm: 0.3612\n",
      "Epoch 370: avg loss training: 2.8169,... Gradient Norm: 0.1173\n",
      "Epoch 371: avg loss training: 2.8169,... Gradient Norm: 0.3204\n",
      "Epoch 372: avg loss training: 2.8169,... Gradient Norm: 0.1209\n",
      "Epoch 373: avg loss training: 2.8169,... Gradient Norm: 0.4497\n",
      "Epoch 374: avg loss training: 2.8168,... Gradient Norm: 0.2592\n",
      "Epoch 375: avg loss training: 2.8168,... Gradient Norm: 0.2201\n",
      "Epoch 376: avg loss training: 2.8168,... Gradient Norm: 0.2216\n",
      "Epoch 377: avg loss training: 2.8168,... Gradient Norm: 0.3228\n",
      "Epoch 378: avg loss training: 2.8168,... Gradient Norm: 0.3034\n",
      "Epoch 379: avg loss training: 2.8168,... Gradient Norm: 0.2856\n",
      "Epoch 380: avg loss training: 2.8168,... Gradient Norm: 0.4348\n",
      "Epoch 381: avg loss training: 2.8168,... Gradient Norm: 0.1838\n",
      "Epoch 382: avg loss training: 2.8167,... Gradient Norm: 0.1966\n",
      "Epoch 383: avg loss training: 2.8167,... Gradient Norm: 0.1565\n",
      "Epoch 384: avg loss training: 2.8167,... Gradient Norm: 0.2167\n",
      "Epoch 385: avg loss training: 2.8167,... Gradient Norm: 0.2877\n",
      "Epoch 386: avg loss training: 2.8167,... Gradient Norm: 0.3088\n",
      "Epoch 387: avg loss training: 2.8167,... Gradient Norm: 0.1335\n",
      "Epoch 388: avg loss training: 2.8167,... Gradient Norm: 0.2000\n",
      "Epoch 389: avg loss training: 2.8166,... Gradient Norm: 0.2008\n",
      "Epoch 390: avg loss training: 2.8167,... Gradient Norm: 0.4631\n",
      "Epoch 391: avg loss training: 2.8166,... Gradient Norm: 0.2039\n",
      "Epoch 392: avg loss training: 2.8166,... Gradient Norm: 0.3668\n",
      "Epoch 393: avg loss training: 2.8166,... Gradient Norm: 0.2473\n",
      "Epoch 394: avg loss training: 2.8166,... Gradient Norm: 0.1202\n",
      "Epoch 395: avg loss training: 2.8166,... Gradient Norm: 0.4001\n",
      "Epoch 396: avg loss training: 2.8166,... Gradient Norm: 0.2361\n",
      "Epoch 397: avg loss training: 2.8166,... Gradient Norm: 0.4132\n",
      "Epoch 398: avg loss training: 2.8166,... Gradient Norm: 0.2779\n",
      "Epoch 399: avg loss training: 2.8165,... Gradient Norm: 0.2764\n",
      "Epoch 400: avg loss training: 2.8165,... Gradient Norm: 0.3677\n",
      "Epoch 401: avg loss training: 2.8165,... Gradient Norm: 0.1526\n",
      "Epoch 402: avg loss training: 2.8165,... Gradient Norm: 0.3800\n",
      "Epoch 403: avg loss training: 2.8165,... Gradient Norm: 0.2579\n",
      "Epoch 404: avg loss training: 2.8165,... Gradient Norm: 0.3566\n",
      "Epoch 405: avg loss training: 2.8165,... Gradient Norm: 0.1903\n",
      "Epoch 406: avg loss training: 2.8165,... Gradient Norm: 0.2483\n",
      "Epoch 407: avg loss training: 2.8165,... Gradient Norm: 0.2207\n",
      "Epoch 408: avg loss training: 2.8164,... Gradient Norm: 0.1931\n",
      "Epoch 409: avg loss training: 2.8164,... Gradient Norm: 0.2452\n",
      "Epoch 410: avg loss training: 2.8164,... Gradient Norm: 0.2316\n",
      "Epoch 411: avg loss training: 2.8164,... Gradient Norm: 0.3923\n",
      "Epoch 412: avg loss training: 2.8164,... Gradient Norm: 0.3055\n",
      "Epoch 413: avg loss training: 2.8164,... Gradient Norm: 0.1741\n",
      "Epoch 414: avg loss training: 2.8164,... Gradient Norm: 0.4300\n",
      "Epoch 415: avg loss training: 2.8164,... Gradient Norm: 0.1590\n",
      "Epoch 416: avg loss training: 2.8164,... Gradient Norm: 0.2924\n",
      "Epoch 417: avg loss training: 2.8164,... Gradient Norm: 0.1734\n",
      "Epoch 418: avg loss training: 2.8163,... Gradient Norm: 0.3748\n",
      "Epoch 419: avg loss training: 2.8163,... Gradient Norm: 0.1524\n",
      "Epoch 420: avg loss training: 2.8163,... Gradient Norm: 0.3431\n",
      "Epoch 421: avg loss training: 2.8163,... Gradient Norm: 0.2125\n",
      "Epoch 422: avg loss training: 2.8163,... Gradient Norm: 0.1912\n",
      "Epoch 423: avg loss training: 2.8163,... Gradient Norm: 0.2082\n",
      "Epoch 424: avg loss training: 2.8163,... Gradient Norm: 0.1840\n",
      "Epoch 425: avg loss training: 2.8163,... Gradient Norm: 0.3348\n",
      "Epoch 426: avg loss training: 2.8163,... Gradient Norm: 0.2098\n",
      "Epoch 427: avg loss training: 2.8163,... Gradient Norm: 0.2519\n",
      "Epoch 428: avg loss training: 2.8163,... Gradient Norm: 0.1908\n",
      "Epoch 429: avg loss training: 2.8163,... Gradient Norm: 0.1313\n",
      "Epoch 430: avg loss training: 2.8163,... Gradient Norm: 0.3049\n",
      "Epoch 431: avg loss training: 2.8162,... Gradient Norm: 0.1615\n",
      "Epoch 432: avg loss training: 2.8162,... Gradient Norm: 0.1433\n",
      "Epoch 433: avg loss training: 2.8162,... Gradient Norm: 0.2743\n",
      "Epoch 434: avg loss training: 2.8162,... Gradient Norm: 0.1207\n",
      "Epoch 435: avg loss training: 2.8162,... Gradient Norm: 0.3127\n",
      "Epoch 436: avg loss training: 2.8162,... Gradient Norm: 0.1412\n",
      "Epoch 437: avg loss training: 2.8162,... Gradient Norm: 0.1515\n",
      "Epoch 438: avg loss training: 2.8162,... Gradient Norm: 0.0912\n",
      "Epoch 439: avg loss training: 2.8162,... Gradient Norm: 0.2615\n",
      "Epoch 440: avg loss training: 2.8162,... Gradient Norm: 0.1397\n",
      "Epoch 441: avg loss training: 2.8162,... Gradient Norm: 0.1479\n",
      "Epoch 442: avg loss training: 2.8162,... Gradient Norm: 0.1172\n",
      "Epoch 443: avg loss training: 2.8161,... Gradient Norm: 0.2379\n",
      "Epoch 444: avg loss training: 2.8161,... Gradient Norm: 0.1873\n",
      "Epoch 445: avg loss training: 2.8161,... Gradient Norm: 0.1515\n",
      "Epoch 446: avg loss training: 2.8161,... Gradient Norm: 0.3512\n",
      "Epoch 447: avg loss training: 2.8161,... Gradient Norm: 0.2404\n",
      "Epoch 448: avg loss training: 2.8161,... Gradient Norm: 0.2684\n",
      "Epoch 449: avg loss training: 2.8161,... Gradient Norm: 0.1430\n",
      "Epoch 450: avg loss training: 2.8161,... Gradient Norm: 0.4001\n",
      "Epoch 451: avg loss training: 2.8161,... Gradient Norm: 0.2440\n",
      "Epoch 452: avg loss training: 2.8161,... Gradient Norm: 0.2289\n",
      "Epoch 453: avg loss training: 2.8161,... Gradient Norm: 0.2896\n",
      "Epoch 454: avg loss training: 2.8161,... Gradient Norm: 0.2137\n",
      "Epoch 455: avg loss training: 2.8161,... Gradient Norm: 0.2896\n",
      "Epoch 456: avg loss training: 2.8161,... Gradient Norm: 0.2712\n",
      "Epoch 457: avg loss training: 2.8161,... Gradient Norm: 0.3974\n",
      "Epoch 458: avg loss training: 2.8161,... Gradient Norm: 0.3134\n",
      "Epoch 459: avg loss training: 2.8160,... Gradient Norm: 0.2284\n",
      "Epoch 460: avg loss training: 2.8160,... Gradient Norm: 0.3264\n",
      "Epoch 461: avg loss training: 2.8160,... Gradient Norm: 0.2210\n",
      "Epoch 462: avg loss training: 2.8160,... Gradient Norm: 0.3092\n",
      "Epoch 463: avg loss training: 2.8160,... Gradient Norm: 0.1363\n",
      "Epoch 464: avg loss training: 2.8160,... Gradient Norm: 0.1828\n",
      "Epoch 465: avg loss training: 2.8160,... Gradient Norm: 0.1254\n",
      "Epoch 466: avg loss training: 2.8160,... Gradient Norm: 0.2775\n",
      "Epoch 467: avg loss training: 2.8160,... Gradient Norm: 0.2374\n",
      "Epoch 468: avg loss training: 2.8160,... Gradient Norm: 0.2243\n",
      "Epoch 469: avg loss training: 2.8160,... Gradient Norm: 0.2679\n",
      "Epoch 470: avg loss training: 2.8160,... Gradient Norm: 0.2610\n",
      "Epoch 471: avg loss training: 2.8160,... Gradient Norm: 0.2599\n",
      "Epoch 472: avg loss training: 2.8160,... Gradient Norm: 0.2393\n",
      "Epoch 473: avg loss training: 2.8160,... Gradient Norm: 0.2766\n",
      "Epoch 474: avg loss training: 2.8160,... Gradient Norm: 0.2983\n",
      "Epoch 475: avg loss training: 2.8159,... Gradient Norm: 0.2078\n",
      "Epoch 476: avg loss training: 2.8159,... Gradient Norm: 0.2603\n",
      "Epoch 477: avg loss training: 2.8159,... Gradient Norm: 0.1088\n",
      "Epoch 478: avg loss training: 2.8159,... Gradient Norm: 0.2715\n",
      "Epoch 479: avg loss training: 2.8159,... Gradient Norm: 0.2260\n",
      "Epoch 480: avg loss training: 2.8159,... Gradient Norm: 0.3339\n",
      "Epoch 481: avg loss training: 2.8159,... Gradient Norm: 0.2940\n",
      "Epoch 482: avg loss training: 2.8159,... Gradient Norm: 0.1964\n",
      "Epoch 483: avg loss training: 2.8159,... Gradient Norm: 0.3192\n",
      "Epoch 484: avg loss training: 2.8159,... Gradient Norm: 0.2612\n",
      "Epoch 485: avg loss training: 2.8159,... Gradient Norm: 0.3266\n",
      "Epoch 486: avg loss training: 2.8159,... Gradient Norm: 0.3259\n",
      "Epoch 487: avg loss training: 2.8159,... Gradient Norm: 0.1349\n",
      "Epoch 488: avg loss training: 2.8159,... Gradient Norm: 0.3445\n",
      "Epoch 489: avg loss training: 2.8159,... Gradient Norm: 0.3104\n",
      "Epoch 490: avg loss training: 2.8159,... Gradient Norm: 0.2724\n",
      "Epoch 491: avg loss training: 2.8159,... Gradient Norm: 0.3763\n",
      "Epoch 492: avg loss training: 2.8159,... Gradient Norm: 0.1353\n",
      "Epoch 493: avg loss training: 2.8159,... Gradient Norm: 0.3259\n",
      "Epoch 494: avg loss training: 2.8159,... Gradient Norm: 0.3337\n",
      "Epoch 495: avg loss training: 2.8158,... Gradient Norm: 0.1203\n",
      "Epoch 496: avg loss training: 2.8158,... Gradient Norm: 0.2824\n",
      "Epoch 497: avg loss training: 2.8158,... Gradient Norm: 0.1421\n",
      "Epoch 498: avg loss training: 2.8158,... Gradient Norm: 0.2205\n",
      "Epoch 499: avg loss training: 2.8158,... Gradient Norm: 0.2431\n",
      "Epoch 500: avg loss training: 2.8158,... Gradient Norm: 0.1373\n",
      "Epoch 501: avg loss training: 2.8158,... Gradient Norm: 0.2652\n",
      "Epoch 502: avg loss training: 2.8158,... Gradient Norm: 0.1331\n",
      "Epoch 503: avg loss training: 2.8158,... Gradient Norm: 0.2238\n",
      "Epoch 504: avg loss training: 2.8158,... Gradient Norm: 0.2183\n",
      "Epoch 505: avg loss training: 2.8158,... Gradient Norm: 0.1353\n",
      "Epoch 506: avg loss training: 2.8158,... Gradient Norm: 0.2440\n",
      "Epoch 507: avg loss training: 2.8158,... Gradient Norm: 0.1247\n",
      "Epoch 508: avg loss training: 2.8158,... Gradient Norm: 0.2606\n",
      "Epoch 509: avg loss training: 2.8158,... Gradient Norm: 0.2222\n",
      "Epoch 510: avg loss training: 2.8158,... Gradient Norm: 0.2681\n",
      "Epoch 511: avg loss training: 2.8158,... Gradient Norm: 0.2363\n",
      "Epoch 512: avg loss training: 2.8158,... Gradient Norm: 0.2388\n",
      "Epoch 513: avg loss training: 2.8158,... Gradient Norm: 0.1663\n",
      "Epoch 514: avg loss training: 2.8158,... Gradient Norm: 0.2491\n",
      "Epoch 515: avg loss training: 2.8158,... Gradient Norm: 0.3173\n",
      "Epoch 516: avg loss training: 2.8158,... Gradient Norm: 0.3049\n",
      "Epoch 517: avg loss training: 2.8158,... Gradient Norm: 0.2800\n",
      "Epoch 518: avg loss training: 2.8158,... Gradient Norm: 0.2877\n",
      "Epoch 519: avg loss training: 2.8157,... Gradient Norm: 0.1871\n",
      "Epoch 520: avg loss training: 2.8157,... Gradient Norm: 0.3164\n",
      "Epoch 521: avg loss training: 2.8157,... Gradient Norm: 0.3359\n",
      "Epoch 522: avg loss training: 2.8157,... Gradient Norm: 0.2653\n",
      "Epoch 523: avg loss training: 2.8157,... Gradient Norm: 0.2772\n",
      "Epoch 524: avg loss training: 2.8157,... Gradient Norm: 0.2959\n",
      "Epoch 525: avg loss training: 2.8157,... Gradient Norm: 0.2919\n",
      "Epoch 526: avg loss training: 2.8157,... Gradient Norm: 0.3127\n",
      "Epoch 527: avg loss training: 2.8157,... Gradient Norm: 0.2047\n",
      "Epoch 528: avg loss training: 2.8157,... Gradient Norm: 0.1903\n",
      "Epoch 529: avg loss training: 2.8157,... Gradient Norm: 0.3042\n",
      "Epoch 530: avg loss training: 2.8157,... Gradient Norm: 0.2816\n",
      "Epoch 531: avg loss training: 2.8157,... Gradient Norm: 0.3043\n",
      "Epoch 532: avg loss training: 2.8157,... Gradient Norm: 0.2725\n",
      "Epoch 533: avg loss training: 2.8157,... Gradient Norm: 0.2862\n",
      "Epoch 534: avg loss training: 2.8157,... Gradient Norm: 0.3241\n",
      "Epoch 535: avg loss training: 2.8157,... Gradient Norm: 0.2365\n",
      "Epoch 536: avg loss training: 2.8157,... Gradient Norm: 0.2474\n",
      "Epoch 537: avg loss training: 2.8157,... Gradient Norm: 0.2106\n",
      "Epoch 538: avg loss training: 2.8157,... Gradient Norm: 0.2284\n",
      "Epoch 539: avg loss training: 2.8157,... Gradient Norm: 0.3802\n",
      "Epoch 540: avg loss training: 2.8157,... Gradient Norm: 0.3000\n",
      "Epoch 541: avg loss training: 2.8157,... Gradient Norm: 0.2937\n",
      "Epoch 542: avg loss training: 2.8157,... Gradient Norm: 0.2974\n",
      "Epoch 543: avg loss training: 2.8157,... Gradient Norm: 0.1836\n",
      "Epoch 544: avg loss training: 2.8157,... Gradient Norm: 0.4083\n",
      "Epoch 545: avg loss training: 2.8157,... Gradient Norm: 0.3411\n",
      "Epoch 546: avg loss training: 2.8157,... Gradient Norm: 0.2847\n",
      "Epoch 547: avg loss training: 2.8157,... Gradient Norm: 0.3549\n",
      "Epoch 548: avg loss training: 2.8157,... Gradient Norm: 0.1840\n",
      "Epoch 549: avg loss training: 2.8157,... Gradient Norm: 0.3703\n",
      "Epoch 550: avg loss training: 2.8156,... Gradient Norm: 0.3656\n",
      "Epoch 551: avg loss training: 2.8156,... Gradient Norm: 0.1865\n",
      "Epoch 552: avg loss training: 2.8156,... Gradient Norm: 0.2097\n",
      "Epoch 553: avg loss training: 2.8156,... Gradient Norm: 0.1944\n",
      "Epoch 554: avg loss training: 2.8156,... Gradient Norm: 0.2112\n",
      "Epoch 555: avg loss training: 2.8156,... Gradient Norm: 0.2835\n",
      "Epoch 556: avg loss training: 2.8156,... Gradient Norm: 0.1744\n",
      "Epoch 557: avg loss training: 2.8156,... Gradient Norm: 0.1810\n",
      "Epoch 558: avg loss training: 2.8156,... Gradient Norm: 0.2219\n",
      "Epoch 559: avg loss training: 2.8156,... Gradient Norm: 0.2686\n",
      "Epoch 560: avg loss training: 2.8156,... Gradient Norm: 0.2883\n",
      "Epoch 561: avg loss training: 2.8156,... Gradient Norm: 0.2379\n",
      "Epoch 562: avg loss training: 2.8156,... Gradient Norm: 0.3262\n",
      "Epoch 563: avg loss training: 2.8156,... Gradient Norm: 0.3047\n",
      "Epoch 564: avg loss training: 2.8156,... Gradient Norm: 0.2649\n",
      "Epoch 565: avg loss training: 2.8156,... Gradient Norm: 0.3456\n",
      "Epoch 566: avg loss training: 2.8156,... Gradient Norm: 0.2625\n",
      "Epoch 567: avg loss training: 2.8156,... Gradient Norm: 0.3187\n",
      "Epoch 568: avg loss training: 2.8156,... Gradient Norm: 0.3295\n",
      "Epoch 569: avg loss training: 2.8156,... Gradient Norm: 0.2434\n",
      "Epoch 570: avg loss training: 2.8156,... Gradient Norm: 0.2701\n",
      "Epoch 571: avg loss training: 2.8156,... Gradient Norm: 0.2925\n",
      "Epoch 572: avg loss training: 2.8156,... Gradient Norm: 0.1625\n",
      "Epoch 573: avg loss training: 2.8156,... Gradient Norm: 0.2836\n",
      "Epoch 574: avg loss training: 2.8156,... Gradient Norm: 0.2853\n",
      "Epoch 575: avg loss training: 2.8156,... Gradient Norm: 0.1828\n",
      "Epoch 576: avg loss training: 2.8156,... Gradient Norm: 0.2206\n",
      "Epoch 577: avg loss training: 2.8156,... Gradient Norm: 0.2098\n",
      "Epoch 578: avg loss training: 2.8156,... Gradient Norm: 0.1319\n",
      "Epoch 579: avg loss training: 2.8156,... Gradient Norm: 0.2740\n",
      "Epoch 580: avg loss training: 2.8156,... Gradient Norm: 0.2546\n",
      "Epoch 581: avg loss training: 2.8156,... Gradient Norm: 0.2687\n",
      "Epoch 582: avg loss training: 2.8156,... Gradient Norm: 0.3942\n",
      "Epoch 583: avg loss training: 2.8156,... Gradient Norm: 0.1433\n",
      "Epoch 584: avg loss training: 2.8156,... Gradient Norm: 0.3440\n",
      "Epoch 585: avg loss training: 2.8156,... Gradient Norm: 0.3335\n",
      "Epoch 586: avg loss training: 2.8156,... Gradient Norm: 0.1647\n",
      "Epoch 587: avg loss training: 2.8156,... Gradient Norm: 0.3355\n",
      "Epoch 588: avg loss training: 2.8156,... Gradient Norm: 0.1740\n",
      "Epoch 589: avg loss training: 2.8155,... Gradient Norm: 0.2854\n",
      "Epoch 590: avg loss training: 2.8155,... Gradient Norm: 0.1596\n",
      "Epoch 591: avg loss training: 2.8155,... Gradient Norm: 0.1360\n",
      "Epoch 592: avg loss training: 2.8155,... Gradient Norm: 0.3055\n",
      "Epoch 593: avg loss training: 2.8155,... Gradient Norm: 0.2632\n",
      "Epoch 594: avg loss training: 2.8155,... Gradient Norm: 0.2138\n",
      "Epoch 595: avg loss training: 2.8155,... Gradient Norm: 0.1899\n",
      "Epoch 596: avg loss training: 2.8155,... Gradient Norm: 0.2696\n",
      "Epoch 597: avg loss training: 2.8155,... Gradient Norm: 0.2360\n",
      "Epoch 598: avg loss training: 2.8155,... Gradient Norm: 0.2629\n",
      "Epoch 599: avg loss training: 2.8155,... Gradient Norm: 0.2033\n",
      "Epoch 600: avg loss training: 2.8155,... Gradient Norm: 0.1734\n",
      "Epoch 601: avg loss training: 2.8155,... Gradient Norm: 0.2885\n",
      "Epoch 602: avg loss training: 2.8155,... Gradient Norm: 0.3240\n",
      "Epoch 603: avg loss training: 2.8155,... Gradient Norm: 0.2857\n",
      "Epoch 604: avg loss training: 2.8155,... Gradient Norm: 0.2794\n",
      "Epoch 605: avg loss training: 2.8155,... Gradient Norm: 0.1734\n",
      "Epoch 606: avg loss training: 2.8155,... Gradient Norm: 0.2977\n",
      "Epoch 607: avg loss training: 2.8155,... Gradient Norm: 0.2878\n",
      "Epoch 608: avg loss training: 2.8155,... Gradient Norm: 0.2488\n",
      "Epoch 609: avg loss training: 2.8155,... Gradient Norm: 0.3143\n",
      "Epoch 610: avg loss training: 2.8155,... Gradient Norm: 0.1725\n",
      "Epoch 611: avg loss training: 2.8155,... Gradient Norm: 0.2797\n",
      "Epoch 612: avg loss training: 2.8155,... Gradient Norm: 0.2874\n",
      "Epoch 613: avg loss training: 2.8155,... Gradient Norm: 0.1549\n",
      "Epoch 614: avg loss training: 2.8155,... Gradient Norm: 0.1733\n",
      "Epoch 615: avg loss training: 2.8155,... Gradient Norm: 0.2543\n",
      "Epoch 616: avg loss training: 2.8155,... Gradient Norm: 0.2589\n",
      "Epoch 617: avg loss training: 2.8155,... Gradient Norm: 0.2641\n",
      "Epoch 618: avg loss training: 2.8155,... Gradient Norm: 0.2788\n",
      "Epoch 619: avg loss training: 2.8155,... Gradient Norm: 0.1465\n",
      "Epoch 620: avg loss training: 2.8155,... Gradient Norm: 0.2443\n",
      "Epoch 621: avg loss training: 2.8155,... Gradient Norm: 0.0905\n",
      "Epoch 622: avg loss training: 2.8155,... Gradient Norm: 0.1974\n",
      "Epoch 623: avg loss training: 2.8155,... Gradient Norm: 0.0863\n",
      "Epoch 624: avg loss training: 2.8155,... Gradient Norm: 0.1887\n",
      "Epoch 625: avg loss training: 2.8155,... Gradient Norm: 0.1551\n",
      "Epoch 626: avg loss training: 2.8155,... Gradient Norm: 0.2523\n",
      "Epoch 627: avg loss training: 2.8155,... Gradient Norm: 0.1128\n",
      "Epoch 628: avg loss training: 2.8155,... Gradient Norm: 0.2566\n",
      "Epoch 629: avg loss training: 2.8155,... Gradient Norm: 0.0665\n",
      "Epoch 630: avg loss training: 2.8155,... Gradient Norm: 0.2953\n",
      "Epoch 631: avg loss training: 2.8155,... Gradient Norm: 0.2486\n",
      "Epoch 632: avg loss training: 2.8155,... Gradient Norm: 0.3216\n",
      "Epoch 633: avg loss training: 2.8155,... Gradient Norm: 0.2498\n",
      "Epoch 634: avg loss training: 2.8155,... Gradient Norm: 0.1749\n",
      "Epoch 635: avg loss training: 2.8155,... Gradient Norm: 0.3453\n",
      "Epoch 636: avg loss training: 2.8155,... Gradient Norm: 0.1864\n",
      "Epoch 637: avg loss training: 2.8155,... Gradient Norm: 0.1806\n",
      "Epoch 638: avg loss training: 2.8155,... Gradient Norm: 0.2072\n",
      "Epoch 639: avg loss training: 2.8155,... Gradient Norm: 0.3345\n",
      "Epoch 640: avg loss training: 2.8155,... Gradient Norm: 0.2109\n",
      "Epoch 641: avg loss training: 2.8155,... Gradient Norm: 0.2166\n",
      "Epoch 642: avg loss training: 2.8155,... Gradient Norm: 0.1905\n",
      "Epoch 643: avg loss training: 2.8155,... Gradient Norm: 0.2904\n",
      "Epoch 644: avg loss training: 2.8155,... Gradient Norm: 0.1395\n",
      "Epoch 645: avg loss training: 2.8155,... Gradient Norm: 0.3555\n",
      "Epoch 646: avg loss training: 2.8155,... Gradient Norm: 0.1170\n",
      "Epoch 647: avg loss training: 2.8155,... Gradient Norm: 0.1826\n",
      "Epoch 648: avg loss training: 2.8155,... Gradient Norm: 0.1560\n",
      "Epoch 649: avg loss training: 2.8155,... Gradient Norm: 0.2511\n",
      "Epoch 650: avg loss training: 2.8154,... Gradient Norm: 0.1041\n",
      "Epoch 651: avg loss training: 2.8154,... Gradient Norm: 0.1880\n",
      "Epoch 652: avg loss training: 2.8154,... Gradient Norm: 0.2336\n",
      "Epoch 653: avg loss training: 2.8154,... Gradient Norm: 0.1473\n",
      "Epoch 654: avg loss training: 2.8154,... Gradient Norm: 0.1781\n",
      "Epoch 655: avg loss training: 2.8154,... Gradient Norm: 0.1132\n",
      "Epoch 656: avg loss training: 2.8154,... Gradient Norm: 0.1817\n",
      "Epoch 657: avg loss training: 2.8154,... Gradient Norm: 0.2764\n",
      "Epoch 658: avg loss training: 2.8154,... Gradient Norm: 0.2912\n",
      "Epoch 659: avg loss training: 2.8154,... Gradient Norm: 0.1746\n",
      "Epoch 660: avg loss training: 2.8154,... Gradient Norm: 0.2232\n",
      "Epoch 661: avg loss training: 2.8154,... Gradient Norm: 0.1652\n",
      "Epoch 662: avg loss training: 2.8154,... Gradient Norm: 0.1679\n",
      "Epoch 663: avg loss training: 2.8154,... Gradient Norm: 0.2375\n",
      "Epoch 664: avg loss training: 2.8154,... Gradient Norm: 0.1367\n",
      "Epoch 665: avg loss training: 2.8154,... Gradient Norm: 0.3046\n",
      "Epoch 666: avg loss training: 2.8154,... Gradient Norm: 0.1268\n",
      "Epoch 667: avg loss training: 2.8154,... Gradient Norm: 0.2719\n",
      "Epoch 668: avg loss training: 2.8154,... Gradient Norm: 0.1147\n",
      "Epoch 669: avg loss training: 2.8154,... Gradient Norm: 0.1167\n",
      "Epoch 670: avg loss training: 2.8154,... Gradient Norm: 0.1140\n",
      "Epoch 671: avg loss training: 2.8154,... Gradient Norm: 0.2451\n",
      "Epoch 672: avg loss training: 2.8154,... Gradient Norm: 0.1315\n",
      "Epoch 673: avg loss training: 2.8154,... Gradient Norm: 0.1769\n",
      "Epoch 674: avg loss training: 2.8154,... Gradient Norm: 0.1651\n",
      "Epoch 675: avg loss training: 2.8154,... Gradient Norm: 0.1990\n",
      "Epoch 676: avg loss training: 2.8154,... Gradient Norm: 0.2203\n",
      "Epoch 677: avg loss training: 2.8154,... Gradient Norm: 0.1571\n",
      "Epoch 678: avg loss training: 2.8154,... Gradient Norm: 0.2441\n",
      "Epoch 679: avg loss training: 2.8154,... Gradient Norm: 0.1737\n",
      "Epoch 680: avg loss training: 2.8154,... Gradient Norm: 0.2616\n",
      "Epoch 681: avg loss training: 2.8154,... Gradient Norm: 0.1783\n",
      "Epoch 682: avg loss training: 2.8154,... Gradient Norm: 0.2107\n",
      "Epoch 683: avg loss training: 2.8154,... Gradient Norm: 0.1720\n",
      "Epoch 684: avg loss training: 2.8154,... Gradient Norm: 0.2769\n",
      "Epoch 685: avg loss training: 2.8154,... Gradient Norm: 0.2601\n",
      "Epoch 686: avg loss training: 2.8154,... Gradient Norm: 0.3083\n",
      "Epoch 687: avg loss training: 2.8154,... Gradient Norm: 0.3558\n",
      "Epoch 688: avg loss training: 2.8154,... Gradient Norm: 0.3178\n",
      "Epoch 689: avg loss training: 2.8154,... Gradient Norm: 0.2093\n",
      "Epoch 690: avg loss training: 2.8154,... Gradient Norm: 0.3407\n",
      "Epoch 691: avg loss training: 2.8154,... Gradient Norm: 0.3329\n",
      "Epoch 692: avg loss training: 2.8154,... Gradient Norm: 0.3078\n",
      "Epoch 693: avg loss training: 2.8154,... Gradient Norm: 0.3536\n",
      "Epoch 694: avg loss training: 2.8154,... Gradient Norm: 0.3140\n",
      "Epoch 695: avg loss training: 2.8154,... Gradient Norm: 0.1806\n",
      "Epoch 696: avg loss training: 2.8154,... Gradient Norm: 0.3134\n",
      "Epoch 697: avg loss training: 2.8154,... Gradient Norm: 0.3151\n",
      "Epoch 698: avg loss training: 2.8154,... Gradient Norm: 0.1671\n",
      "Epoch 699: avg loss training: 2.8154,... Gradient Norm: 0.2659\n",
      "Epoch 700: avg loss training: 2.8154,... Gradient Norm: 0.3361\n",
      "Epoch 701: avg loss training: 2.8154,... Gradient Norm: 0.2310\n",
      "Epoch 702: avg loss training: 2.8154,... Gradient Norm: 0.1774\n",
      "Epoch 703: avg loss training: 2.8154,... Gradient Norm: 0.2785\n",
      "Epoch 704: avg loss training: 2.8154,... Gradient Norm: 0.2515\n",
      "Epoch 705: avg loss training: 2.8154,... Gradient Norm: 0.2510\n",
      "Epoch 706: avg loss training: 2.8154,... Gradient Norm: 0.2289\n",
      "Epoch 707: avg loss training: 2.8154,... Gradient Norm: 0.2890\n",
      "Epoch 708: avg loss training: 2.8154,... Gradient Norm: 0.1991\n",
      "Epoch 709: avg loss training: 2.8154,... Gradient Norm: 0.1899\n",
      "Epoch 710: avg loss training: 2.8154,... Gradient Norm: 0.2339\n",
      "Epoch 711: avg loss training: 2.8154,... Gradient Norm: 0.2394\n",
      "Epoch 712: avg loss training: 2.8154,... Gradient Norm: 0.2563\n",
      "Epoch 713: avg loss training: 2.8154,... Gradient Norm: 0.2499\n",
      "Epoch 714: avg loss training: 2.8154,... Gradient Norm: 0.3233\n",
      "Epoch 715: avg loss training: 2.8154,... Gradient Norm: 0.2944\n",
      "Epoch 716: avg loss training: 2.8154,... Gradient Norm: 0.2696\n",
      "Epoch 717: avg loss training: 2.8154,... Gradient Norm: 0.2712\n",
      "Epoch 718: avg loss training: 2.8154,... Gradient Norm: 0.1790\n",
      "Epoch 719: avg loss training: 2.8154,... Gradient Norm: 0.2197\n",
      "Epoch 720: avg loss training: 2.8154,... Gradient Norm: 0.2168\n",
      "Epoch 721: avg loss training: 2.8154,... Gradient Norm: 0.3226\n",
      "Epoch 722: avg loss training: 2.8154,... Gradient Norm: 0.1418\n",
      "Epoch 723: avg loss training: 2.8154,... Gradient Norm: 0.2117\n",
      "Epoch 724: avg loss training: 2.8154,... Gradient Norm: 0.2478\n",
      "Epoch 725: avg loss training: 2.8154,... Gradient Norm: 0.2704\n",
      "Epoch 726: avg loss training: 2.8154,... Gradient Norm: 0.1298\n",
      "Epoch 727: avg loss training: 2.8154,... Gradient Norm: 0.3302\n",
      "Epoch 728: avg loss training: 2.8154,... Gradient Norm: 0.2471\n",
      "Epoch 729: avg loss training: 2.8154,... Gradient Norm: 0.1368\n",
      "Epoch 730: avg loss training: 2.8154,... Gradient Norm: 0.2467\n",
      "Epoch 731: avg loss training: 2.8154,... Gradient Norm: 0.2289\n",
      "Epoch 732: avg loss training: 2.8154,... Gradient Norm: 0.1733\n",
      "Epoch 733: avg loss training: 2.8154,... Gradient Norm: 0.2613\n",
      "Epoch 734: avg loss training: 2.8154,... Gradient Norm: 0.1111\n",
      "Epoch 735: avg loss training: 2.8154,... Gradient Norm: 0.1477\n",
      "Epoch 736: avg loss training: 2.8154,... Gradient Norm: 0.2688\n",
      "Epoch 737: avg loss training: 2.8154,... Gradient Norm: 0.2372\n",
      "Epoch 738: avg loss training: 2.8154,... Gradient Norm: 0.1697\n",
      "Epoch 739: avg loss training: 2.8154,... Gradient Norm: 0.2841\n",
      "Epoch 740: avg loss training: 2.8154,... Gradient Norm: 0.1564\n",
      "Epoch 741: avg loss training: 2.8154,... Gradient Norm: 0.1410\n",
      "Epoch 742: avg loss training: 2.8154,... Gradient Norm: 0.2528\n",
      "Epoch 743: avg loss training: 2.8154,... Gradient Norm: 0.1358\n",
      "Epoch 744: avg loss training: 2.8154,... Gradient Norm: 0.1242\n",
      "Epoch 745: avg loss training: 2.8154,... Gradient Norm: 0.2211\n",
      "Epoch 746: avg loss training: 2.8154,... Gradient Norm: 0.2353\n",
      "Epoch 747: avg loss training: 2.8154,... Gradient Norm: 0.2409\n",
      "Epoch 748: avg loss training: 2.8154,... Gradient Norm: 0.2238\n",
      "Epoch 749: avg loss training: 2.8154,... Gradient Norm: 0.1257\n",
      "Epoch 750: avg loss training: 2.8154,... Gradient Norm: 0.1530\n",
      "Epoch 751: avg loss training: 2.8154,... Gradient Norm: 0.1702\n",
      "Epoch 752: avg loss training: 2.8154,... Gradient Norm: 0.1567\n",
      "Epoch 753: avg loss training: 2.8154,... Gradient Norm: 0.1369\n",
      "Epoch 754: avg loss training: 2.8154,... Gradient Norm: 0.1965\n",
      "Epoch 755: avg loss training: 2.8154,... Gradient Norm: 0.1273\n",
      "Epoch 756: avg loss training: 2.8154,... Gradient Norm: 0.2240\n",
      "Epoch 757: avg loss training: 2.8154,... Gradient Norm: 0.1309\n",
      "Epoch 758: avg loss training: 2.8154,... Gradient Norm: 0.1557\n",
      "Epoch 759: avg loss training: 2.8154,... Gradient Norm: 0.1144\n",
      "Epoch 760: avg loss training: 2.8154,... Gradient Norm: 0.0967\n",
      "Epoch 761: avg loss training: 2.8154,... Gradient Norm: 0.2086\n",
      "Epoch 762: avg loss training: 2.8154,... Gradient Norm: 0.2601\n",
      "Epoch 763: avg loss training: 2.8154,... Gradient Norm: 0.1439\n",
      "Epoch 764: avg loss training: 2.8154,... Gradient Norm: 0.2226\n",
      "Epoch 765: avg loss training: 2.8154,... Gradient Norm: 0.1563\n",
      "Epoch 766: avg loss training: 2.8154,... Gradient Norm: 0.1835\n",
      "Epoch 767: avg loss training: 2.8154,... Gradient Norm: 0.1270\n",
      "Epoch 768: avg loss training: 2.8154,... Gradient Norm: 0.2524\n",
      "Epoch 769: avg loss training: 2.8154,... Gradient Norm: 0.1488\n",
      "Epoch 770: avg loss training: 2.8154,... Gradient Norm: 0.3128\n",
      "Epoch 771: avg loss training: 2.8154,... Gradient Norm: 0.2014\n",
      "Epoch 772: avg loss training: 2.8154,... Gradient Norm: 0.1427\n",
      "Epoch 773: avg loss training: 2.8154,... Gradient Norm: 0.1601\n",
      "Epoch 774: avg loss training: 2.8154,... Gradient Norm: 0.1797\n",
      "Epoch 775: avg loss training: 2.8154,... Gradient Norm: 0.2379\n",
      "Epoch 776: avg loss training: 2.8153,... Gradient Norm: 0.2210\n",
      "Epoch 777: avg loss training: 2.8153,... Gradient Norm: 0.3101\n",
      "Epoch 778: avg loss training: 2.8153,... Gradient Norm: 0.0838\n",
      "Epoch 779: avg loss training: 2.8153,... Gradient Norm: 0.1567\n",
      "Epoch 780: avg loss training: 2.8153,... Gradient Norm: 0.1389\n",
      "Epoch 781: avg loss training: 2.8153,... Gradient Norm: 0.2769\n",
      "Epoch 782: avg loss training: 2.8153,... Gradient Norm: 0.1955\n",
      "Epoch 783: avg loss training: 2.8153,... Gradient Norm: 0.2155\n",
      "Epoch 784: avg loss training: 2.8153,... Gradient Norm: 0.1747\n",
      "Epoch 785: avg loss training: 2.8153,... Gradient Norm: 0.2291\n",
      "Epoch 786: avg loss training: 2.8153,... Gradient Norm: 0.1743\n",
      "Epoch 787: avg loss training: 2.8153,... Gradient Norm: 0.1708\n",
      "Epoch 788: avg loss training: 2.8153,... Gradient Norm: 0.1540\n",
      "Epoch 789: avg loss training: 2.8153,... Gradient Norm: 0.2499\n",
      "Epoch 790: avg loss training: 2.8153,... Gradient Norm: 0.2417\n",
      "Epoch 791: avg loss training: 2.8153,... Gradient Norm: 0.2653\n",
      "Epoch 792: avg loss training: 2.8153,... Gradient Norm: 0.1818\n",
      "Epoch 793: avg loss training: 2.8153,... Gradient Norm: 0.2374\n",
      "Epoch 794: avg loss training: 2.8153,... Gradient Norm: 0.2010\n",
      "Epoch 795: avg loss training: 2.8153,... Gradient Norm: 0.2015\n",
      "Epoch 796: avg loss training: 2.8153,... Gradient Norm: 0.2283\n",
      "Epoch 797: avg loss training: 2.8153,... Gradient Norm: 0.1349\n",
      "Epoch 798: avg loss training: 2.8153,... Gradient Norm: 0.2124\n",
      "Epoch 799: avg loss training: 2.8153,... Gradient Norm: 0.2001\n",
      "Epoch 800: avg loss training: 2.8153,... Gradient Norm: 0.2877\n",
      "Epoch 801: avg loss training: 2.8153,... Gradient Norm: 0.1748\n",
      "Epoch 802: avg loss training: 2.8153,... Gradient Norm: 0.1709\n",
      "Epoch 803: avg loss training: 2.8153,... Gradient Norm: 0.1962\n",
      "Epoch 804: avg loss training: 2.8153,... Gradient Norm: 0.1506\n",
      "Epoch 805: avg loss training: 2.8153,... Gradient Norm: 0.1933\n",
      "Epoch 806: avg loss training: 2.8153,... Gradient Norm: 0.1451\n",
      "Epoch 807: avg loss training: 2.8153,... Gradient Norm: 0.1644\n",
      "Epoch 808: avg loss training: 2.8153,... Gradient Norm: 0.1262\n",
      "Epoch 809: avg loss training: 2.8153,... Gradient Norm: 0.2135\n",
      "Epoch 810: avg loss training: 2.8153,... Gradient Norm: 0.1103\n",
      "Epoch 811: avg loss training: 2.8153,... Gradient Norm: 0.2651\n",
      "Epoch 812: avg loss training: 2.8153,... Gradient Norm: 0.1104\n",
      "Epoch 813: avg loss training: 2.8153,... Gradient Norm: 0.2277\n",
      "Epoch 814: avg loss training: 2.8153,... Gradient Norm: 0.1862\n",
      "Epoch 815: avg loss training: 2.8153,... Gradient Norm: 0.2132\n",
      "Epoch 816: avg loss training: 2.8153,... Gradient Norm: 0.2427\n",
      "Epoch 817: avg loss training: 2.8153,... Gradient Norm: 0.1615\n",
      "Epoch 818: avg loss training: 2.8153,... Gradient Norm: 0.2418\n",
      "Epoch 819: avg loss training: 2.8153,... Gradient Norm: 0.2023\n",
      "Epoch 820: avg loss training: 2.8153,... Gradient Norm: 0.2323\n",
      "Epoch 821: avg loss training: 2.8153,... Gradient Norm: 0.2136\n",
      "Epoch 822: avg loss training: 2.8153,... Gradient Norm: 0.1042\n",
      "Epoch 823: avg loss training: 2.8153,... Gradient Norm: 0.1597\n",
      "Epoch 824: avg loss training: 2.8153,... Gradient Norm: 0.1239\n",
      "Epoch 825: avg loss training: 2.8153,... Gradient Norm: 0.1377\n",
      "Epoch 826: avg loss training: 2.8153,... Gradient Norm: 0.1606\n",
      "Epoch 827: avg loss training: 2.8153,... Gradient Norm: 0.1577\n",
      "Epoch 828: avg loss training: 2.8153,... Gradient Norm: 0.1172\n",
      "Epoch 829: avg loss training: 2.8153,... Gradient Norm: 0.1736\n",
      "Epoch 830: avg loss training: 2.8153,... Gradient Norm: 0.2139\n",
      "Epoch 831: avg loss training: 2.8153,... Gradient Norm: 0.1332\n",
      "Epoch 832: avg loss training: 2.8153,... Gradient Norm: 0.2620\n",
      "Epoch 833: avg loss training: 2.8153,... Gradient Norm: 0.1917\n",
      "Epoch 834: avg loss training: 2.8153,... Gradient Norm: 0.1962\n",
      "Epoch 835: avg loss training: 2.8153,... Gradient Norm: 0.1877\n",
      "Epoch 836: avg loss training: 2.8153,... Gradient Norm: 0.1848\n",
      "Epoch 837: avg loss training: 2.8153,... Gradient Norm: 0.2417\n",
      "Epoch 838: avg loss training: 2.8153,... Gradient Norm: 0.2311\n",
      "Epoch 839: avg loss training: 2.8153,... Gradient Norm: 0.2060\n",
      "Epoch 840: avg loss training: 2.8153,... Gradient Norm: 0.2851\n",
      "Epoch 841: avg loss training: 2.8153,... Gradient Norm: 0.2995\n",
      "Epoch 842: avg loss training: 2.8153,... Gradient Norm: 0.1480\n",
      "Epoch 843: avg loss training: 2.8153,... Gradient Norm: 0.2705\n",
      "Epoch 844: avg loss training: 2.8153,... Gradient Norm: 0.2430\n",
      "Epoch 845: avg loss training: 2.8153,... Gradient Norm: 0.2190\n",
      "Epoch 846: avg loss training: 2.8153,... Gradient Norm: 0.2035\n",
      "Epoch 847: avg loss training: 2.8153,... Gradient Norm: 0.3077\n",
      "Epoch 848: avg loss training: 2.8153,... Gradient Norm: 0.2520\n",
      "Epoch 849: avg loss training: 2.8153,... Gradient Norm: 0.2316\n",
      "Epoch 850: avg loss training: 2.8153,... Gradient Norm: 0.2779\n",
      "Epoch 851: avg loss training: 2.8153,... Gradient Norm: 0.2016\n",
      "Epoch 852: avg loss training: 2.8153,... Gradient Norm: 0.1127\n",
      "Epoch 853: avg loss training: 2.8153,... Gradient Norm: 0.2165\n",
      "Epoch 854: avg loss training: 2.8153,... Gradient Norm: 0.2174\n",
      "Epoch 855: avg loss training: 2.8153,... Gradient Norm: 0.0563\n",
      "Epoch 856: avg loss training: 2.8153,... Gradient Norm: 0.2258\n",
      "Epoch 857: avg loss training: 2.8153,... Gradient Norm: 0.2144\n",
      "Epoch 858: avg loss training: 2.8153,... Gradient Norm: 0.1691\n",
      "Epoch 859: avg loss training: 2.8153,... Gradient Norm: 0.1981\n",
      "Epoch 860: avg loss training: 2.8153,... Gradient Norm: 0.1482\n",
      "Epoch 861: avg loss training: 2.8153,... Gradient Norm: 0.2390\n",
      "Epoch 862: avg loss training: 2.8153,... Gradient Norm: 0.2428\n",
      "Epoch 863: avg loss training: 2.8153,... Gradient Norm: 0.1202\n",
      "Epoch 864: avg loss training: 2.8153,... Gradient Norm: 0.1980\n",
      "Epoch 865: avg loss training: 2.8153,... Gradient Norm: 0.2028\n",
      "Epoch 866: avg loss training: 2.8153,... Gradient Norm: 0.2156\n",
      "Epoch 867: avg loss training: 2.8153,... Gradient Norm: 0.2998\n",
      "Epoch 868: avg loss training: 2.8153,... Gradient Norm: 0.2376\n",
      "Epoch 869: avg loss training: 2.8153,... Gradient Norm: 0.2306\n",
      "Epoch 870: avg loss training: 2.8153,... Gradient Norm: 0.2871\n",
      "Epoch 871: avg loss training: 2.8153,... Gradient Norm: 0.1144\n",
      "Epoch 872: avg loss training: 2.8153,... Gradient Norm: 0.2440\n",
      "Epoch 873: avg loss training: 2.8153,... Gradient Norm: 0.2156\n",
      "Epoch 874: avg loss training: 2.8153,... Gradient Norm: 0.2238\n",
      "Epoch 875: avg loss training: 2.8153,... Gradient Norm: 0.2348\n",
      "Epoch 876: avg loss training: 2.8153,... Gradient Norm: 0.1113\n",
      "Epoch 877: avg loss training: 2.8153,... Gradient Norm: 0.2541\n",
      "Epoch 878: avg loss training: 2.8153,... Gradient Norm: 0.1869\n",
      "Epoch 879: avg loss training: 2.8153,... Gradient Norm: 0.1286\n",
      "Epoch 880: avg loss training: 2.8153,... Gradient Norm: 0.2746\n",
      "Epoch 881: avg loss training: 2.8153,... Gradient Norm: 0.1063\n",
      "Epoch 882: avg loss training: 2.8153,... Gradient Norm: 0.1805\n",
      "Epoch 883: avg loss training: 2.8153,... Gradient Norm: 0.2290\n",
      "Epoch 884: avg loss training: 2.8153,... Gradient Norm: 0.1837\n",
      "Epoch 885: avg loss training: 2.8153,... Gradient Norm: 0.2752\n",
      "Epoch 886: avg loss training: 2.8153,... Gradient Norm: 0.1670\n",
      "Epoch 887: avg loss training: 2.8153,... Gradient Norm: 0.1862\n",
      "Epoch 888: avg loss training: 2.8153,... Gradient Norm: 0.1367\n",
      "Epoch 889: avg loss training: 2.8153,... Gradient Norm: 0.1763\n",
      "Epoch 890: avg loss training: 2.8153,... Gradient Norm: 0.2032\n",
      "Epoch 891: avg loss training: 2.8153,... Gradient Norm: 0.1991\n",
      "Epoch 892: avg loss training: 2.8153,... Gradient Norm: 0.2339\n",
      "Epoch 893: avg loss training: 2.8153,... Gradient Norm: 0.2326\n",
      "Epoch 894: avg loss training: 2.8153,... Gradient Norm: 0.2090\n",
      "Epoch 895: avg loss training: 2.8153,... Gradient Norm: 0.2287\n",
      "Epoch 896: avg loss training: 2.8153,... Gradient Norm: 0.2286\n",
      "Epoch 897: avg loss training: 2.8153,... Gradient Norm: 0.1780\n",
      "Epoch 898: avg loss training: 2.8153,... Gradient Norm: 0.1955\n",
      "Epoch 899: avg loss training: 2.8153,... Gradient Norm: 0.1794\n",
      "Epoch 900: avg loss training: 2.8153,... Gradient Norm: 0.2663\n",
      "Epoch 901: avg loss training: 2.8153,... Gradient Norm: 0.2271\n",
      "Epoch 902: avg loss training: 2.8153,... Gradient Norm: 0.1653\n",
      "Epoch 903: avg loss training: 2.8153,... Gradient Norm: 0.1461\n",
      "Epoch 904: avg loss training: 2.8153,... Gradient Norm: 0.2761\n",
      "Epoch 905: avg loss training: 2.8153,... Gradient Norm: 0.2661\n",
      "Epoch 906: avg loss training: 2.8153,... Gradient Norm: 0.2479\n",
      "Epoch 907: avg loss training: 2.8153,... Gradient Norm: 0.2258\n",
      "Epoch 908: avg loss training: 2.8153,... Gradient Norm: 0.1714\n",
      "Epoch 909: avg loss training: 2.8153,... Gradient Norm: 0.2071\n",
      "Epoch 910: avg loss training: 2.8153,... Gradient Norm: 0.2334\n",
      "Epoch 911: avg loss training: 2.8153,... Gradient Norm: 0.2860\n",
      "Epoch 912: avg loss training: 2.8153,... Gradient Norm: 0.1491\n",
      "Epoch 913: avg loss training: 2.8153,... Gradient Norm: 0.2265\n",
      "Epoch 914: avg loss training: 2.8153,... Gradient Norm: 0.2261\n",
      "Epoch 915: avg loss training: 2.8153,... Gradient Norm: 0.1073\n",
      "Epoch 916: avg loss training: 2.8153,... Gradient Norm: 0.2184\n",
      "Epoch 917: avg loss training: 2.8153,... Gradient Norm: 0.2158\n",
      "Epoch 918: avg loss training: 2.8153,... Gradient Norm: 0.1441\n",
      "Epoch 919: avg loss training: 2.8153,... Gradient Norm: 0.3137\n",
      "Epoch 920: avg loss training: 2.8153,... Gradient Norm: 0.1864\n",
      "Epoch 921: avg loss training: 2.8153,... Gradient Norm: 0.1977\n",
      "Epoch 922: avg loss training: 2.8153,... Gradient Norm: 0.2777\n",
      "Epoch 923: avg loss training: 2.8153,... Gradient Norm: 0.2011\n",
      "Epoch 924: avg loss training: 2.8153,... Gradient Norm: 0.2991\n",
      "Epoch 925: avg loss training: 2.8153,... Gradient Norm: 0.2715\n",
      "Epoch 926: avg loss training: 2.8153,... Gradient Norm: 0.0767\n",
      "Epoch 927: avg loss training: 2.8153,... Gradient Norm: 0.2478\n",
      "Epoch 928: avg loss training: 2.8153,... Gradient Norm: 0.1397\n",
      "Epoch 929: avg loss training: 2.8153,... Gradient Norm: 0.1549\n",
      "Epoch 930: avg loss training: 2.8153,... Gradient Norm: 0.1758\n",
      "Epoch 931: avg loss training: 2.8153,... Gradient Norm: 0.1840\n",
      "Epoch 932: avg loss training: 2.8153,... Gradient Norm: 0.1114\n",
      "Epoch 933: avg loss training: 2.8153,... Gradient Norm: 0.2195\n",
      "Epoch 934: avg loss training: 2.8153,... Gradient Norm: 0.1476\n",
      "Epoch 935: avg loss training: 2.8153,... Gradient Norm: 0.2463\n",
      "Epoch 936: avg loss training: 2.8153,... Gradient Norm: 0.1768\n",
      "Epoch 937: avg loss training: 2.8153,... Gradient Norm: 0.2486\n",
      "Epoch 938: avg loss training: 2.8153,... Gradient Norm: 0.2469\n",
      "Epoch 939: avg loss training: 2.8153,... Gradient Norm: 0.2155\n",
      "Epoch 940: avg loss training: 2.8153,... Gradient Norm: 0.3129\n",
      "Epoch 941: avg loss training: 2.8153,... Gradient Norm: 0.1584\n",
      "Epoch 942: avg loss training: 2.8153,... Gradient Norm: 0.2716\n",
      "Epoch 943: avg loss training: 2.8153,... Gradient Norm: 0.2109\n",
      "Epoch 944: avg loss training: 2.8153,... Gradient Norm: 0.1585\n",
      "Epoch 945: avg loss training: 2.8153,... Gradient Norm: 0.2695\n",
      "Epoch 946: avg loss training: 2.8153,... Gradient Norm: 0.1630\n",
      "Epoch 947: avg loss training: 2.8153,... Gradient Norm: 0.2426\n",
      "Epoch 948: avg loss training: 2.8153,... Gradient Norm: 0.2448\n",
      "Epoch 949: avg loss training: 2.8153,... Gradient Norm: 0.1220\n",
      "Epoch 950: avg loss training: 2.8153,... Gradient Norm: 0.2668\n",
      "Epoch 951: avg loss training: 2.8153,... Gradient Norm: 0.2628\n",
      "Epoch 952: avg loss training: 2.8153,... Gradient Norm: 0.2806\n",
      "Epoch 953: avg loss training: 2.8153,... Gradient Norm: 0.2472\n",
      "Epoch 954: avg loss training: 2.8153,... Gradient Norm: 0.1616\n",
      "Epoch 955: avg loss training: 2.8153,... Gradient Norm: 0.1884\n",
      "Epoch 956: avg loss training: 2.8153,... Gradient Norm: 0.1843\n",
      "Epoch 957: avg loss training: 2.8153,... Gradient Norm: 0.2231\n",
      "Epoch 958: avg loss training: 2.8153,... Gradient Norm: 0.2254\n",
      "Epoch 959: avg loss training: 2.8153,... Gradient Norm: 0.1306\n",
      "Epoch 960: avg loss training: 2.8153,... Gradient Norm: 0.1677\n",
      "Epoch 961: avg loss training: 2.8153,... Gradient Norm: 0.1835\n",
      "Epoch 962: avg loss training: 2.8153,... Gradient Norm: 0.2291\n",
      "Epoch 963: avg loss training: 2.8153,... Gradient Norm: 0.1533\n",
      "Epoch 964: avg loss training: 2.8153,... Gradient Norm: 0.1828\n",
      "Epoch 965: avg loss training: 2.8153,... Gradient Norm: 0.1717\n",
      "Epoch 966: avg loss training: 2.8153,... Gradient Norm: 0.2260\n",
      "Epoch 967: avg loss training: 2.8153,... Gradient Norm: 0.2452\n",
      "Epoch 968: avg loss training: 2.8153,... Gradient Norm: 0.2641\n",
      "Epoch 969: avg loss training: 2.8153,... Gradient Norm: 0.1739\n",
      "Epoch 970: avg loss training: 2.8153,... Gradient Norm: 0.2014\n",
      "Epoch 971: avg loss training: 2.8153,... Gradient Norm: 0.1622\n",
      "Epoch 972: avg loss training: 2.8153,... Gradient Norm: 0.1237\n",
      "Epoch 973: avg loss training: 2.8153,... Gradient Norm: 0.1710\n",
      "Epoch 974: avg loss training: 2.8153,... Gradient Norm: 0.0585\n",
      "Epoch 975: avg loss training: 2.8153,... Gradient Norm: 0.2367\n",
      "Epoch 976: avg loss training: 2.8153,... Gradient Norm: 0.1707\n",
      "Epoch 977: avg loss training: 2.8153,... Gradient Norm: 0.2091\n",
      "Epoch 978: avg loss training: 2.8153,... Gradient Norm: 0.2131\n",
      "Epoch 979: avg loss training: 2.8153,... Gradient Norm: 0.2113\n",
      "Epoch 980: avg loss training: 2.8153,... Gradient Norm: 0.2702\n",
      "Epoch 981: avg loss training: 2.8153,... Gradient Norm: 0.1949\n",
      "Epoch 982: avg loss training: 2.8153,... Gradient Norm: 0.2304\n",
      "Epoch 983: avg loss training: 2.8153,... Gradient Norm: 0.1557\n",
      "Epoch 984: avg loss training: 2.8153,... Gradient Norm: 0.1567\n",
      "Epoch 985: avg loss training: 2.8153,... Gradient Norm: 0.1834\n",
      "Epoch 986: avg loss training: 2.8153,... Gradient Norm: 0.2524\n",
      "Epoch 987: avg loss training: 2.8153,... Gradient Norm: 0.2309\n",
      "Epoch 988: avg loss training: 2.8153,... Gradient Norm: 0.1728\n",
      "Epoch 989: avg loss training: 2.8153,... Gradient Norm: 0.1672\n",
      "Epoch 990: avg loss training: 2.8153,... Gradient Norm: 0.1597\n",
      "Epoch 991: avg loss training: 2.8153,... Gradient Norm: 0.1441\n",
      "Epoch 992: avg loss training: 2.8153,... Gradient Norm: 0.2148\n",
      "Epoch 993: avg loss training: 2.8153,... Gradient Norm: 0.1109\n",
      "Epoch 994: avg loss training: 2.8153,... Gradient Norm: 0.1542\n",
      "Epoch 995: avg loss training: 2.8153,... Gradient Norm: 0.1422\n",
      "Epoch 996: avg loss training: 2.8153,... Gradient Norm: 0.1484\n",
      "Epoch 997: avg loss training: 2.8153,... Gradient Norm: 0.2495\n",
      "Epoch 998: avg loss training: 2.8153,... Gradient Norm: 0.2851\n",
      "Epoch 999: avg loss training: 2.8153,... Gradient Norm: 0.2248\n",
      "Epoch 1000: avg loss training: 2.8153,... Gradient Norm: 0.1907\n",
      "final result: (0.1154686069553259, 0.048736438757693845) ********************\n"
     ]
    }
   ],
   "source": [
    "dataset = Multi(300, preprocessor=None, double=True) # 1500 obs.\n",
    "x, y = dataset.cause.flatten().numpy(), dataset.effect.flatten().numpy()\n",
    "\n",
    "score_new, score_orig, model_x, model_y, new_x_f, new_x_r, f_forward, f_reverse= loci_w_marginal(\n",
    "    x, y, independence_test=False, neural_network=True, \n",
    "    return_function=True, n_steps=1000, marginal_loglik = True\n",
    ")\n",
    "\n",
    "print(f\"final result: {score_new, score_orig} ********************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9949b205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ef5a257",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SIM(100, preprocessor=None, double=True) # 1000 obs.\n",
    "x, y = dataset.cause.flatten().numpy(), dataset.effect.flatten().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e4ce79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: avg loss training: 3.5054,... Gradient Norm: 7.0736\n",
      "Epoch 2: avg loss training: 121.1535,... Gradient Norm: 1378.5118\n",
      "Epoch 3: avg loss training: 9.3714,... Gradient Norm: 79.8502\n",
      "Epoch 4: avg loss training: 32.6478,... Gradient Norm: 403.7557\n",
      "Epoch 5: avg loss training: 13.3923,... Gradient Norm: 118.4930\n",
      "Epoch 6: avg loss training: 10.6220,... Gradient Norm: 59.1076\n",
      "Epoch 7: avg loss training: 12.0520,... Gradient Norm: 102.0290\n",
      "Epoch 8: avg loss training: 7.2301,... Gradient Norm: 32.1839\n",
      "Epoch 9: avg loss training: 5.2143,... Gradient Norm: 24.2067\n",
      "Epoch 10: avg loss training: 4.0753,... Gradient Norm: 9.6553\n",
      "Epoch 11: avg loss training: 4.0822,... Gradient Norm: 10.8846\n",
      "Epoch 12: avg loss training: 4.5232,... Gradient Norm: 18.2017\n",
      "Epoch 13: avg loss training: 4.6607,... Gradient Norm: 19.3141\n",
      "Epoch 14: avg loss training: 4.4461,... Gradient Norm: 16.0315\n",
      "Epoch 15: avg loss training: 4.1127,... Gradient Norm: 11.1010\n",
      "Epoch 16: avg loss training: 3.9262,... Gradient Norm: 9.3152\n",
      "Epoch 17: avg loss training: 3.8429,... Gradient Norm: 7.9247\n",
      "Epoch 18: avg loss training: 3.6750,... Gradient Norm: 5.4076\n",
      "Epoch 19: avg loss training: 3.5685,... Gradient Norm: 6.3419\n",
      "Epoch 20: avg loss training: 3.4601,... Gradient Norm: 6.1526\n",
      "Epoch 21: avg loss training: 3.3575,... Gradient Norm: 3.5278\n",
      "Epoch 22: avg loss training: 3.3416,... Gradient Norm: 4.4829\n",
      "Epoch 23: avg loss training: 3.3503,... Gradient Norm: 6.6656\n",
      "Epoch 24: avg loss training: 3.2845,... Gradient Norm: 4.9516\n",
      "Epoch 25: avg loss training: 3.2241,... Gradient Norm: 2.3249\n",
      "Epoch 26: avg loss training: 3.2216,... Gradient Norm: 4.9838\n",
      "Epoch 27: avg loss training: 3.1902,... Gradient Norm: 4.6961\n",
      "Epoch 28: avg loss training: 3.1440,... Gradient Norm: 2.5177\n",
      "Epoch 29: avg loss training: 3.1414,... Gradient Norm: 5.3304\n",
      "Epoch 30: avg loss training: 3.0956,... Gradient Norm: 3.7525\n",
      "Epoch 31: avg loss training: 3.0578,... Gradient Norm: 2.6075\n",
      "Epoch 32: avg loss training: 3.0492,... Gradient Norm: 4.8906\n",
      "Epoch 33: avg loss training: 3.0122,... Gradient Norm: 2.2986\n",
      "Epoch 34: avg loss training: 2.9995,... Gradient Norm: 2.4958\n",
      "Epoch 35: avg loss training: 2.9901,... Gradient Norm: 3.7528\n",
      "Epoch 36: avg loss training: 2.9581,... Gradient Norm: 1.3232\n",
      "Epoch 37: avg loss training: 2.9488,... Gradient Norm: 2.6872\n",
      "Epoch 38: avg loss training: 2.9399,... Gradient Norm: 3.0523\n",
      "Epoch 39: avg loss training: 2.9227,... Gradient Norm: 1.3803\n",
      "Epoch 40: avg loss training: 2.9190,... Gradient Norm: 3.3027\n",
      "Epoch 41: avg loss training: 2.9041,... Gradient Norm: 2.6344\n",
      "Epoch 42: avg loss training: 2.8895,... Gradient Norm: 1.1243\n",
      "Epoch 43: avg loss training: 2.8918,... Gradient Norm: 3.7619\n",
      "Epoch 44: avg loss training: 2.8803,... Gradient Norm: 1.3245\n",
      "Epoch 45: avg loss training: 2.8791,... Gradient Norm: 2.8887\n",
      "Epoch 46: avg loss training: 2.8655,... Gradient Norm: 1.4827\n",
      "Epoch 47: avg loss training: 2.8618,... Gradient Norm: 2.2247\n",
      "Epoch 48: avg loss training: 2.8601,... Gradient Norm: 2.1154\n",
      "Epoch 49: avg loss training: 2.8540,... Gradient Norm: 1.5518\n",
      "Epoch 50: avg loss training: 2.8509,... Gradient Norm: 2.7345\n",
      "Epoch 51: avg loss training: 2.8430,... Gradient Norm: 0.6800\n",
      "Epoch 52: avg loss training: 2.8442,... Gradient Norm: 2.7753\n",
      "Epoch 53: avg loss training: 2.8364,... Gradient Norm: 0.6758\n",
      "Epoch 54: avg loss training: 2.8355,... Gradient Norm: 2.2932\n",
      "Epoch 55: avg loss training: 2.8312,... Gradient Norm: 0.8967\n",
      "Epoch 56: avg loss training: 2.8311,... Gradient Norm: 2.1027\n",
      "Epoch 57: avg loss training: 2.8248,... Gradient Norm: 0.7713\n",
      "Epoch 58: avg loss training: 2.8247,... Gradient Norm: 1.7719\n",
      "Epoch 59: avg loss training: 2.8218,... Gradient Norm: 1.0516\n",
      "Epoch 60: avg loss training: 2.8202,... Gradient Norm: 1.6174\n",
      "Epoch 61: avg loss training: 2.8182,... Gradient Norm: 0.6720\n",
      "Epoch 62: avg loss training: 2.8173,... Gradient Norm: 1.6897\n",
      "Epoch 63: avg loss training: 2.8136,... Gradient Norm: 0.6021\n",
      "Epoch 64: avg loss training: 2.8142,... Gradient Norm: 1.7477\n",
      "Epoch 65: avg loss training: 2.8113,... Gradient Norm: 0.6075\n",
      "Epoch 66: avg loss training: 2.8106,... Gradient Norm: 1.4030\n",
      "Epoch 67: avg loss training: 2.8087,... Gradient Norm: 0.7462\n",
      "Epoch 68: avg loss training: 2.8073,... Gradient Norm: 1.3027\n",
      "Epoch 69: avg loss training: 2.8053,... Gradient Norm: 0.5753\n",
      "Epoch 70: avg loss training: 2.8049,... Gradient Norm: 1.4163\n",
      "Epoch 71: avg loss training: 2.8022,... Gradient Norm: 0.4437\n",
      "Epoch 72: avg loss training: 2.8016,... Gradient Norm: 0.9436\n",
      "Epoch 73: avg loss training: 2.7995,... Gradient Norm: 0.7745\n",
      "Epoch 74: avg loss training: 2.7982,... Gradient Norm: 0.5031\n",
      "Epoch 75: avg loss training: 2.7970,... Gradient Norm: 1.0629\n",
      "Epoch 76: avg loss training: 2.7950,... Gradient Norm: 0.2172\n",
      "Epoch 77: avg loss training: 2.7943,... Gradient Norm: 0.8883\n",
      "Epoch 78: avg loss training: 2.7926,... Gradient Norm: 0.6013\n",
      "Epoch 79: avg loss training: 2.7916,... Gradient Norm: 0.6497\n",
      "Epoch 80: avg loss training: 2.7904,... Gradient Norm: 1.0325\n",
      "Epoch 81: avg loss training: 2.7895,... Gradient Norm: 1.0685\n",
      "Epoch 82: avg loss training: 2.7882,... Gradient Norm: 0.6654\n",
      "Epoch 83: avg loss training: 2.7870,... Gradient Norm: 0.2522\n",
      "Epoch 84: avg loss training: 2.7861,... Gradient Norm: 0.3581\n",
      "Epoch 85: avg loss training: 2.7850,... Gradient Norm: 0.5512\n",
      "Epoch 86: avg loss training: 2.7842,... Gradient Norm: 0.7989\n",
      "Epoch 87: avg loss training: 2.7833,... Gradient Norm: 1.0102\n",
      "Epoch 88: avg loss training: 2.7824,... Gradient Norm: 1.1930\n",
      "Epoch 89: avg loss training: 2.7814,... Gradient Norm: 1.4363\n",
      "Epoch 90: avg loss training: 2.7810,... Gradient Norm: 1.6944\n",
      "Epoch 91: avg loss training: 2.7802,... Gradient Norm: 1.8612\n",
      "Epoch 92: avg loss training: 2.7795,... Gradient Norm: 1.9956\n",
      "Epoch 93: avg loss training: 2.7783,... Gradient Norm: 1.8351\n",
      "Epoch 94: avg loss training: 2.7774,... Gradient Norm: 1.7372\n",
      "Epoch 95: avg loss training: 2.7763,... Gradient Norm: 1.5142\n",
      "Epoch 96: avg loss training: 2.7752,... Gradient Norm: 1.2452\n",
      "Epoch 97: avg loss training: 2.7740,... Gradient Norm: 1.0209\n",
      "Epoch 98: avg loss training: 2.7729,... Gradient Norm: 0.8317\n",
      "Epoch 99: avg loss training: 2.7720,... Gradient Norm: 0.7028\n",
      "Epoch 100: avg loss training: 2.7711,... Gradient Norm: 0.7175\n",
      "Epoch 101: avg loss training: 2.7703,... Gradient Norm: 0.7614\n",
      "Epoch 102: avg loss training: 2.7697,... Gradient Norm: 1.0954\n",
      "Epoch 103: avg loss training: 2.7692,... Gradient Norm: 1.6452\n",
      "Epoch 104: avg loss training: 2.7696,... Gradient Norm: 2.6261\n",
      "Epoch 105: avg loss training: 2.7716,... Gradient Norm: 4.4573\n",
      "Epoch 106: avg loss training: 2.7801,... Gradient Norm: 7.1976\n",
      "Epoch 107: avg loss training: 2.7875,... Gradient Norm: 9.2867\n",
      "Epoch 108: avg loss training: 2.7811,... Gradient Norm: 7.5794\n",
      "Epoch 109: avg loss training: 2.7659,... Gradient Norm: 1.6293\n",
      "Epoch 110: avg loss training: 2.7714,... Gradient Norm: 5.3093\n",
      "Epoch 111: avg loss training: 2.7825,... Gradient Norm: 8.2171\n",
      "Epoch 112: avg loss training: 2.7680,... Gradient Norm: 3.9239\n",
      "Epoch 113: avg loss training: 2.7673,... Gradient Norm: 3.6757\n",
      "Epoch 114: avg loss training: 2.7756,... Gradient Norm: 6.7730\n",
      "Epoch 115: avg loss training: 2.7650,... Gradient Norm: 2.4197\n",
      "Epoch 116: avg loss training: 2.7671,... Gradient Norm: 4.1647\n",
      "Epoch 117: avg loss training: 2.7720,... Gradient Norm: 5.6836\n",
      "Epoch 118: avg loss training: 2.7625,... Gradient Norm: 0.3274\n",
      "Epoch 119: avg loss training: 2.7697,... Gradient Norm: 5.7662\n",
      "Epoch 120: avg loss training: 2.7694,... Gradient Norm: 5.4587\n",
      "Epoch 121: avg loss training: 2.7619,... Gradient Norm: 1.1318\n",
      "Epoch 122: avg loss training: 2.7702,... Gradient Norm: 6.2588\n",
      "Epoch 123: avg loss training: 2.7647,... Gradient Norm: 3.7179\n",
      "Epoch 124: avg loss training: 2.7631,... Gradient Norm: 3.0268\n",
      "Epoch 125: avg loss training: 2.7679,... Gradient Norm: 5.5975\n",
      "Epoch 126: avg loss training: 2.7609,... Gradient Norm: 1.4196\n",
      "Epoch 127: avg loss training: 2.7638,... Gradient Norm: 3.7055\n",
      "Epoch 128: avg loss training: 2.7626,... Gradient Norm: 3.4585\n",
      "Epoch 129: avg loss training: 2.7601,... Gradient Norm: 1.2264\n",
      "Epoch 130: avg loss training: 2.7629,... Gradient Norm: 3.7491\n",
      "Epoch 131: avg loss training: 2.7595,... Gradient Norm: 1.0201\n",
      "Epoch 132: avg loss training: 2.7610,... Gradient Norm: 2.9174\n",
      "Epoch 133: avg loss training: 2.7606,... Gradient Norm: 2.7205\n",
      "Epoch 134: avg loss training: 2.7589,... Gradient Norm: 1.0681\n",
      "Epoch 135: avg loss training: 2.7606,... Gradient Norm: 3.0052\n",
      "Epoch 136: avg loss training: 2.7585,... Gradient Norm: 0.9583\n",
      "Epoch 137: avg loss training: 2.7591,... Gradient Norm: 2.1631\n",
      "Epoch 138: avg loss training: 2.7590,... Gradient Norm: 2.2608\n",
      "Epoch 139: avg loss training: 2.7578,... Gradient Norm: 0.2969\n",
      "Epoch 140: avg loss training: 2.7584,... Gradient Norm: 1.9921\n",
      "Epoch 141: avg loss training: 2.7576,... Gradient Norm: 0.9118\n",
      "Epoch 142: avg loss training: 2.7576,... Gradient Norm: 1.3651\n",
      "Epoch 143: avg loss training: 2.7577,... Gradient Norm: 1.7999\n",
      "Epoch 144: avg loss training: 2.7569,... Gradient Norm: 0.1562\n",
      "Epoch 145: avg loss training: 2.7572,... Gradient Norm: 1.4436\n",
      "Epoch 146: avg loss training: 2.7568,... Gradient Norm: 0.8041\n",
      "Epoch 147: avg loss training: 2.7566,... Gradient Norm: 0.8466\n",
      "Epoch 148: avg loss training: 2.7566,... Gradient Norm: 1.1170\n",
      "Epoch 149: avg loss training: 2.7561,... Gradient Norm: 0.2095\n",
      "Epoch 150: avg loss training: 2.7562,... Gradient Norm: 1.1433\n",
      "Epoch 151: avg loss training: 2.7559,... Gradient Norm: 0.5096\n",
      "Epoch 152: avg loss training: 2.7558,... Gradient Norm: 0.7734\n",
      "Epoch 153: avg loss training: 2.7558,... Gradient Norm: 0.9992\n",
      "Epoch 154: avg loss training: 2.7554,... Gradient Norm: 0.0879\n",
      "Epoch 155: avg loss training: 2.7555,... Gradient Norm: 1.0306\n",
      "Epoch 156: avg loss training: 2.7552,... Gradient Norm: 0.8753\n",
      "Epoch 157: avg loss training: 2.7550,... Gradient Norm: 0.3157\n",
      "Epoch 158: avg loss training: 2.7550,... Gradient Norm: 0.9404\n",
      "Epoch 159: avg loss training: 2.7547,... Gradient Norm: 0.5716\n",
      "Epoch 160: avg loss training: 2.7545,... Gradient Norm: 0.3941\n",
      "Epoch 161: avg loss training: 2.7545,... Gradient Norm: 0.7931\n",
      "Epoch 162: avg loss training: 2.7542,... Gradient Norm: 0.3311\n",
      "Epoch 163: avg loss training: 2.7541,... Gradient Norm: 0.4484\n",
      "Epoch 164: avg loss training: 2.7541,... Gradient Norm: 0.7276\n",
      "Epoch 165: avg loss training: 2.7538,... Gradient Norm: 0.3110\n",
      "Epoch 166: avg loss training: 2.7537,... Gradient Norm: 0.4434\n",
      "Epoch 167: avg loss training: 2.7536,... Gradient Norm: 0.7332\n",
      "Epoch 168: avg loss training: 2.7534,... Gradient Norm: 0.2741\n",
      "Epoch 169: avg loss training: 2.7533,... Gradient Norm: 0.4830\n",
      "Epoch 170: avg loss training: 2.7532,... Gradient Norm: 0.6364\n",
      "Epoch 171: avg loss training: 2.7530,... Gradient Norm: 0.1439\n",
      "Epoch 172: avg loss training: 2.7530,... Gradient Norm: 0.4357\n",
      "Epoch 173: avg loss training: 2.7528,... Gradient Norm: 0.3818\n",
      "Epoch 174: avg loss training: 2.7527,... Gradient Norm: 0.1526\n",
      "Epoch 175: avg loss training: 2.7526,... Gradient Norm: 0.3377\n",
      "Epoch 176: avg loss training: 2.7525,... Gradient Norm: 0.1596\n",
      "Epoch 177: avg loss training: 2.7524,... Gradient Norm: 0.2133\n",
      "Epoch 178: avg loss training: 2.7523,... Gradient Norm: 0.1898\n",
      "Epoch 179: avg loss training: 2.7522,... Gradient Norm: 0.1366\n",
      "Epoch 180: avg loss training: 2.7521,... Gradient Norm: 0.1997\n",
      "Epoch 181: avg loss training: 2.7520,... Gradient Norm: 0.1927\n",
      "Epoch 182: avg loss training: 2.7519,... Gradient Norm: 0.1298\n",
      "Epoch 183: avg loss training: 2.7518,... Gradient Norm: 0.1131\n",
      "Epoch 184: avg loss training: 2.7517,... Gradient Norm: 0.1606\n",
      "Epoch 185: avg loss training: 2.7516,... Gradient Norm: 0.1956\n",
      "Epoch 186: avg loss training: 2.7515,... Gradient Norm: 0.2384\n",
      "Epoch 187: avg loss training: 2.7514,... Gradient Norm: 0.1535\n",
      "Epoch 188: avg loss training: 2.7513,... Gradient Norm: 0.1285\n",
      "Epoch 189: avg loss training: 2.7512,... Gradient Norm: 0.1203\n",
      "Epoch 190: avg loss training: 2.7512,... Gradient Norm: 0.1637\n",
      "Epoch 191: avg loss training: 2.7511,... Gradient Norm: 0.1292\n",
      "Epoch 192: avg loss training: 2.7510,... Gradient Norm: 0.1767\n",
      "Epoch 193: avg loss training: 2.7509,... Gradient Norm: 0.0772\n",
      "Epoch 194: avg loss training: 2.7508,... Gradient Norm: 0.1418\n",
      "Epoch 195: avg loss training: 2.7507,... Gradient Norm: 0.1447\n",
      "Epoch 196: avg loss training: 2.7507,... Gradient Norm: 0.1322\n",
      "Epoch 197: avg loss training: 2.7506,... Gradient Norm: 0.2013\n",
      "Epoch 198: avg loss training: 2.7505,... Gradient Norm: 0.1275\n",
      "Epoch 199: avg loss training: 2.7504,... Gradient Norm: 0.1038\n",
      "Epoch 200: avg loss training: 2.7504,... Gradient Norm: 0.1621\n",
      "Epoch 201: avg loss training: 2.7502,... Gradient Norm: 0.1662\n",
      "Epoch 202: avg loss training: 2.7502,... Gradient Norm: 0.1502\n",
      "Epoch 203: avg loss training: 2.7501,... Gradient Norm: 0.1100\n",
      "Epoch 204: avg loss training: 2.7500,... Gradient Norm: 0.1577\n",
      "Epoch 205: avg loss training: 2.7500,... Gradient Norm: 0.1526\n",
      "Epoch 206: avg loss training: 2.7499,... Gradient Norm: 0.0643\n",
      "Epoch 207: avg loss training: 2.7498,... Gradient Norm: 0.2027\n",
      "Epoch 208: avg loss training: 2.7497,... Gradient Norm: 0.3283\n",
      "Epoch 209: avg loss training: 2.7497,... Gradient Norm: 0.3233\n",
      "Epoch 210: avg loss training: 2.7496,... Gradient Norm: 0.1177\n",
      "Epoch 211: avg loss training: 2.7496,... Gradient Norm: 0.2962\n",
      "Epoch 212: avg loss training: 2.7495,... Gradient Norm: 0.3052\n",
      "Epoch 213: avg loss training: 2.7494,... Gradient Norm: 0.2298\n",
      "Epoch 214: avg loss training: 2.7493,... Gradient Norm: 0.1416\n",
      "Epoch 215: avg loss training: 2.7493,... Gradient Norm: 0.0753\n",
      "Epoch 216: avg loss training: 2.7492,... Gradient Norm: 0.1139\n",
      "Epoch 217: avg loss training: 2.7492,... Gradient Norm: 0.1358\n",
      "Epoch 218: avg loss training: 2.7491,... Gradient Norm: 0.1256\n",
      "Epoch 219: avg loss training: 2.7491,... Gradient Norm: 0.1766\n",
      "Epoch 220: avg loss training: 2.7490,... Gradient Norm: 0.2185\n",
      "Epoch 221: avg loss training: 2.7490,... Gradient Norm: 0.2441\n",
      "Epoch 222: avg loss training: 2.7489,... Gradient Norm: 0.0804\n",
      "Epoch 223: avg loss training: 2.7488,... Gradient Norm: 0.1832\n",
      "Epoch 224: avg loss training: 2.7488,... Gradient Norm: 0.1957\n",
      "Epoch 225: avg loss training: 2.7487,... Gradient Norm: 0.0935\n",
      "Epoch 226: avg loss training: 2.7487,... Gradient Norm: 0.2329\n",
      "Epoch 227: avg loss training: 2.7487,... Gradient Norm: 0.3196\n",
      "Epoch 228: avg loss training: 2.7486,... Gradient Norm: 0.2677\n",
      "Epoch 229: avg loss training: 2.7485,... Gradient Norm: 0.0853\n",
      "Epoch 230: avg loss training: 2.7484,... Gradient Norm: 0.1984\n",
      "Epoch 231: avg loss training: 2.7484,... Gradient Norm: 0.1266\n",
      "Epoch 232: avg loss training: 2.7483,... Gradient Norm: 0.1594\n",
      "Epoch 233: avg loss training: 2.7483,... Gradient Norm: 0.1384\n",
      "Epoch 234: avg loss training: 2.7483,... Gradient Norm: 0.1283\n",
      "Epoch 235: avg loss training: 2.7482,... Gradient Norm: 0.0924\n",
      "Epoch 236: avg loss training: 2.7482,... Gradient Norm: 0.2082\n",
      "Epoch 237: avg loss training: 2.7481,... Gradient Norm: 0.1953\n",
      "Epoch 238: avg loss training: 2.7481,... Gradient Norm: 0.1995\n",
      "Epoch 239: avg loss training: 2.7480,... Gradient Norm: 0.3743\n",
      "Epoch 240: avg loss training: 2.7480,... Gradient Norm: 0.1228\n",
      "Epoch 241: avg loss training: 2.7479,... Gradient Norm: 0.3966\n",
      "Epoch 242: avg loss training: 2.7479,... Gradient Norm: 0.2211\n",
      "Epoch 243: avg loss training: 2.7478,... Gradient Norm: 0.2639\n",
      "Epoch 244: avg loss training: 2.7478,... Gradient Norm: 0.4056\n",
      "Epoch 245: avg loss training: 2.7477,... Gradient Norm: 0.1338\n",
      "Epoch 246: avg loss training: 2.7477,... Gradient Norm: 0.3495\n",
      "Epoch 247: avg loss training: 2.7476,... Gradient Norm: 0.2100\n",
      "Epoch 248: avg loss training: 2.7476,... Gradient Norm: 0.3589\n",
      "Epoch 249: avg loss training: 2.7476,... Gradient Norm: 0.4696\n",
      "Epoch 250: avg loss training: 2.7475,... Gradient Norm: 0.1327\n",
      "Epoch 251: avg loss training: 2.7475,... Gradient Norm: 0.6028\n",
      "Epoch 252: avg loss training: 2.7474,... Gradient Norm: 0.2598\n",
      "Epoch 253: avg loss training: 2.7474,... Gradient Norm: 0.4423\n",
      "Epoch 254: avg loss training: 2.7473,... Gradient Norm: 0.2330\n",
      "Epoch 255: avg loss training: 2.7472,... Gradient Norm: 0.4889\n",
      "Epoch 256: avg loss training: 2.7472,... Gradient Norm: 0.4063\n",
      "Epoch 257: avg loss training: 2.7471,... Gradient Norm: 0.2260\n",
      "Epoch 258: avg loss training: 2.7471,... Gradient Norm: 0.3414\n",
      "Epoch 259: avg loss training: 2.7470,... Gradient Norm: 0.2250\n",
      "Epoch 260: avg loss training: 2.7470,... Gradient Norm: 0.1957\n",
      "Epoch 261: avg loss training: 2.7469,... Gradient Norm: 0.1877\n",
      "Epoch 262: avg loss training: 2.7469,... Gradient Norm: 0.2057\n",
      "Epoch 263: avg loss training: 2.7468,... Gradient Norm: 0.2636\n",
      "Epoch 264: avg loss training: 2.7468,... Gradient Norm: 0.1169\n",
      "Epoch 265: avg loss training: 2.7468,... Gradient Norm: 0.3164\n",
      "Epoch 266: avg loss training: 2.7467,... Gradient Norm: 0.2268\n",
      "Epoch 267: avg loss training: 2.7467,... Gradient Norm: 0.2777\n",
      "Epoch 268: avg loss training: 2.7466,... Gradient Norm: 0.1715\n",
      "Epoch 269: avg loss training: 2.7466,... Gradient Norm: 0.3677\n",
      "Epoch 270: avg loss training: 2.7465,... Gradient Norm: 0.3768\n",
      "Epoch 271: avg loss training: 2.7465,... Gradient Norm: 0.2519\n",
      "Epoch 272: avg loss training: 2.7465,... Gradient Norm: 0.5670\n",
      "Epoch 273: avg loss training: 2.7464,... Gradient Norm: 0.1469\n",
      "Epoch 274: avg loss training: 2.7464,... Gradient Norm: 0.4324\n",
      "Epoch 275: avg loss training: 2.7463,... Gradient Norm: 0.1832\n",
      "Epoch 276: avg loss training: 2.7463,... Gradient Norm: 0.4722\n",
      "Epoch 277: avg loss training: 2.7463,... Gradient Norm: 0.4696\n",
      "Epoch 278: avg loss training: 2.7462,... Gradient Norm: 0.2144\n",
      "Epoch 279: avg loss training: 2.7461,... Gradient Norm: 0.4099\n",
      "Epoch 280: avg loss training: 2.7461,... Gradient Norm: 0.2177\n",
      "Epoch 281: avg loss training: 2.7461,... Gradient Norm: 0.1958\n",
      "Epoch 282: avg loss training: 2.7460,... Gradient Norm: 0.2463\n",
      "Epoch 283: avg loss training: 2.7460,... Gradient Norm: 0.1155\n",
      "Epoch 284: avg loss training: 2.7459,... Gradient Norm: 0.1153\n",
      "Epoch 285: avg loss training: 2.7459,... Gradient Norm: 0.1245\n",
      "Epoch 286: avg loss training: 2.7458,... Gradient Norm: 0.1600\n",
      "Epoch 287: avg loss training: 2.7458,... Gradient Norm: 0.1261\n",
      "Epoch 288: avg loss training: 2.7457,... Gradient Norm: 0.1149\n",
      "Epoch 289: avg loss training: 2.7457,... Gradient Norm: 0.1050\n",
      "Epoch 290: avg loss training: 2.7457,... Gradient Norm: 0.1420\n",
      "Epoch 291: avg loss training: 2.7456,... Gradient Norm: 0.1527\n",
      "Epoch 292: avg loss training: 2.7456,... Gradient Norm: 0.0611\n",
      "Epoch 293: avg loss training: 2.7455,... Gradient Norm: 0.1741\n",
      "Epoch 294: avg loss training: 2.7455,... Gradient Norm: 0.1671\n",
      "Epoch 295: avg loss training: 2.7455,... Gradient Norm: 0.1312\n",
      "Epoch 296: avg loss training: 2.7454,... Gradient Norm: 0.1682\n",
      "Epoch 297: avg loss training: 2.7454,... Gradient Norm: 0.2320\n",
      "Epoch 298: avg loss training: 2.7453,... Gradient Norm: 0.1943\n",
      "Epoch 299: avg loss training: 2.7453,... Gradient Norm: 0.4297\n",
      "Epoch 300: avg loss training: 2.7453,... Gradient Norm: 0.1513\n",
      "Epoch 301: avg loss training: 2.7452,... Gradient Norm: 0.1934\n",
      "Epoch 302: avg loss training: 2.7452,... Gradient Norm: 0.1669\n",
      "Epoch 303: avg loss training: 2.7452,... Gradient Norm: 0.1338\n",
      "Epoch 304: avg loss training: 2.7452,... Gradient Norm: 0.3718\n",
      "Epoch 305: avg loss training: 2.7451,... Gradient Norm: 0.3564\n",
      "Epoch 306: avg loss training: 2.7451,... Gradient Norm: 0.1424\n",
      "Epoch 307: avg loss training: 2.7450,... Gradient Norm: 0.1002\n",
      "Epoch 308: avg loss training: 2.7450,... Gradient Norm: 0.1406\n",
      "Epoch 309: avg loss training: 2.7450,... Gradient Norm: 0.2171\n",
      "Epoch 310: avg loss training: 2.7449,... Gradient Norm: 0.2120\n",
      "Epoch 311: avg loss training: 2.7449,... Gradient Norm: 0.1025\n",
      "Epoch 312: avg loss training: 2.7449,... Gradient Norm: 0.1418\n",
      "Epoch 313: avg loss training: 2.7449,... Gradient Norm: 0.1950\n",
      "Epoch 314: avg loss training: 2.7448,... Gradient Norm: 0.3212\n",
      "Epoch 315: avg loss training: 2.7448,... Gradient Norm: 0.3239\n",
      "Epoch 316: avg loss training: 2.7448,... Gradient Norm: 0.1714\n",
      "Epoch 317: avg loss training: 2.7447,... Gradient Norm: 0.1026\n",
      "Epoch 318: avg loss training: 2.7447,... Gradient Norm: 0.3588\n",
      "Epoch 319: avg loss training: 2.7447,... Gradient Norm: 0.3725\n",
      "Epoch 320: avg loss training: 2.7446,... Gradient Norm: 0.1381\n",
      "Epoch 321: avg loss training: 2.7446,... Gradient Norm: 0.2893\n",
      "Epoch 322: avg loss training: 2.7446,... Gradient Norm: 0.2496\n",
      "Epoch 323: avg loss training: 2.7446,... Gradient Norm: 0.1653\n",
      "Epoch 324: avg loss training: 2.7445,... Gradient Norm: 0.1743\n",
      "Epoch 325: avg loss training: 2.7445,... Gradient Norm: 0.0793\n",
      "Epoch 326: avg loss training: 2.7445,... Gradient Norm: 0.0874\n",
      "Epoch 327: avg loss training: 2.7444,... Gradient Norm: 0.1343\n",
      "Epoch 328: avg loss training: 2.7444,... Gradient Norm: 0.1254\n",
      "Epoch 329: avg loss training: 2.7444,... Gradient Norm: 0.0621\n",
      "Epoch 330: avg loss training: 2.7444,... Gradient Norm: 0.1780\n",
      "Epoch 331: avg loss training: 2.7443,... Gradient Norm: 0.2196\n",
      "Epoch 332: avg loss training: 2.7443,... Gradient Norm: 0.1577\n",
      "Epoch 333: avg loss training: 2.7443,... Gradient Norm: 0.2736\n",
      "Epoch 334: avg loss training: 2.7443,... Gradient Norm: 0.2516\n",
      "Epoch 335: avg loss training: 2.7442,... Gradient Norm: 0.1377\n",
      "Epoch 336: avg loss training: 2.7442,... Gradient Norm: 0.3692\n",
      "Epoch 337: avg loss training: 2.7442,... Gradient Norm: 0.1299\n",
      "Epoch 338: avg loss training: 2.7442,... Gradient Norm: 0.4530\n",
      "Epoch 339: avg loss training: 2.7441,... Gradient Norm: 0.2464\n",
      "Epoch 340: avg loss training: 2.7441,... Gradient Norm: 0.5280\n",
      "Epoch 341: avg loss training: 2.7441,... Gradient Norm: 0.2306\n",
      "Epoch 342: avg loss training: 2.7441,... Gradient Norm: 0.4091\n",
      "Epoch 343: avg loss training: 2.7441,... Gradient Norm: 0.3390\n",
      "Epoch 344: avg loss training: 2.7440,... Gradient Norm: 0.3695\n",
      "Epoch 345: avg loss training: 2.7440,... Gradient Norm: 0.2266\n",
      "Epoch 346: avg loss training: 2.7440,... Gradient Norm: 0.3360\n",
      "Epoch 347: avg loss training: 2.7439,... Gradient Norm: 0.1351\n",
      "Epoch 348: avg loss training: 2.7439,... Gradient Norm: 0.3427\n",
      "Epoch 349: avg loss training: 2.7439,... Gradient Norm: 0.3142\n",
      "Epoch 350: avg loss training: 2.7439,... Gradient Norm: 0.2396\n",
      "Epoch 351: avg loss training: 2.7439,... Gradient Norm: 0.3642\n",
      "Epoch 352: avg loss training: 2.7438,... Gradient Norm: 0.3033\n",
      "Epoch 353: avg loss training: 2.7438,... Gradient Norm: 0.4649\n",
      "Epoch 354: avg loss training: 2.7438,... Gradient Norm: 0.3447\n",
      "Epoch 355: avg loss training: 2.7438,... Gradient Norm: 0.4772\n",
      "Epoch 356: avg loss training: 2.7437,... Gradient Norm: 0.4359\n",
      "Epoch 357: avg loss training: 2.7437,... Gradient Norm: 0.3346\n",
      "Epoch 358: avg loss training: 2.7437,... Gradient Norm: 0.4680\n",
      "Epoch 359: avg loss training: 2.7437,... Gradient Norm: 0.1241\n",
      "Epoch 360: avg loss training: 2.7437,... Gradient Norm: 0.5712\n",
      "Epoch 361: avg loss training: 2.7436,... Gradient Norm: 0.3565\n",
      "Epoch 362: avg loss training: 2.7436,... Gradient Norm: 0.4847\n",
      "Epoch 363: avg loss training: 2.7436,... Gradient Norm: 0.5176\n",
      "Epoch 364: avg loss training: 2.7435,... Gradient Norm: 0.1578\n",
      "Epoch 365: avg loss training: 2.7435,... Gradient Norm: 0.5666\n",
      "Epoch 366: avg loss training: 2.7435,... Gradient Norm: 0.1626\n",
      "Epoch 367: avg loss training: 2.7435,... Gradient Norm: 0.3800\n",
      "Epoch 368: avg loss training: 2.7435,... Gradient Norm: 0.3714\n",
      "Epoch 369: avg loss training: 2.7434,... Gradient Norm: 0.1694\n",
      "Epoch 370: avg loss training: 2.7434,... Gradient Norm: 0.4382\n",
      "Epoch 371: avg loss training: 2.7434,... Gradient Norm: 0.1411\n",
      "Epoch 372: avg loss training: 2.7434,... Gradient Norm: 0.2849\n",
      "Epoch 373: avg loss training: 2.7433,... Gradient Norm: 0.1503\n",
      "Epoch 374: avg loss training: 2.7433,... Gradient Norm: 0.2329\n",
      "Epoch 375: avg loss training: 2.7433,... Gradient Norm: 0.1831\n",
      "Epoch 376: avg loss training: 2.7433,... Gradient Norm: 0.0878\n",
      "Epoch 377: avg loss training: 2.7433,... Gradient Norm: 0.2562\n",
      "Epoch 378: avg loss training: 2.7432,... Gradient Norm: 0.1559\n",
      "Epoch 379: avg loss training: 2.7432,... Gradient Norm: 0.1919\n",
      "Epoch 380: avg loss training: 2.7432,... Gradient Norm: 0.3487\n",
      "Epoch 381: avg loss training: 2.7432,... Gradient Norm: 0.1767\n",
      "Epoch 382: avg loss training: 2.7432,... Gradient Norm: 0.2538\n",
      "Epoch 383: avg loss training: 2.7431,... Gradient Norm: 0.1395\n",
      "Epoch 384: avg loss training: 2.7431,... Gradient Norm: 0.2759\n",
      "Epoch 385: avg loss training: 2.7431,... Gradient Norm: 0.1352\n",
      "Epoch 386: avg loss training: 2.7431,... Gradient Norm: 0.2007\n",
      "Epoch 387: avg loss training: 2.7431,... Gradient Norm: 0.2291\n",
      "Epoch 388: avg loss training: 2.7431,... Gradient Norm: 0.2951\n",
      "Epoch 389: avg loss training: 2.7430,... Gradient Norm: 0.2523\n",
      "Epoch 390: avg loss training: 2.7430,... Gradient Norm: 0.2082\n",
      "Epoch 391: avg loss training: 2.7430,... Gradient Norm: 0.3569\n",
      "Epoch 392: avg loss training: 2.7430,... Gradient Norm: 0.2382\n",
      "Epoch 393: avg loss training: 2.7430,... Gradient Norm: 0.4033\n",
      "Epoch 394: avg loss training: 2.7430,... Gradient Norm: 0.3195\n",
      "Epoch 395: avg loss training: 2.7429,... Gradient Norm: 0.2418\n",
      "Epoch 396: avg loss training: 2.7429,... Gradient Norm: 0.5120\n",
      "Epoch 397: avg loss training: 2.7429,... Gradient Norm: 0.2044\n",
      "Epoch 398: avg loss training: 2.7429,... Gradient Norm: 0.3568\n",
      "Epoch 399: avg loss training: 2.7428,... Gradient Norm: 0.1905\n",
      "Epoch 400: avg loss training: 2.7428,... Gradient Norm: 0.4460\n",
      "Epoch 401: avg loss training: 2.7428,... Gradient Norm: 0.3334\n",
      "Epoch 402: avg loss training: 2.7428,... Gradient Norm: 0.3842\n",
      "Epoch 403: avg loss training: 2.7428,... Gradient Norm: 0.4339\n",
      "Epoch 404: avg loss training: 2.7428,... Gradient Norm: 0.1013\n",
      "Epoch 405: avg loss training: 2.7428,... Gradient Norm: 0.5631\n",
      "Epoch 406: avg loss training: 2.7427,... Gradient Norm: 0.3840\n",
      "Epoch 407: avg loss training: 2.7427,... Gradient Norm: 0.3743\n",
      "Epoch 408: avg loss training: 2.7427,... Gradient Norm: 0.4075\n",
      "Epoch 409: avg loss training: 2.7427,... Gradient Norm: 0.2626\n",
      "Epoch 410: avg loss training: 2.7427,... Gradient Norm: 0.4833\n",
      "Epoch 411: avg loss training: 2.7427,... Gradient Norm: 0.4386\n",
      "Epoch 412: avg loss training: 2.7426,... Gradient Norm: 0.3121\n",
      "Epoch 413: avg loss training: 2.7426,... Gradient Norm: 0.3638\n",
      "Epoch 414: avg loss training: 2.7426,... Gradient Norm: 0.2933\n",
      "Epoch 415: avg loss training: 2.7426,... Gradient Norm: 0.4224\n",
      "Epoch 416: avg loss training: 2.7426,... Gradient Norm: 0.3269\n",
      "Epoch 417: avg loss training: 2.7426,... Gradient Norm: 0.3633\n",
      "Epoch 418: avg loss training: 2.7426,... Gradient Norm: 0.3902\n",
      "Epoch 419: avg loss training: 2.7425,... Gradient Norm: 0.0833\n",
      "Epoch 420: avg loss training: 2.7425,... Gradient Norm: 0.4303\n",
      "Epoch 421: avg loss training: 2.7425,... Gradient Norm: 0.3141\n",
      "Epoch 422: avg loss training: 2.7425,... Gradient Norm: 0.3629\n",
      "Epoch 423: avg loss training: 2.7425,... Gradient Norm: 0.3894\n",
      "Epoch 424: avg loss training: 2.7424,... Gradient Norm: 0.1727\n",
      "Epoch 425: avg loss training: 2.7425,... Gradient Norm: 0.5180\n",
      "Epoch 426: avg loss training: 2.7424,... Gradient Norm: 0.4463\n",
      "Epoch 427: avg loss training: 2.7424,... Gradient Norm: 0.3112\n",
      "Epoch 428: avg loss training: 2.7424,... Gradient Norm: 0.4777\n",
      "Epoch 429: avg loss training: 2.7424,... Gradient Norm: 0.2673\n",
      "Epoch 430: avg loss training: 2.7424,... Gradient Norm: 0.3865\n",
      "Epoch 431: avg loss training: 2.7424,... Gradient Norm: 0.4878\n",
      "Epoch 432: avg loss training: 2.7423,... Gradient Norm: 0.1582\n",
      "Epoch 433: avg loss training: 2.7423,... Gradient Norm: 0.4927\n",
      "Epoch 434: avg loss training: 2.7423,... Gradient Norm: 0.4688\n",
      "Epoch 435: avg loss training: 2.7423,... Gradient Norm: 0.1519\n",
      "Epoch 436: avg loss training: 2.7423,... Gradient Norm: 0.5233\n",
      "Epoch 437: avg loss training: 2.7423,... Gradient Norm: 0.4925\n",
      "Epoch 438: avg loss training: 2.7423,... Gradient Norm: 0.1941\n",
      "Epoch 439: avg loss training: 2.7423,... Gradient Norm: 0.3493\n",
      "Epoch 440: avg loss training: 2.7422,... Gradient Norm: 0.2783\n",
      "Epoch 441: avg loss training: 2.7422,... Gradient Norm: 0.2350\n",
      "Epoch 442: avg loss training: 2.7422,... Gradient Norm: 0.3049\n",
      "Epoch 443: avg loss training: 2.7422,... Gradient Norm: 0.1456\n",
      "Epoch 444: avg loss training: 2.7422,... Gradient Norm: 0.3267\n",
      "Epoch 445: avg loss training: 2.7422,... Gradient Norm: 0.3473\n",
      "Epoch 446: avg loss training: 2.7421,... Gradient Norm: 0.1894\n",
      "Epoch 447: avg loss training: 2.7422,... Gradient Norm: 0.4396\n",
      "Epoch 448: avg loss training: 2.7421,... Gradient Norm: 0.3905\n",
      "Epoch 449: avg loss training: 2.7421,... Gradient Norm: 0.1456\n",
      "Epoch 450: avg loss training: 2.7421,... Gradient Norm: 0.4762\n",
      "Epoch 451: avg loss training: 2.7421,... Gradient Norm: 0.4625\n",
      "Epoch 452: avg loss training: 2.7421,... Gradient Norm: 0.2268\n",
      "Epoch 453: avg loss training: 2.7421,... Gradient Norm: 0.4720\n",
      "Epoch 454: avg loss training: 2.7421,... Gradient Norm: 0.5049\n",
      "Epoch 455: avg loss training: 2.7420,... Gradient Norm: 0.2626\n",
      "Epoch 456: avg loss training: 2.7420,... Gradient Norm: 0.3933\n",
      "Epoch 457: avg loss training: 2.7420,... Gradient Norm: 0.4649\n",
      "Epoch 458: avg loss training: 2.7420,... Gradient Norm: 0.2854\n",
      "Epoch 459: avg loss training: 2.7420,... Gradient Norm: 0.3776\n",
      "Epoch 460: avg loss training: 2.7420,... Gradient Norm: 0.4459\n",
      "Epoch 461: avg loss training: 2.7420,... Gradient Norm: 0.2544\n",
      "Epoch 462: avg loss training: 2.7420,... Gradient Norm: 0.3406\n",
      "Epoch 463: avg loss training: 2.7420,... Gradient Norm: 0.4454\n",
      "Epoch 464: avg loss training: 2.7419,... Gradient Norm: 0.2838\n",
      "Epoch 465: avg loss training: 2.7419,... Gradient Norm: 0.3043\n",
      "Epoch 466: avg loss training: 2.7419,... Gradient Norm: 0.4555\n",
      "Epoch 467: avg loss training: 2.7419,... Gradient Norm: 0.3073\n",
      "Epoch 468: avg loss training: 2.7419,... Gradient Norm: 0.3202\n",
      "Epoch 469: avg loss training: 2.7419,... Gradient Norm: 0.4069\n",
      "Epoch 470: avg loss training: 2.7419,... Gradient Norm: 0.3465\n",
      "Epoch 471: avg loss training: 2.7419,... Gradient Norm: 0.2539\n",
      "Epoch 472: avg loss training: 2.7419,... Gradient Norm: 0.3998\n",
      "Epoch 473: avg loss training: 2.7418,... Gradient Norm: 0.3192\n",
      "Epoch 474: avg loss training: 2.7418,... Gradient Norm: 0.2804\n",
      "Epoch 475: avg loss training: 2.7418,... Gradient Norm: 0.3517\n",
      "Epoch 476: avg loss training: 2.7418,... Gradient Norm: 0.2541\n",
      "Epoch 477: avg loss training: 2.7418,... Gradient Norm: 0.3925\n",
      "Epoch 478: avg loss training: 2.7418,... Gradient Norm: 0.3278\n",
      "Epoch 479: avg loss training: 2.7418,... Gradient Norm: 0.2018\n",
      "Epoch 480: avg loss training: 2.7418,... Gradient Norm: 0.3436\n",
      "Epoch 481: avg loss training: 2.7418,... Gradient Norm: 0.3805\n",
      "Epoch 482: avg loss training: 2.7418,... Gradient Norm: 0.2160\n",
      "Epoch 483: avg loss training: 2.7418,... Gradient Norm: 0.2698\n",
      "Epoch 484: avg loss training: 2.7418,... Gradient Norm: 0.2051\n",
      "Epoch 485: avg loss training: 2.7417,... Gradient Norm: 0.1739\n",
      "Epoch 486: avg loss training: 2.7417,... Gradient Norm: 0.3555\n",
      "Epoch 487: avg loss training: 2.7417,... Gradient Norm: 0.2857\n",
      "Epoch 488: avg loss training: 2.7417,... Gradient Norm: 0.3561\n",
      "Epoch 489: avg loss training: 2.7417,... Gradient Norm: 0.3258\n",
      "Epoch 490: avg loss training: 2.7417,... Gradient Norm: 0.2392\n",
      "Epoch 491: avg loss training: 2.7417,... Gradient Norm: 0.2950\n",
      "Epoch 492: avg loss training: 2.7417,... Gradient Norm: 0.2638\n",
      "Epoch 493: avg loss training: 2.7417,... Gradient Norm: 0.2882\n",
      "Epoch 494: avg loss training: 2.7417,... Gradient Norm: 0.2786\n",
      "Epoch 495: avg loss training: 2.7417,... Gradient Norm: 0.1854\n",
      "Epoch 496: avg loss training: 2.7417,... Gradient Norm: 0.2315\n",
      "Epoch 497: avg loss training: 2.7416,... Gradient Norm: 0.1965\n",
      "Epoch 498: avg loss training: 2.7416,... Gradient Norm: 0.1836\n",
      "Epoch 499: avg loss training: 2.7416,... Gradient Norm: 0.2872\n",
      "Epoch 500: avg loss training: 2.7416,... Gradient Norm: 0.1665\n",
      "Epoch 501: avg loss training: 2.7416,... Gradient Norm: 0.2572\n",
      "Epoch 502: avg loss training: 2.7416,... Gradient Norm: 0.1530\n",
      "Epoch 503: avg loss training: 2.7416,... Gradient Norm: 0.2373\n",
      "Epoch 504: avg loss training: 2.7416,... Gradient Norm: 0.2543\n",
      "Epoch 505: avg loss training: 2.7416,... Gradient Norm: 0.1319\n",
      "Epoch 506: avg loss training: 2.7416,... Gradient Norm: 0.1791\n",
      "Epoch 507: avg loss training: 2.7416,... Gradient Norm: 0.2325\n",
      "Epoch 508: avg loss training: 2.7416,... Gradient Norm: 0.1915\n",
      "Epoch 509: avg loss training: 2.7416,... Gradient Norm: 0.3607\n",
      "Epoch 510: avg loss training: 2.7416,... Gradient Norm: 0.2567\n",
      "Epoch 511: avg loss training: 2.7416,... Gradient Norm: 0.2935\n",
      "Epoch 512: avg loss training: 2.7415,... Gradient Norm: 0.2545\n",
      "Epoch 513: avg loss training: 2.7415,... Gradient Norm: 0.3000\n",
      "Epoch 514: avg loss training: 2.7415,... Gradient Norm: 0.2131\n",
      "Epoch 515: avg loss training: 2.7415,... Gradient Norm: 0.2131\n",
      "Epoch 516: avg loss training: 2.7415,... Gradient Norm: 0.1305\n",
      "Epoch 517: avg loss training: 2.7415,... Gradient Norm: 0.2104\n",
      "Epoch 518: avg loss training: 2.7415,... Gradient Norm: 0.3433\n",
      "Epoch 519: avg loss training: 2.7415,... Gradient Norm: 0.2546\n",
      "Epoch 520: avg loss training: 2.7415,... Gradient Norm: 0.3210\n",
      "Epoch 521: avg loss training: 2.7415,... Gradient Norm: 0.2898\n",
      "Epoch 522: avg loss training: 2.7415,... Gradient Norm: 0.2410\n",
      "Epoch 523: avg loss training: 2.7415,... Gradient Norm: 0.3668\n",
      "Epoch 524: avg loss training: 2.7415,... Gradient Norm: 0.1069\n",
      "Epoch 525: avg loss training: 2.7415,... Gradient Norm: 0.2664\n",
      "Epoch 526: avg loss training: 2.7415,... Gradient Norm: 0.2072\n",
      "Epoch 527: avg loss training: 2.7415,... Gradient Norm: 0.2344\n",
      "Epoch 528: avg loss training: 2.7415,... Gradient Norm: 0.3084\n",
      "Epoch 529: avg loss training: 2.7414,... Gradient Norm: 0.1818\n",
      "Epoch 530: avg loss training: 2.7414,... Gradient Norm: 0.3107\n",
      "Epoch 531: avg loss training: 2.7414,... Gradient Norm: 0.3178\n",
      "Epoch 532: avg loss training: 2.7414,... Gradient Norm: 0.0909\n",
      "Epoch 533: avg loss training: 2.7414,... Gradient Norm: 0.1829\n",
      "Epoch 534: avg loss training: 2.7414,... Gradient Norm: 0.1980\n",
      "Epoch 535: avg loss training: 2.7414,... Gradient Norm: 0.0716\n",
      "Epoch 536: avg loss training: 2.7414,... Gradient Norm: 0.2246\n",
      "Epoch 537: avg loss training: 2.7414,... Gradient Norm: 0.1952\n",
      "Epoch 538: avg loss training: 2.7414,... Gradient Norm: 0.2434\n",
      "Epoch 539: avg loss training: 2.7414,... Gradient Norm: 0.1949\n",
      "Epoch 540: avg loss training: 2.7414,... Gradient Norm: 0.1989\n",
      "Epoch 541: avg loss training: 2.7414,... Gradient Norm: 0.1610\n",
      "Epoch 542: avg loss training: 2.7414,... Gradient Norm: 0.0989\n",
      "Epoch 543: avg loss training: 2.7414,... Gradient Norm: 0.1337\n",
      "Epoch 544: avg loss training: 2.7414,... Gradient Norm: 0.1537\n",
      "Epoch 545: avg loss training: 2.7414,... Gradient Norm: 0.2575\n",
      "Epoch 546: avg loss training: 2.7414,... Gradient Norm: 0.2194\n",
      "Epoch 547: avg loss training: 2.7414,... Gradient Norm: 0.1936\n",
      "Epoch 548: avg loss training: 2.7414,... Gradient Norm: 0.2099\n",
      "Epoch 549: avg loss training: 2.7413,... Gradient Norm: 0.0988\n",
      "Epoch 550: avg loss training: 2.7413,... Gradient Norm: 0.2974\n",
      "Epoch 551: avg loss training: 2.7413,... Gradient Norm: 0.2319\n",
      "Epoch 552: avg loss training: 2.7413,... Gradient Norm: 0.3239\n",
      "Epoch 553: avg loss training: 2.7413,... Gradient Norm: 0.2701\n",
      "Epoch 554: avg loss training: 2.7413,... Gradient Norm: 0.1990\n",
      "Epoch 555: avg loss training: 2.7413,... Gradient Norm: 0.2996\n",
      "Epoch 556: avg loss training: 2.7413,... Gradient Norm: 0.3540\n",
      "Epoch 557: avg loss training: 2.7413,... Gradient Norm: 0.1611\n",
      "Epoch 558: avg loss training: 2.7413,... Gradient Norm: 0.2500\n",
      "Epoch 559: avg loss training: 2.7413,... Gradient Norm: 0.2697\n",
      "Epoch 560: avg loss training: 2.7413,... Gradient Norm: 0.1853\n",
      "Epoch 561: avg loss training: 2.7413,... Gradient Norm: 0.2533\n",
      "Epoch 562: avg loss training: 2.7413,... Gradient Norm: 0.2005\n",
      "Epoch 563: avg loss training: 2.7413,... Gradient Norm: 0.1865\n",
      "Epoch 564: avg loss training: 2.7413,... Gradient Norm: 0.2743\n",
      "Epoch 565: avg loss training: 2.7413,... Gradient Norm: 0.1013\n",
      "Epoch 566: avg loss training: 2.7413,... Gradient Norm: 0.3114\n",
      "Epoch 567: avg loss training: 2.7413,... Gradient Norm: 0.2378\n",
      "Epoch 568: avg loss training: 2.7413,... Gradient Norm: 0.2079\n",
      "Epoch 569: avg loss training: 2.7413,... Gradient Norm: 0.1641\n",
      "Epoch 570: avg loss training: 2.7413,... Gradient Norm: 0.2459\n",
      "Epoch 571: avg loss training: 2.7413,... Gradient Norm: 0.2906\n",
      "Epoch 572: avg loss training: 2.7413,... Gradient Norm: 0.2011\n",
      "Epoch 573: avg loss training: 2.7413,... Gradient Norm: 0.2614\n",
      "Epoch 574: avg loss training: 2.7412,... Gradient Norm: 0.2016\n",
      "Epoch 575: avg loss training: 2.7412,... Gradient Norm: 0.0686\n",
      "Epoch 576: avg loss training: 2.7412,... Gradient Norm: 0.3147\n",
      "Epoch 577: avg loss training: 2.7412,... Gradient Norm: 0.2295\n",
      "Epoch 578: avg loss training: 2.7412,... Gradient Norm: 0.1810\n",
      "Epoch 579: avg loss training: 2.7412,... Gradient Norm: 0.1783\n",
      "Epoch 580: avg loss training: 2.7412,... Gradient Norm: 0.2473\n",
      "Epoch 581: avg loss training: 2.7412,... Gradient Norm: 0.3246\n",
      "Epoch 582: avg loss training: 2.7412,... Gradient Norm: 0.0704\n",
      "Epoch 583: avg loss training: 2.7412,... Gradient Norm: 0.1522\n",
      "Epoch 584: avg loss training: 2.7412,... Gradient Norm: 0.1484\n",
      "Epoch 585: avg loss training: 2.7412,... Gradient Norm: 0.0656\n",
      "Epoch 586: avg loss training: 2.7412,... Gradient Norm: 0.2706\n",
      "Epoch 587: avg loss training: 2.7412,... Gradient Norm: 0.1005\n",
      "Epoch 588: avg loss training: 2.7412,... Gradient Norm: 0.1627\n",
      "Epoch 589: avg loss training: 2.7412,... Gradient Norm: 0.0711\n",
      "Epoch 590: avg loss training: 2.7412,... Gradient Norm: 0.1625\n",
      "Epoch 591: avg loss training: 2.7412,... Gradient Norm: 0.2455\n",
      "Epoch 592: avg loss training: 2.7412,... Gradient Norm: 0.2459\n",
      "Epoch 593: avg loss training: 2.7412,... Gradient Norm: 0.2220\n",
      "Epoch 594: avg loss training: 2.7412,... Gradient Norm: 0.1603\n",
      "Epoch 595: avg loss training: 2.7412,... Gradient Norm: 0.1114\n",
      "Epoch 596: avg loss training: 2.7412,... Gradient Norm: 0.3179\n",
      "Epoch 597: avg loss training: 2.7412,... Gradient Norm: 0.1487\n",
      "Epoch 598: avg loss training: 2.7412,... Gradient Norm: 0.1700\n",
      "Epoch 599: avg loss training: 2.7412,... Gradient Norm: 0.1544\n",
      "Epoch 600: avg loss training: 2.7412,... Gradient Norm: 0.0808\n",
      "Epoch 601: avg loss training: 2.7412,... Gradient Norm: 0.2694\n",
      "Epoch 602: avg loss training: 2.7412,... Gradient Norm: 0.0580\n",
      "Epoch 603: avg loss training: 2.7412,... Gradient Norm: 0.1638\n",
      "Epoch 604: avg loss training: 2.7411,... Gradient Norm: 0.1466\n",
      "Epoch 605: avg loss training: 2.7411,... Gradient Norm: 0.1143\n",
      "Epoch 606: avg loss training: 2.7411,... Gradient Norm: 0.1690\n",
      "Epoch 607: avg loss training: 2.7411,... Gradient Norm: 0.2348\n",
      "Epoch 608: avg loss training: 2.7411,... Gradient Norm: 0.1861\n",
      "Epoch 609: avg loss training: 2.7411,... Gradient Norm: 0.2055\n",
      "Epoch 610: avg loss training: 2.7411,... Gradient Norm: 0.1403\n",
      "Epoch 611: avg loss training: 2.7411,... Gradient Norm: 0.2205\n",
      "Epoch 612: avg loss training: 2.7411,... Gradient Norm: 0.2598\n",
      "Epoch 613: avg loss training: 2.7411,... Gradient Norm: 0.2031\n",
      "Epoch 614: avg loss training: 2.7411,... Gradient Norm: 0.1222\n",
      "Epoch 615: avg loss training: 2.7411,... Gradient Norm: 0.0949\n",
      "Epoch 616: avg loss training: 2.7411,... Gradient Norm: 0.0984\n",
      "Epoch 617: avg loss training: 2.7411,... Gradient Norm: 0.2313\n",
      "Epoch 618: avg loss training: 2.7411,... Gradient Norm: 0.1217\n",
      "Epoch 619: avg loss training: 2.7411,... Gradient Norm: 0.1304\n",
      "Epoch 620: avg loss training: 2.7411,... Gradient Norm: 0.0567\n",
      "Epoch 621: avg loss training: 2.7411,... Gradient Norm: 0.1390\n",
      "Epoch 622: avg loss training: 2.7411,... Gradient Norm: 0.0754\n",
      "Epoch 623: avg loss training: 2.7411,... Gradient Norm: 0.0468\n",
      "Epoch 624: avg loss training: 2.7411,... Gradient Norm: 0.1236\n",
      "Epoch 625: avg loss training: 2.7411,... Gradient Norm: 0.1637\n",
      "Epoch 626: avg loss training: 2.7411,... Gradient Norm: 0.2387\n",
      "Epoch 627: avg loss training: 2.7411,... Gradient Norm: 0.2585\n",
      "Epoch 628: avg loss training: 2.7411,... Gradient Norm: 0.1467\n",
      "Epoch 629: avg loss training: 2.7411,... Gradient Norm: 0.2023\n",
      "Epoch 630: avg loss training: 2.7411,... Gradient Norm: 0.1761\n",
      "Epoch 631: avg loss training: 2.7411,... Gradient Norm: 0.2778\n",
      "Epoch 632: avg loss training: 2.7411,... Gradient Norm: 0.2710\n",
      "Epoch 633: avg loss training: 2.7411,... Gradient Norm: 0.2627\n",
      "Epoch 634: avg loss training: 2.7411,... Gradient Norm: 0.2243\n",
      "Epoch 635: avg loss training: 2.7411,... Gradient Norm: 0.3698\n",
      "Epoch 636: avg loss training: 2.7411,... Gradient Norm: 0.1327\n",
      "Epoch 637: avg loss training: 2.7411,... Gradient Norm: 0.2608\n",
      "Epoch 638: avg loss training: 2.7411,... Gradient Norm: 0.2589\n",
      "Epoch 639: avg loss training: 2.7411,... Gradient Norm: 0.1639\n",
      "Epoch 640: avg loss training: 2.7411,... Gradient Norm: 0.1671\n",
      "Epoch 641: avg loss training: 2.7411,... Gradient Norm: 0.3246\n",
      "Epoch 642: avg loss training: 2.7411,... Gradient Norm: 0.2298\n",
      "Epoch 643: avg loss training: 2.7411,... Gradient Norm: 0.2534\n",
      "Epoch 644: avg loss training: 2.7410,... Gradient Norm: 0.0916\n",
      "Epoch 645: avg loss training: 2.7410,... Gradient Norm: 0.3416\n",
      "Epoch 646: avg loss training: 2.7410,... Gradient Norm: 0.1513\n",
      "Epoch 647: avg loss training: 2.7410,... Gradient Norm: 0.2159\n",
      "Epoch 648: avg loss training: 2.7410,... Gradient Norm: 0.1352\n",
      "Epoch 649: avg loss training: 2.7410,... Gradient Norm: 0.0726\n",
      "Epoch 650: avg loss training: 2.7410,... Gradient Norm: 0.2887\n",
      "Epoch 651: avg loss training: 2.7410,... Gradient Norm: 0.1047\n",
      "Epoch 652: avg loss training: 2.7410,... Gradient Norm: 0.2280\n",
      "Epoch 653: avg loss training: 2.7410,... Gradient Norm: 0.2390\n",
      "Epoch 654: avg loss training: 2.7410,... Gradient Norm: 0.1942\n",
      "Epoch 655: avg loss training: 2.7410,... Gradient Norm: 0.1034\n",
      "Epoch 656: avg loss training: 2.7410,... Gradient Norm: 0.0925\n",
      "Epoch 657: avg loss training: 2.7410,... Gradient Norm: 0.0943\n",
      "Epoch 658: avg loss training: 2.7410,... Gradient Norm: 0.0612\n",
      "Epoch 659: avg loss training: 2.7410,... Gradient Norm: 0.2944\n",
      "Epoch 660: avg loss training: 2.7410,... Gradient Norm: 0.1803\n",
      "Epoch 661: avg loss training: 2.7410,... Gradient Norm: 0.2703\n",
      "Epoch 662: avg loss training: 2.7410,... Gradient Norm: 0.1192\n",
      "Epoch 663: avg loss training: 2.7410,... Gradient Norm: 0.3435\n",
      "Epoch 664: avg loss training: 2.7410,... Gradient Norm: 0.0986\n",
      "Epoch 665: avg loss training: 2.7410,... Gradient Norm: 0.0870\n",
      "Epoch 666: avg loss training: 2.7410,... Gradient Norm: 0.1050\n",
      "Epoch 667: avg loss training: 2.7410,... Gradient Norm: 0.1438\n",
      "Epoch 668: avg loss training: 2.7410,... Gradient Norm: 0.0543\n",
      "Epoch 669: avg loss training: 2.7410,... Gradient Norm: 0.3202\n",
      "Epoch 670: avg loss training: 2.7410,... Gradient Norm: 0.1269\n",
      "Epoch 671: avg loss training: 2.7410,... Gradient Norm: 0.2581\n",
      "Epoch 672: avg loss training: 2.7410,... Gradient Norm: 0.0843\n",
      "Epoch 673: avg loss training: 2.7410,... Gradient Norm: 0.0892\n",
      "Epoch 674: avg loss training: 2.7410,... Gradient Norm: 0.3565\n",
      "Epoch 675: avg loss training: 2.7410,... Gradient Norm: 0.0725\n",
      "Epoch 676: avg loss training: 2.7410,... Gradient Norm: 0.1610\n",
      "Epoch 677: avg loss training: 2.7410,... Gradient Norm: 0.1730\n",
      "Epoch 678: avg loss training: 2.7410,... Gradient Norm: 0.0962\n",
      "Epoch 679: avg loss training: 2.7410,... Gradient Norm: 0.2641\n",
      "Epoch 680: avg loss training: 2.7410,... Gradient Norm: 0.0629\n",
      "Epoch 681: avg loss training: 2.7410,... Gradient Norm: 0.1938\n",
      "Epoch 682: avg loss training: 2.7410,... Gradient Norm: 0.1415\n",
      "Epoch 683: avg loss training: 2.7410,... Gradient Norm: 0.0979\n",
      "Epoch 684: avg loss training: 2.7410,... Gradient Norm: 0.1015\n",
      "Epoch 685: avg loss training: 2.7410,... Gradient Norm: 0.2486\n",
      "Epoch 686: avg loss training: 2.7410,... Gradient Norm: 0.1217\n",
      "Epoch 687: avg loss training: 2.7410,... Gradient Norm: 0.1370\n",
      "Epoch 688: avg loss training: 2.7410,... Gradient Norm: 0.0755\n",
      "Epoch 689: avg loss training: 2.7410,... Gradient Norm: 0.0652\n",
      "Epoch 690: avg loss training: 2.7410,... Gradient Norm: 0.0846\n",
      "Epoch 691: avg loss training: 2.7410,... Gradient Norm: 0.2443\n",
      "Epoch 692: avg loss training: 2.7410,... Gradient Norm: 0.1219\n",
      "Epoch 693: avg loss training: 2.7410,... Gradient Norm: 0.1585\n",
      "Epoch 694: avg loss training: 2.7410,... Gradient Norm: 0.1292\n",
      "Epoch 695: avg loss training: 2.7410,... Gradient Norm: 0.1613\n",
      "Epoch 696: avg loss training: 2.7410,... Gradient Norm: 0.3043\n",
      "Epoch 697: avg loss training: 2.7410,... Gradient Norm: 0.1101\n",
      "Epoch 698: avg loss training: 2.7410,... Gradient Norm: 0.2512\n",
      "Epoch 699: avg loss training: 2.7410,... Gradient Norm: 0.1273\n",
      "Epoch 700: avg loss training: 2.7410,... Gradient Norm: 0.0674\n",
      "Epoch 701: avg loss training: 2.7410,... Gradient Norm: 0.3370\n",
      "Epoch 702: avg loss training: 2.7410,... Gradient Norm: 0.0667\n",
      "Epoch 703: avg loss training: 2.7410,... Gradient Norm: 0.1326\n",
      "Epoch 704: avg loss training: 2.7410,... Gradient Norm: 0.1294\n",
      "Epoch 705: avg loss training: 2.7410,... Gradient Norm: 0.0924\n",
      "Epoch 706: avg loss training: 2.7409,... Gradient Norm: 0.0995\n",
      "Epoch 707: avg loss training: 2.7409,... Gradient Norm: 0.2576\n",
      "Epoch 708: avg loss training: 2.7409,... Gradient Norm: 0.1075\n",
      "Epoch 709: avg loss training: 2.7409,... Gradient Norm: 0.1417\n",
      "Epoch 710: avg loss training: 2.7409,... Gradient Norm: 0.1317\n",
      "Epoch 711: avg loss training: 2.7409,... Gradient Norm: 0.0660\n",
      "Epoch 712: avg loss training: 2.7409,... Gradient Norm: 0.2460\n",
      "Epoch 713: avg loss training: 2.7409,... Gradient Norm: 0.0868\n",
      "Epoch 714: avg loss training: 2.7409,... Gradient Norm: 0.0719\n",
      "Epoch 715: avg loss training: 2.7409,... Gradient Norm: 0.1518\n",
      "Epoch 716: avg loss training: 2.7409,... Gradient Norm: 0.0688\n",
      "Epoch 717: avg loss training: 2.7409,... Gradient Norm: 0.1503\n",
      "Epoch 718: avg loss training: 2.7409,... Gradient Norm: 0.0819\n",
      "Epoch 719: avg loss training: 2.7409,... Gradient Norm: 0.2435\n",
      "Epoch 720: avg loss training: 2.7409,... Gradient Norm: 0.1159\n",
      "Epoch 721: avg loss training: 2.7409,... Gradient Norm: 0.1562\n",
      "Epoch 722: avg loss training: 2.7409,... Gradient Norm: 0.1418\n",
      "Epoch 723: avg loss training: 2.7409,... Gradient Norm: 0.0814\n",
      "Epoch 724: avg loss training: 2.7409,... Gradient Norm: 0.2670\n",
      "Epoch 725: avg loss training: 2.7409,... Gradient Norm: 0.0723\n",
      "Epoch 726: avg loss training: 2.7409,... Gradient Norm: 0.0734\n",
      "Epoch 727: avg loss training: 2.7409,... Gradient Norm: 0.1417\n",
      "Epoch 728: avg loss training: 2.7409,... Gradient Norm: 0.1002\n",
      "Epoch 729: avg loss training: 2.7409,... Gradient Norm: 0.0676\n",
      "Epoch 730: avg loss training: 2.7409,... Gradient Norm: 0.2484\n",
      "Epoch 731: avg loss training: 2.7409,... Gradient Norm: 0.0657\n",
      "Epoch 732: avg loss training: 2.7409,... Gradient Norm: 0.1090\n",
      "Epoch 733: avg loss training: 2.7409,... Gradient Norm: 0.1084\n",
      "Epoch 734: avg loss training: 2.7409,... Gradient Norm: 0.1240\n",
      "Epoch 735: avg loss training: 2.7409,... Gradient Norm: 0.0827\n",
      "Epoch 736: avg loss training: 2.7409,... Gradient Norm: 0.0900\n",
      "Epoch 737: avg loss training: 2.7409,... Gradient Norm: 0.0795\n",
      "Epoch 738: avg loss training: 2.7409,... Gradient Norm: 0.0813\n",
      "Epoch 739: avg loss training: 2.7409,... Gradient Norm: 0.1780\n",
      "Epoch 740: avg loss training: 2.7409,... Gradient Norm: 0.0521\n",
      "Epoch 741: avg loss training: 2.7409,... Gradient Norm: 0.0845\n",
      "Epoch 742: avg loss training: 2.7409,... Gradient Norm: 0.0806\n",
      "Epoch 743: avg loss training: 2.7409,... Gradient Norm: 0.0701\n",
      "Epoch 744: avg loss training: 2.7409,... Gradient Norm: 0.0805\n",
      "Epoch 745: avg loss training: 2.7409,... Gradient Norm: 0.1157\n",
      "Epoch 746: avg loss training: 2.7409,... Gradient Norm: 0.0409\n",
      "Epoch 747: avg loss training: 2.7409,... Gradient Norm: 0.0619\n",
      "Epoch 748: avg loss training: 2.7409,... Gradient Norm: 0.0556\n",
      "Epoch 749: avg loss training: 2.7409,... Gradient Norm: 0.0834\n",
      "Epoch 750: avg loss training: 2.7409,... Gradient Norm: 0.2381\n",
      "Epoch 751: avg loss training: 2.7409,... Gradient Norm: 0.1053\n",
      "Epoch 752: avg loss training: 2.7409,... Gradient Norm: 0.1060\n",
      "Epoch 753: avg loss training: 2.7409,... Gradient Norm: 0.0671\n",
      "Epoch 754: avg loss training: 2.7409,... Gradient Norm: 0.0702\n",
      "Epoch 755: avg loss training: 2.7409,... Gradient Norm: 0.1436\n",
      "Epoch 756: avg loss training: 2.7409,... Gradient Norm: 0.1245\n",
      "Epoch 757: avg loss training: 2.7409,... Gradient Norm: 0.0872\n",
      "Epoch 758: avg loss training: 2.7409,... Gradient Norm: 0.1113\n",
      "Epoch 759: avg loss training: 2.7409,... Gradient Norm: 0.0828\n",
      "Epoch 760: avg loss training: 2.7409,... Gradient Norm: 0.2661\n",
      "Epoch 761: avg loss training: 2.7409,... Gradient Norm: 0.0610\n",
      "Epoch 762: avg loss training: 2.7409,... Gradient Norm: 0.1106\n",
      "Epoch 763: avg loss training: 2.7409,... Gradient Norm: 0.0738\n",
      "Epoch 764: avg loss training: 2.7409,... Gradient Norm: 0.0636\n",
      "Epoch 765: avg loss training: 2.7409,... Gradient Norm: 0.0701\n",
      "Epoch 766: avg loss training: 2.7409,... Gradient Norm: 0.3144\n",
      "Epoch 767: avg loss training: 2.7409,... Gradient Norm: 0.1232\n",
      "Epoch 768: avg loss training: 2.7409,... Gradient Norm: 0.1441\n",
      "Epoch 769: avg loss training: 2.7409,... Gradient Norm: 0.1284\n",
      "Epoch 770: avg loss training: 2.7409,... Gradient Norm: 0.0968\n",
      "Epoch 771: avg loss training: 2.7409,... Gradient Norm: 0.0915\n",
      "Epoch 772: avg loss training: 2.7409,... Gradient Norm: 0.2832\n",
      "Epoch 773: avg loss training: 2.7409,... Gradient Norm: 0.0618\n",
      "Epoch 774: avg loss training: 2.7409,... Gradient Norm: 0.1383\n",
      "Epoch 775: avg loss training: 2.7409,... Gradient Norm: 0.1375\n",
      "Epoch 776: avg loss training: 2.7409,... Gradient Norm: 0.0681\n",
      "Epoch 777: avg loss training: 2.7409,... Gradient Norm: 0.0682\n",
      "Epoch 778: avg loss training: 2.7409,... Gradient Norm: 0.1511\n",
      "Epoch 779: avg loss training: 2.7409,... Gradient Norm: 0.0852\n",
      "Epoch 780: avg loss training: 2.7409,... Gradient Norm: 0.0711\n",
      "Epoch 781: avg loss training: 2.7409,... Gradient Norm: 0.1102\n",
      "Epoch 782: avg loss training: 2.7409,... Gradient Norm: 0.2047\n",
      "Epoch 783: avg loss training: 2.7409,... Gradient Norm: 0.0709\n",
      "Epoch 784: avg loss training: 2.7409,... Gradient Norm: 0.0835\n",
      "Epoch 785: avg loss training: 2.7409,... Gradient Norm: 0.0859\n",
      "Epoch 786: avg loss training: 2.7409,... Gradient Norm: 0.0646\n",
      "Epoch 787: avg loss training: 2.7409,... Gradient Norm: 0.0563\n",
      "Epoch 788: avg loss training: 2.7409,... Gradient Norm: 0.1790\n",
      "Epoch 789: avg loss training: 2.7409,... Gradient Norm: 0.1938\n",
      "Epoch 790: avg loss training: 2.7409,... Gradient Norm: 0.1331\n",
      "Epoch 791: avg loss training: 2.7409,... Gradient Norm: 0.1176\n",
      "Epoch 792: avg loss training: 2.7409,... Gradient Norm: 0.0969\n",
      "Epoch 793: avg loss training: 2.7409,... Gradient Norm: 0.0919\n",
      "Epoch 794: avg loss training: 2.7409,... Gradient Norm: 0.0806\n",
      "Epoch 795: avg loss training: 2.7409,... Gradient Norm: 0.2565\n",
      "Epoch 796: avg loss training: 2.7409,... Gradient Norm: 0.1217\n",
      "Epoch 797: avg loss training: 2.7409,... Gradient Norm: 0.1392\n",
      "Epoch 798: avg loss training: 2.7409,... Gradient Norm: 0.1197\n",
      "Epoch 799: avg loss training: 2.7409,... Gradient Norm: 0.0573\n",
      "Epoch 800: avg loss training: 2.7409,... Gradient Norm: 0.0753\n",
      "Epoch 801: avg loss training: 2.7409,... Gradient Norm: 0.2565\n",
      "Epoch 802: avg loss training: 2.7409,... Gradient Norm: 0.0828\n",
      "Epoch 803: avg loss training: 2.7409,... Gradient Norm: 0.0825\n",
      "Epoch 804: avg loss training: 2.7409,... Gradient Norm: 0.1181\n",
      "Epoch 805: avg loss training: 2.7409,... Gradient Norm: 0.0957\n",
      "Epoch 806: avg loss training: 2.7409,... Gradient Norm: 0.0511\n",
      "Epoch 807: avg loss training: 2.7409,... Gradient Norm: 0.3136\n",
      "Epoch 808: avg loss training: 2.7409,... Gradient Norm: 0.0628\n",
      "Epoch 809: avg loss training: 2.7409,... Gradient Norm: 0.1058\n",
      "Epoch 810: avg loss training: 2.7409,... Gradient Norm: 0.1128\n",
      "Epoch 811: avg loss training: 2.7409,... Gradient Norm: 0.0843\n",
      "Epoch 812: avg loss training: 2.7409,... Gradient Norm: 0.0743\n",
      "Epoch 813: avg loss training: 2.7409,... Gradient Norm: 0.1364\n",
      "Epoch 814: avg loss training: 2.7409,... Gradient Norm: 0.0691\n",
      "Epoch 815: avg loss training: 2.7409,... Gradient Norm: 0.1244\n",
      "Epoch 816: avg loss training: 2.7409,... Gradient Norm: 0.1089\n",
      "Epoch 817: avg loss training: 2.7409,... Gradient Norm: 0.2499\n",
      "Epoch 818: avg loss training: 2.7409,... Gradient Norm: 0.0659\n",
      "Epoch 819: avg loss training: 2.7409,... Gradient Norm: 0.1061\n",
      "Epoch 820: avg loss training: 2.7409,... Gradient Norm: 0.1134\n",
      "Epoch 821: avg loss training: 2.7409,... Gradient Norm: 0.0748\n",
      "Epoch 822: avg loss training: 2.7409,... Gradient Norm: 0.0657\n",
      "Epoch 823: avg loss training: 2.7409,... Gradient Norm: 0.0653\n",
      "Epoch 824: avg loss training: 2.7409,... Gradient Norm: 0.2302\n",
      "Epoch 825: avg loss training: 2.7409,... Gradient Norm: 0.1066\n",
      "Epoch 826: avg loss training: 2.7409,... Gradient Norm: 0.1140\n",
      "Epoch 827: avg loss training: 2.7409,... Gradient Norm: 0.2343\n",
      "Epoch 828: avg loss training: 2.7409,... Gradient Norm: 0.1147\n",
      "Epoch 829: avg loss training: 2.7409,... Gradient Norm: 0.1001\n",
      "Epoch 830: avg loss training: 2.7409,... Gradient Norm: 0.0940\n",
      "Epoch 831: avg loss training: 2.7409,... Gradient Norm: 0.0682\n",
      "Epoch 832: avg loss training: 2.7409,... Gradient Norm: 0.0494\n",
      "Epoch 833: avg loss training: 2.7409,... Gradient Norm: 0.0746\n",
      "Epoch 834: avg loss training: 2.7409,... Gradient Norm: 0.3202\n",
      "Epoch 835: avg loss training: 2.7409,... Gradient Norm: 0.1153\n",
      "Epoch 836: avg loss training: 2.7409,... Gradient Norm: 0.1124\n",
      "Epoch 837: avg loss training: 2.7409,... Gradient Norm: 0.0958\n",
      "Epoch 838: avg loss training: 2.7409,... Gradient Norm: 0.0961\n",
      "Epoch 839: avg loss training: 2.7409,... Gradient Norm: 0.0981\n",
      "Epoch 840: avg loss training: 2.7409,... Gradient Norm: 0.0814\n",
      "Epoch 841: avg loss training: 2.7409,... Gradient Norm: 0.2172\n",
      "Epoch 842: avg loss training: 2.7409,... Gradient Norm: 0.1283\n",
      "Epoch 843: avg loss training: 2.7409,... Gradient Norm: 0.1322\n",
      "Epoch 844: avg loss training: 2.7408,... Gradient Norm: 0.1262\n",
      "Epoch 845: avg loss training: 2.7408,... Gradient Norm: 0.0777\n",
      "Epoch 846: avg loss training: 2.7408,... Gradient Norm: 0.2608\n",
      "Epoch 847: avg loss training: 2.7408,... Gradient Norm: 0.0620\n",
      "Epoch 848: avg loss training: 2.7408,... Gradient Norm: 0.1108\n",
      "Epoch 849: avg loss training: 2.7408,... Gradient Norm: 0.1145\n",
      "Epoch 850: avg loss training: 2.7408,... Gradient Norm: 0.0678\n",
      "Epoch 851: avg loss training: 2.7408,... Gradient Norm: 0.0670\n",
      "Epoch 852: avg loss training: 2.7408,... Gradient Norm: 0.0725\n",
      "Epoch 853: avg loss training: 2.7408,... Gradient Norm: 0.2679\n",
      "Epoch 854: avg loss training: 2.7408,... Gradient Norm: 0.0840\n",
      "Epoch 855: avg loss training: 2.7408,... Gradient Norm: 0.1228\n",
      "Epoch 856: avg loss training: 2.7408,... Gradient Norm: 0.1191\n",
      "Epoch 857: avg loss training: 2.7408,... Gradient Norm: 0.0727\n",
      "Epoch 858: avg loss training: 2.7408,... Gradient Norm: 0.0682\n",
      "Epoch 859: avg loss training: 2.7408,... Gradient Norm: 0.2469\n",
      "Epoch 860: avg loss training: 2.7408,... Gradient Norm: 0.0578\n",
      "Epoch 861: avg loss training: 2.7408,... Gradient Norm: 0.1114\n",
      "Epoch 862: avg loss training: 2.7408,... Gradient Norm: 0.1144\n",
      "Epoch 863: avg loss training: 2.7408,... Gradient Norm: 0.1118\n",
      "Epoch 864: avg loss training: 2.7408,... Gradient Norm: 0.0556\n",
      "Epoch 865: avg loss training: 2.7408,... Gradient Norm: 0.2545\n",
      "Epoch 866: avg loss training: 2.7408,... Gradient Norm: 0.0655\n",
      "Epoch 867: avg loss training: 2.7408,... Gradient Norm: 0.0555\n",
      "Epoch 868: avg loss training: 2.7408,... Gradient Norm: 0.1342\n",
      "Epoch 869: avg loss training: 2.7408,... Gradient Norm: 0.1005\n",
      "Epoch 870: avg loss training: 2.7408,... Gradient Norm: 0.1049\n",
      "Epoch 871: avg loss training: 2.7408,... Gradient Norm: 0.1375\n",
      "Epoch 872: avg loss training: 2.7408,... Gradient Norm: 0.1248\n",
      "Epoch 873: avg loss training: 2.7408,... Gradient Norm: 0.0902\n",
      "Epoch 874: avg loss training: 2.7408,... Gradient Norm: 0.1095\n",
      "Epoch 875: avg loss training: 2.7408,... Gradient Norm: 0.0537\n",
      "Epoch 876: avg loss training: 2.7408,... Gradient Norm: 0.0620\n",
      "Epoch 877: avg loss training: 2.7408,... Gradient Norm: 0.2395\n",
      "Epoch 878: avg loss training: 2.7408,... Gradient Norm: 0.0557\n",
      "Epoch 879: avg loss training: 2.7408,... Gradient Norm: 0.1215\n",
      "Epoch 880: avg loss training: 2.7408,... Gradient Norm: 0.1208\n",
      "Epoch 881: avg loss training: 2.7408,... Gradient Norm: 0.0586\n",
      "Epoch 882: avg loss training: 2.7408,... Gradient Norm: 0.0641\n",
      "Epoch 883: avg loss training: 2.7408,... Gradient Norm: 0.0753\n",
      "Epoch 884: avg loss training: 2.7408,... Gradient Norm: 0.3227\n",
      "Epoch 885: avg loss training: 2.7408,... Gradient Norm: 0.0758\n",
      "Epoch 886: avg loss training: 2.7408,... Gradient Norm: 0.1433\n",
      "Epoch 887: avg loss training: 2.7408,... Gradient Norm: 0.1296\n",
      "Epoch 888: avg loss training: 2.7408,... Gradient Norm: 0.0890\n",
      "Epoch 889: avg loss training: 2.7408,... Gradient Norm: 0.0708\n",
      "Epoch 890: avg loss training: 2.7408,... Gradient Norm: 0.2579\n",
      "Epoch 891: avg loss training: 2.7408,... Gradient Norm: 0.0469\n",
      "Epoch 892: avg loss training: 2.7408,... Gradient Norm: 0.1270\n",
      "Epoch 893: avg loss training: 2.7408,... Gradient Norm: 0.1206\n",
      "Epoch 894: avg loss training: 2.7408,... Gradient Norm: 0.0613\n",
      "Epoch 895: avg loss training: 2.7408,... Gradient Norm: 0.0648\n",
      "Epoch 896: avg loss training: 2.7408,... Gradient Norm: 0.2554\n",
      "Epoch 897: avg loss training: 2.7408,... Gradient Norm: 0.0648\n",
      "Epoch 898: avg loss training: 2.7408,... Gradient Norm: 0.1251\n",
      "Epoch 899: avg loss training: 2.7408,... Gradient Norm: 0.1449\n",
      "Epoch 900: avg loss training: 2.7408,... Gradient Norm: 0.1042\n",
      "Epoch 901: avg loss training: 2.7408,... Gradient Norm: 0.0708\n",
      "Epoch 902: avg loss training: 2.7408,... Gradient Norm: 0.2542\n",
      "Epoch 903: avg loss training: 2.7408,... Gradient Norm: 0.0779\n",
      "Epoch 904: avg loss training: 2.7408,... Gradient Norm: 0.0604\n",
      "Epoch 905: avg loss training: 2.7408,... Gradient Norm: 0.1172\n",
      "Epoch 906: avg loss training: 2.7408,... Gradient Norm: 0.1243\n",
      "Epoch 907: avg loss training: 2.7408,... Gradient Norm: 0.0862\n",
      "Epoch 908: avg loss training: 2.7408,... Gradient Norm: 0.1456\n",
      "Epoch 909: avg loss training: 2.7408,... Gradient Norm: 0.1572\n",
      "Epoch 910: avg loss training: 2.7408,... Gradient Norm: 0.0641\n",
      "Epoch 911: avg loss training: 2.7408,... Gradient Norm: 0.0855\n",
      "Epoch 912: avg loss training: 2.7408,... Gradient Norm: 0.1218\n",
      "Epoch 913: avg loss training: 2.7408,... Gradient Norm: 0.1506\n",
      "Epoch 914: avg loss training: 2.7408,... Gradient Norm: 0.0618\n",
      "Epoch 915: avg loss training: 2.7408,... Gradient Norm: 0.1461\n",
      "Epoch 916: avg loss training: 2.7408,... Gradient Norm: 0.0644\n",
      "Epoch 917: avg loss training: 2.7408,... Gradient Norm: 0.0695\n",
      "Epoch 918: avg loss training: 2.7408,... Gradient Norm: 0.0908\n",
      "Epoch 919: avg loss training: 2.7408,... Gradient Norm: 0.0452\n",
      "Epoch 920: avg loss training: 2.7408,... Gradient Norm: 0.2346\n",
      "Epoch 921: avg loss training: 2.7408,... Gradient Norm: 0.1074\n",
      "Epoch 922: avg loss training: 2.7408,... Gradient Norm: 0.1105\n",
      "Epoch 923: avg loss training: 2.7408,... Gradient Norm: 0.0742\n",
      "Epoch 924: avg loss training: 2.7408,... Gradient Norm: 0.0656\n",
      "Epoch 925: avg loss training: 2.7408,... Gradient Norm: 0.0678\n",
      "Epoch 926: avg loss training: 2.7408,... Gradient Norm: 0.0745\n",
      "Epoch 927: avg loss training: 2.7408,... Gradient Norm: 0.3031\n",
      "Epoch 928: avg loss training: 2.7408,... Gradient Norm: 0.1086\n",
      "Epoch 929: avg loss training: 2.7408,... Gradient Norm: 0.1610\n",
      "Epoch 930: avg loss training: 2.7408,... Gradient Norm: 0.1322\n",
      "Epoch 931: avg loss training: 2.7408,... Gradient Norm: 0.0949\n",
      "Epoch 932: avg loss training: 2.7408,... Gradient Norm: 0.1438\n",
      "Epoch 933: avg loss training: 2.7408,... Gradient Norm: 0.2237\n",
      "Epoch 934: avg loss training: 2.7408,... Gradient Norm: 0.0980\n",
      "Epoch 935: avg loss training: 2.7408,... Gradient Norm: 0.1170\n",
      "Epoch 936: avg loss training: 2.7408,... Gradient Norm: 0.1181\n",
      "Epoch 937: avg loss training: 2.7408,... Gradient Norm: 0.0589\n",
      "Epoch 938: avg loss training: 2.7408,... Gradient Norm: 0.0607\n",
      "Epoch 939: avg loss training: 2.7408,... Gradient Norm: 0.0676\n",
      "Epoch 940: avg loss training: 2.7408,... Gradient Norm: 0.2324\n",
      "Epoch 941: avg loss training: 2.7408,... Gradient Norm: 0.0572\n",
      "Epoch 942: avg loss training: 2.7408,... Gradient Norm: 0.1067\n",
      "Epoch 943: avg loss training: 2.7408,... Gradient Norm: 0.1271\n",
      "Epoch 944: avg loss training: 2.7408,... Gradient Norm: 0.1193\n",
      "Epoch 945: avg loss training: 2.7408,... Gradient Norm: 0.0647\n",
      "Epoch 946: avg loss training: 2.7408,... Gradient Norm: 0.2600\n",
      "Epoch 947: avg loss training: 2.7408,... Gradient Norm: 0.0584\n",
      "Epoch 948: avg loss training: 2.7408,... Gradient Norm: 0.0533\n",
      "Epoch 949: avg loss training: 2.7408,... Gradient Norm: 0.1126\n",
      "Epoch 950: avg loss training: 2.7408,... Gradient Norm: 0.1093\n",
      "Epoch 951: avg loss training: 2.7408,... Gradient Norm: 0.0679\n",
      "Epoch 952: avg loss training: 2.7408,... Gradient Norm: 0.0611\n",
      "Epoch 953: avg loss training: 2.7408,... Gradient Norm: 0.2594\n",
      "Epoch 954: avg loss training: 2.7408,... Gradient Norm: 0.0590\n",
      "Epoch 955: avg loss training: 2.7408,... Gradient Norm: 0.1229\n",
      "Epoch 956: avg loss training: 2.7408,... Gradient Norm: 0.1369\n",
      "Epoch 957: avg loss training: 2.7408,... Gradient Norm: 0.0511\n",
      "Epoch 958: avg loss training: 2.7408,... Gradient Norm: 0.0623\n",
      "Epoch 959: avg loss training: 2.7408,... Gradient Norm: 0.2634\n",
      "Epoch 960: avg loss training: 2.7408,... Gradient Norm: 0.0576\n",
      "Epoch 961: avg loss training: 2.7408,... Gradient Norm: 0.0927\n",
      "Epoch 962: avg loss training: 2.7408,... Gradient Norm: 0.1312\n",
      "Epoch 963: avg loss training: 2.7408,... Gradient Norm: 0.0888\n",
      "Epoch 964: avg loss training: 2.7408,... Gradient Norm: 0.0655\n",
      "Epoch 965: avg loss training: 2.7408,... Gradient Norm: 0.2538\n",
      "Epoch 966: avg loss training: 2.7408,... Gradient Norm: 0.0630\n",
      "Epoch 967: avg loss training: 2.7408,... Gradient Norm: 0.0648\n",
      "Epoch 968: avg loss training: 2.7408,... Gradient Norm: 0.1434\n",
      "Epoch 969: avg loss training: 2.7408,... Gradient Norm: 0.1748\n",
      "Epoch 970: avg loss training: 2.7408,... Gradient Norm: 0.0788\n",
      "Epoch 971: avg loss training: 2.7408,... Gradient Norm: 0.1624\n",
      "Epoch 972: avg loss training: 2.7408,... Gradient Norm: 0.0933\n",
      "Epoch 973: avg loss training: 2.7408,... Gradient Norm: 0.0625\n",
      "Epoch 974: avg loss training: 2.7408,... Gradient Norm: 0.0916\n",
      "Epoch 975: avg loss training: 2.7408,... Gradient Norm: 0.0942\n",
      "Epoch 976: avg loss training: 2.7408,... Gradient Norm: 0.0618\n",
      "Epoch 977: avg loss training: 2.7408,... Gradient Norm: 0.2567\n",
      "Epoch 978: avg loss training: 2.7408,... Gradient Norm: 0.0544\n",
      "Epoch 979: avg loss training: 2.7408,... Gradient Norm: 0.0883\n",
      "Epoch 980: avg loss training: 2.7408,... Gradient Norm: 0.1256\n",
      "Epoch 981: avg loss training: 2.7408,... Gradient Norm: 0.0692\n",
      "Epoch 982: avg loss training: 2.7408,... Gradient Norm: 0.0648\n",
      "Epoch 983: avg loss training: 2.7408,... Gradient Norm: 0.2374\n",
      "Epoch 984: avg loss training: 2.7408,... Gradient Norm: 0.0591\n",
      "Epoch 985: avg loss training: 2.7408,... Gradient Norm: 0.1180\n",
      "Epoch 986: avg loss training: 2.7408,... Gradient Norm: 0.1036\n",
      "Epoch 987: avg loss training: 2.7408,... Gradient Norm: 0.0541\n",
      "Epoch 988: avg loss training: 2.7408,... Gradient Norm: 0.0817\n",
      "Epoch 989: avg loss training: 2.7408,... Gradient Norm: 0.2517\n",
      "Epoch 990: avg loss training: 2.7408,... Gradient Norm: 0.0875\n",
      "Epoch 991: avg loss training: 2.7408,... Gradient Norm: 0.1174\n",
      "Epoch 992: avg loss training: 2.7408,... Gradient Norm: 0.1137\n",
      "Epoch 993: avg loss training: 2.7408,... Gradient Norm: 0.0598\n",
      "Epoch 994: avg loss training: 2.7408,... Gradient Norm: 0.0703\n",
      "Epoch 995: avg loss training: 2.7408,... Gradient Norm: 0.2436\n",
      "Epoch 996: avg loss training: 2.7408,... Gradient Norm: 0.0773\n",
      "Epoch 997: avg loss training: 2.7408,... Gradient Norm: 0.1162\n",
      "Epoch 998: avg loss training: 2.7408,... Gradient Norm: 0.1390\n",
      "Epoch 999: avg loss training: 2.7408,... Gradient Norm: 0.0871\n",
      "Epoch 1000: avg loss training: 2.7408,... Gradient Norm: 0.0601\n",
      "Epoch 1: avg loss training: 3.5261,... Gradient Norm: 7.3444\n",
      "Epoch 2: avg loss training: 116.9730,... Gradient Norm: 1375.0095\n",
      "Epoch 3: avg loss training: 10.0086,... Gradient Norm: 68.6908\n",
      "Epoch 4: avg loss training: 32.8824,... Gradient Norm: 423.0989\n",
      "Epoch 5: avg loss training: 15.7289,... Gradient Norm: 135.2785\n",
      "Epoch 6: avg loss training: 11.8218,... Gradient Norm: 57.5227\n",
      "Epoch 7: avg loss training: 15.3743,... Gradient Norm: 153.6238\n",
      "Epoch 8: avg loss training: 7.6640,... Gradient Norm: 39.0186\n",
      "Epoch 9: avg loss training: 5.1956,... Gradient Norm: 20.4907\n",
      "Epoch 10: avg loss training: 4.1472,... Gradient Norm: 11.4414\n",
      "Epoch 11: avg loss training: 4.1009,... Gradient Norm: 10.2475\n",
      "Epoch 12: avg loss training: 4.6802,... Gradient Norm: 19.2632\n",
      "Epoch 13: avg loss training: 5.0642,... Gradient Norm: 22.6368\n",
      "Epoch 14: avg loss training: 5.0162,... Gradient Norm: 21.3981\n",
      "Epoch 15: avg loss training: 4.6724,... Gradient Norm: 17.6671\n",
      "Epoch 16: avg loss training: 4.2418,... Gradient Norm: 12.8149\n",
      "Epoch 17: avg loss training: 3.9124,... Gradient Norm: 8.4238\n",
      "Epoch 18: avg loss training: 3.7694,... Gradient Norm: 7.4308\n",
      "Epoch 19: avg loss training: 3.6780,... Gradient Norm: 6.6579\n",
      "Epoch 20: avg loss training: 3.5768,... Gradient Norm: 3.9807\n",
      "Epoch 21: avg loss training: 3.5247,... Gradient Norm: 4.1254\n",
      "Epoch 22: avg loss training: 3.5065,... Gradient Norm: 6.6146\n",
      "Epoch 23: avg loss training: 3.4560,... Gradient Norm: 6.7422\n",
      "Epoch 24: avg loss training: 3.3751,... Gradient Norm: 4.3931\n",
      "Epoch 25: avg loss training: 3.3251,... Gradient Norm: 3.3959\n",
      "Epoch 26: avg loss training: 3.2984,... Gradient Norm: 5.1841\n",
      "Epoch 27: avg loss training: 3.2360,... Gradient Norm: 4.3528\n",
      "Epoch 28: avg loss training: 3.1665,... Gradient Norm: 2.1992\n",
      "Epoch 29: avg loss training: 3.1273,... Gradient Norm: 3.2138\n",
      "Epoch 30: avg loss training: 3.0953,... Gradient Norm: 3.9979\n",
      "Epoch 31: avg loss training: 3.0514,... Gradient Norm: 2.9624\n",
      "Epoch 32: avg loss training: 3.0124,... Gradient Norm: 1.8375\n",
      "Epoch 33: avg loss training: 2.9915,... Gradient Norm: 3.5267\n",
      "Epoch 34: avg loss training: 2.9632,... Gradient Norm: 3.6594\n",
      "Epoch 35: avg loss training: 2.9268,... Gradient Norm: 1.7694\n",
      "Epoch 36: avg loss training: 2.9130,... Gradient Norm: 2.4597\n",
      "Epoch 37: avg loss training: 2.9067,... Gradient Norm: 2.7352\n",
      "Epoch 38: avg loss training: 2.8969,... Gradient Norm: 1.2013\n",
      "Epoch 39: avg loss training: 2.8987,... Gradient Norm: 2.2841\n",
      "Epoch 40: avg loss training: 2.8976,... Gradient Norm: 2.8425\n",
      "Epoch 41: avg loss training: 2.8860,... Gradient Norm: 1.8237\n",
      "Epoch 42: avg loss training: 2.8795,... Gradient Norm: 1.8613\n",
      "Epoch 43: avg loss training: 2.8751,... Gradient Norm: 2.0105\n",
      "Epoch 44: avg loss training: 2.8703,... Gradient Norm: 0.9943\n",
      "Epoch 45: avg loss training: 2.8710,... Gradient Norm: 1.4752\n",
      "Epoch 46: avg loss training: 2.8702,... Gradient Norm: 1.7961\n",
      "Epoch 47: avg loss training: 2.8649,... Gradient Norm: 0.8526\n",
      "Epoch 48: avg loss training: 2.8632,... Gradient Norm: 1.2100\n",
      "Epoch 49: avg loss training: 2.8618,... Gradient Norm: 1.5855\n",
      "Epoch 50: avg loss training: 2.8575,... Gradient Norm: 1.1514\n",
      "Epoch 51: avg loss training: 2.8552,... Gradient Norm: 1.5832\n",
      "Epoch 52: avg loss training: 2.8531,... Gradient Norm: 1.3371\n",
      "Epoch 53: avg loss training: 2.8512,... Gradient Norm: 0.4576\n",
      "Epoch 54: avg loss training: 2.8505,... Gradient Norm: 1.0386\n",
      "Epoch 55: avg loss training: 2.8482,... Gradient Norm: 0.6895\n",
      "Epoch 56: avg loss training: 2.8472,... Gradient Norm: 0.4170\n",
      "Epoch 57: avg loss training: 2.8475,... Gradient Norm: 0.8060\n",
      "Epoch 58: avg loss training: 2.8458,... Gradient Norm: 0.3940\n",
      "Epoch 59: avg loss training: 2.8449,... Gradient Norm: 0.9603\n",
      "Epoch 60: avg loss training: 2.8438,... Gradient Norm: 0.9055\n",
      "Epoch 61: avg loss training: 2.8428,... Gradient Norm: 0.5258\n",
      "Epoch 62: avg loss training: 2.8426,... Gradient Norm: 0.9219\n",
      "Epoch 63: avg loss training: 2.8413,... Gradient Norm: 0.6742\n",
      "Epoch 64: avg loss training: 2.8402,... Gradient Norm: 0.3942\n",
      "Epoch 65: avg loss training: 2.8397,... Gradient Norm: 0.7073\n",
      "Epoch 66: avg loss training: 2.8385,... Gradient Norm: 0.4422\n",
      "Epoch 67: avg loss training: 2.8374,... Gradient Norm: 0.3655\n",
      "Epoch 68: avg loss training: 2.8366,... Gradient Norm: 0.6210\n",
      "Epoch 69: avg loss training: 2.8355,... Gradient Norm: 0.4455\n",
      "Epoch 70: avg loss training: 2.8346,... Gradient Norm: 0.4606\n",
      "Epoch 71: avg loss training: 2.8337,... Gradient Norm: 0.5097\n",
      "Epoch 72: avg loss training: 2.8329,... Gradient Norm: 0.1641\n",
      "Epoch 73: avg loss training: 2.8324,... Gradient Norm: 0.4362\n",
      "Epoch 74: avg loss training: 2.8318,... Gradient Norm: 0.4604\n",
      "Epoch 75: avg loss training: 2.8311,... Gradient Norm: 0.3088\n",
      "Epoch 76: avg loss training: 2.8305,... Gradient Norm: 0.3454\n",
      "Epoch 77: avg loss training: 2.8299,... Gradient Norm: 0.2833\n",
      "Epoch 78: avg loss training: 2.8293,... Gradient Norm: 0.1713\n",
      "Epoch 79: avg loss training: 2.8286,... Gradient Norm: 0.3160\n",
      "Epoch 80: avg loss training: 2.8280,... Gradient Norm: 0.2768\n",
      "Epoch 81: avg loss training: 2.8275,... Gradient Norm: 0.3529\n",
      "Epoch 82: avg loss training: 2.8269,... Gradient Norm: 0.3434\n",
      "Epoch 83: avg loss training: 2.8263,... Gradient Norm: 0.2049\n",
      "Epoch 84: avg loss training: 2.8258,... Gradient Norm: 0.2563\n",
      "Epoch 85: avg loss training: 2.8253,... Gradient Norm: 0.2271\n",
      "Epoch 86: avg loss training: 2.8248,... Gradient Norm: 0.1524\n",
      "Epoch 87: avg loss training: 2.8243,... Gradient Norm: 0.2001\n",
      "Epoch 88: avg loss training: 2.8238,... Gradient Norm: 0.1238\n",
      "Epoch 89: avg loss training: 2.8234,... Gradient Norm: 0.2283\n",
      "Epoch 90: avg loss training: 2.8230,... Gradient Norm: 0.1498\n",
      "Epoch 91: avg loss training: 2.8225,... Gradient Norm: 0.1569\n",
      "Epoch 92: avg loss training: 2.8222,... Gradient Norm: 0.2226\n",
      "Epoch 93: avg loss training: 2.8218,... Gradient Norm: 0.1320\n",
      "Epoch 94: avg loss training: 2.8214,... Gradient Norm: 0.1888\n",
      "Epoch 95: avg loss training: 2.8210,... Gradient Norm: 0.0902\n",
      "Epoch 96: avg loss training: 2.8207,... Gradient Norm: 0.2079\n",
      "Epoch 97: avg loss training: 2.8203,... Gradient Norm: 0.1506\n",
      "Epoch 98: avg loss training: 2.8200,... Gradient Norm: 0.1380\n",
      "Epoch 99: avg loss training: 2.8197,... Gradient Norm: 0.1347\n",
      "Epoch 100: avg loss training: 2.8194,... Gradient Norm: 0.1345\n",
      "Epoch 101: avg loss training: 2.8190,... Gradient Norm: 0.1463\n",
      "Epoch 102: avg loss training: 2.8187,... Gradient Norm: 0.0854\n",
      "Epoch 103: avg loss training: 2.8184,... Gradient Norm: 0.1478\n",
      "Epoch 104: avg loss training: 2.8180,... Gradient Norm: 0.1735\n",
      "Epoch 105: avg loss training: 2.8177,... Gradient Norm: 0.1068\n",
      "Epoch 106: avg loss training: 2.8174,... Gradient Norm: 0.0886\n",
      "Epoch 107: avg loss training: 2.8170,... Gradient Norm: 0.1318\n",
      "Epoch 108: avg loss training: 2.8167,... Gradient Norm: 0.1046\n",
      "Epoch 109: avg loss training: 2.8164,... Gradient Norm: 0.1012\n",
      "Epoch 110: avg loss training: 2.8160,... Gradient Norm: 0.1089\n",
      "Epoch 111: avg loss training: 2.8157,... Gradient Norm: 0.1350\n",
      "Epoch 112: avg loss training: 2.8155,... Gradient Norm: 0.1178\n",
      "Epoch 113: avg loss training: 2.8152,... Gradient Norm: 0.1002\n",
      "Epoch 114: avg loss training: 2.8149,... Gradient Norm: 0.1134\n",
      "Epoch 115: avg loss training: 2.8146,... Gradient Norm: 0.1050\n",
      "Epoch 116: avg loss training: 2.8143,... Gradient Norm: 0.1276\n",
      "Epoch 117: avg loss training: 2.8140,... Gradient Norm: 0.0693\n",
      "Epoch 118: avg loss training: 2.8137,... Gradient Norm: 0.0919\n",
      "Epoch 119: avg loss training: 2.8134,... Gradient Norm: 0.1042\n",
      "Epoch 120: avg loss training: 2.8131,... Gradient Norm: 0.0876\n",
      "Epoch 121: avg loss training: 2.8128,... Gradient Norm: 0.0919\n",
      "Epoch 122: avg loss training: 2.8125,... Gradient Norm: 0.0986\n",
      "Epoch 123: avg loss training: 2.8122,... Gradient Norm: 0.0763\n",
      "Epoch 124: avg loss training: 2.8120,... Gradient Norm: 0.0778\n",
      "Epoch 125: avg loss training: 2.8117,... Gradient Norm: 0.0833\n",
      "Epoch 126: avg loss training: 2.8114,... Gradient Norm: 0.0991\n",
      "Epoch 127: avg loss training: 2.8112,... Gradient Norm: 0.1337\n",
      "Epoch 128: avg loss training: 2.8109,... Gradient Norm: 0.1938\n",
      "Epoch 129: avg loss training: 2.8107,... Gradient Norm: 0.2003\n",
      "Epoch 130: avg loss training: 2.8105,... Gradient Norm: 0.1511\n",
      "Epoch 131: avg loss training: 2.8102,... Gradient Norm: 0.1005\n",
      "Epoch 132: avg loss training: 2.8099,... Gradient Norm: 0.1904\n",
      "Epoch 133: avg loss training: 2.8096,... Gradient Norm: 0.1219\n",
      "Epoch 134: avg loss training: 2.8094,... Gradient Norm: 0.1886\n",
      "Epoch 135: avg loss training: 2.8091,... Gradient Norm: 0.1644\n",
      "Epoch 136: avg loss training: 2.8089,... Gradient Norm: 0.2735\n",
      "Epoch 137: avg loss training: 2.8086,... Gradient Norm: 0.3531\n",
      "Epoch 138: avg loss training: 2.8083,... Gradient Norm: 0.1175\n",
      "Epoch 139: avg loss training: 2.8081,... Gradient Norm: 0.2928\n",
      "Epoch 140: avg loss training: 2.8078,... Gradient Norm: 0.0887\n",
      "Epoch 141: avg loss training: 2.8076,... Gradient Norm: 0.2783\n",
      "Epoch 142: avg loss training: 2.8072,... Gradient Norm: 0.1746\n",
      "Epoch 143: avg loss training: 2.8070,... Gradient Norm: 0.2094\n",
      "Epoch 144: avg loss training: 2.8067,... Gradient Norm: 0.2037\n",
      "Epoch 145: avg loss training: 2.8065,... Gradient Norm: 0.0937\n",
      "Epoch 146: avg loss training: 2.8063,... Gradient Norm: 0.3209\n",
      "Epoch 147: avg loss training: 2.8060,... Gradient Norm: 0.2393\n",
      "Epoch 148: avg loss training: 2.8058,... Gradient Norm: 0.1891\n",
      "Epoch 149: avg loss training: 2.8056,... Gradient Norm: 0.3872\n",
      "Epoch 150: avg loss training: 2.8053,... Gradient Norm: 0.1173\n",
      "Epoch 151: avg loss training: 2.8051,... Gradient Norm: 0.2367\n",
      "Epoch 152: avg loss training: 2.8049,... Gradient Norm: 0.1712\n",
      "Epoch 153: avg loss training: 2.8047,... Gradient Norm: 0.1304\n",
      "Epoch 154: avg loss training: 2.8046,... Gradient Norm: 0.2753\n",
      "Epoch 155: avg loss training: 2.8043,... Gradient Norm: 0.0719\n",
      "Epoch 156: avg loss training: 2.8042,... Gradient Norm: 0.2030\n",
      "Epoch 157: avg loss training: 2.8040,... Gradient Norm: 0.1317\n",
      "Epoch 158: avg loss training: 2.8038,... Gradient Norm: 0.1236\n",
      "Epoch 159: avg loss training: 2.8037,... Gradient Norm: 0.2070\n",
      "Epoch 160: avg loss training: 2.8035,... Gradient Norm: 0.0792\n",
      "Epoch 161: avg loss training: 2.8034,... Gradient Norm: 0.1026\n",
      "Epoch 162: avg loss training: 2.8032,... Gradient Norm: 0.1054\n",
      "Epoch 163: avg loss training: 2.8031,... Gradient Norm: 0.0705\n",
      "Epoch 164: avg loss training: 2.8029,... Gradient Norm: 0.0913\n",
      "Epoch 165: avg loss training: 2.8028,... Gradient Norm: 0.0657\n",
      "Epoch 166: avg loss training: 2.8027,... Gradient Norm: 0.1160\n",
      "Epoch 167: avg loss training: 2.8025,... Gradient Norm: 0.1084\n",
      "Epoch 168: avg loss training: 2.8024,... Gradient Norm: 0.0631\n",
      "Epoch 169: avg loss training: 2.8023,... Gradient Norm: 0.0687\n",
      "Epoch 170: avg loss training: 2.8022,... Gradient Norm: 0.1015\n",
      "Epoch 171: avg loss training: 2.8021,... Gradient Norm: 0.1089\n",
      "Epoch 172: avg loss training: 2.8020,... Gradient Norm: 0.1629\n",
      "Epoch 173: avg loss training: 2.8019,... Gradient Norm: 0.0729\n",
      "Epoch 174: avg loss training: 2.8018,... Gradient Norm: 0.0781\n",
      "Epoch 175: avg loss training: 2.8017,... Gradient Norm: 0.1185\n",
      "Epoch 176: avg loss training: 2.8015,... Gradient Norm: 0.1032\n",
      "Epoch 177: avg loss training: 2.8014,... Gradient Norm: 0.2117\n",
      "Epoch 178: avg loss training: 2.8013,... Gradient Norm: 0.1678\n",
      "Epoch 179: avg loss training: 2.8012,... Gradient Norm: 0.1525\n",
      "Epoch 180: avg loss training: 2.8011,... Gradient Norm: 0.2883\n",
      "Epoch 181: avg loss training: 2.8010,... Gradient Norm: 0.0643\n",
      "Epoch 182: avg loss training: 2.8009,... Gradient Norm: 0.2276\n",
      "Epoch 183: avg loss training: 2.8008,... Gradient Norm: 0.0898\n",
      "Epoch 184: avg loss training: 2.8007,... Gradient Norm: 0.1244\n",
      "Epoch 185: avg loss training: 2.8007,... Gradient Norm: 0.1108\n",
      "Epoch 186: avg loss training: 2.8006,... Gradient Norm: 0.0969\n",
      "Epoch 187: avg loss training: 2.8005,... Gradient Norm: 0.0961\n",
      "Epoch 188: avg loss training: 2.8004,... Gradient Norm: 0.0656\n",
      "Epoch 189: avg loss training: 2.8003,... Gradient Norm: 0.0576\n",
      "Epoch 190: avg loss training: 2.8002,... Gradient Norm: 0.0591\n",
      "Epoch 191: avg loss training: 2.8001,... Gradient Norm: 0.0659\n",
      "Epoch 192: avg loss training: 2.8001,... Gradient Norm: 0.0907\n",
      "Epoch 193: avg loss training: 2.8000,... Gradient Norm: 0.0992\n",
      "Epoch 194: avg loss training: 2.7999,... Gradient Norm: 0.1394\n",
      "Epoch 195: avg loss training: 2.7998,... Gradient Norm: 0.0733\n",
      "Epoch 196: avg loss training: 2.7998,... Gradient Norm: 0.0647\n",
      "Epoch 197: avg loss training: 2.7997,... Gradient Norm: 0.0850\n",
      "Epoch 198: avg loss training: 2.7996,... Gradient Norm: 0.0861\n",
      "Epoch 199: avg loss training: 2.7995,... Gradient Norm: 0.1572\n",
      "Epoch 200: avg loss training: 2.7995,... Gradient Norm: 0.0783\n",
      "Epoch 201: avg loss training: 2.7994,... Gradient Norm: 0.0888\n",
      "Epoch 202: avg loss training: 2.7993,... Gradient Norm: 0.0631\n",
      "Epoch 203: avg loss training: 2.7993,... Gradient Norm: 0.0755\n",
      "Epoch 204: avg loss training: 2.7992,... Gradient Norm: 0.1087\n",
      "Epoch 205: avg loss training: 2.7991,... Gradient Norm: 0.0626\n",
      "Epoch 206: avg loss training: 2.7991,... Gradient Norm: 0.0710\n",
      "Epoch 207: avg loss training: 2.7990,... Gradient Norm: 0.0636\n",
      "Epoch 208: avg loss training: 2.7989,... Gradient Norm: 0.0974\n",
      "Epoch 209: avg loss training: 2.7988,... Gradient Norm: 0.0807\n",
      "Epoch 210: avg loss training: 2.7988,... Gradient Norm: 0.0723\n",
      "Epoch 211: avg loss training: 2.7987,... Gradient Norm: 0.0767\n",
      "Epoch 212: avg loss training: 2.7986,... Gradient Norm: 0.0764\n",
      "Epoch 213: avg loss training: 2.7986,... Gradient Norm: 0.0635\n",
      "Epoch 214: avg loss training: 2.7985,... Gradient Norm: 0.0836\n",
      "Epoch 215: avg loss training: 2.7985,... Gradient Norm: 0.0712\n",
      "Epoch 216: avg loss training: 2.7984,... Gradient Norm: 0.1163\n",
      "Epoch 217: avg loss training: 2.7984,... Gradient Norm: 0.1199\n",
      "Epoch 218: avg loss training: 2.7983,... Gradient Norm: 0.0744\n",
      "Epoch 219: avg loss training: 2.7982,... Gradient Norm: 0.1392\n",
      "Epoch 220: avg loss training: 2.7982,... Gradient Norm: 0.0953\n",
      "Epoch 221: avg loss training: 2.7981,... Gradient Norm: 0.1468\n",
      "Epoch 222: avg loss training: 2.7981,... Gradient Norm: 0.1832\n",
      "Epoch 223: avg loss training: 2.7980,... Gradient Norm: 0.0713\n",
      "Epoch 224: avg loss training: 2.7980,... Gradient Norm: 0.1181\n",
      "Epoch 225: avg loss training: 2.7979,... Gradient Norm: 0.0548\n",
      "Epoch 226: avg loss training: 2.7979,... Gradient Norm: 0.0612\n",
      "Epoch 227: avg loss training: 2.7978,... Gradient Norm: 0.0746\n",
      "Epoch 228: avg loss training: 2.7978,... Gradient Norm: 0.0662\n",
      "Epoch 229: avg loss training: 2.7977,... Gradient Norm: 0.0741\n",
      "Epoch 230: avg loss training: 2.7977,... Gradient Norm: 0.0681\n",
      "Epoch 231: avg loss training: 2.7976,... Gradient Norm: 0.0548\n",
      "Epoch 232: avg loss training: 2.7976,... Gradient Norm: 0.0511\n",
      "Epoch 233: avg loss training: 2.7975,... Gradient Norm: 0.0589\n",
      "Epoch 234: avg loss training: 2.7975,... Gradient Norm: 0.0704\n",
      "Epoch 235: avg loss training: 2.7975,... Gradient Norm: 0.0446\n",
      "Epoch 236: avg loss training: 2.7974,... Gradient Norm: 0.0644\n",
      "Epoch 237: avg loss training: 2.7974,... Gradient Norm: 0.0852\n",
      "Epoch 238: avg loss training: 2.7973,... Gradient Norm: 0.0631\n",
      "Epoch 239: avg loss training: 2.7973,... Gradient Norm: 0.1120\n",
      "Epoch 240: avg loss training: 2.7972,... Gradient Norm: 0.0491\n",
      "Epoch 241: avg loss training: 2.7972,... Gradient Norm: 0.0571\n",
      "Epoch 242: avg loss training: 2.7972,... Gradient Norm: 0.0432\n",
      "Epoch 243: avg loss training: 2.7971,... Gradient Norm: 0.0553\n",
      "Epoch 244: avg loss training: 2.7971,... Gradient Norm: 0.0581\n",
      "Epoch 245: avg loss training: 2.7970,... Gradient Norm: 0.0660\n",
      "Epoch 246: avg loss training: 2.7970,... Gradient Norm: 0.0889\n",
      "Epoch 247: avg loss training: 2.7969,... Gradient Norm: 0.0815\n",
      "Epoch 248: avg loss training: 2.7969,... Gradient Norm: 0.0590\n",
      "Epoch 249: avg loss training: 2.7969,... Gradient Norm: 0.1269\n",
      "Epoch 250: avg loss training: 2.7968,... Gradient Norm: 0.0638\n",
      "Epoch 251: avg loss training: 2.7968,... Gradient Norm: 0.1144\n",
      "Epoch 252: avg loss training: 2.7967,... Gradient Norm: 0.0973\n",
      "Epoch 253: avg loss training: 2.7967,... Gradient Norm: 0.1068\n",
      "Epoch 254: avg loss training: 2.7967,... Gradient Norm: 0.1062\n",
      "Epoch 255: avg loss training: 2.7966,... Gradient Norm: 0.0479\n",
      "Epoch 256: avg loss training: 2.7966,... Gradient Norm: 0.0786\n",
      "Epoch 257: avg loss training: 2.7966,... Gradient Norm: 0.0656\n",
      "Epoch 258: avg loss training: 2.7965,... Gradient Norm: 0.0535\n",
      "Epoch 259: avg loss training: 2.7965,... Gradient Norm: 0.0483\n",
      "Epoch 260: avg loss training: 2.7964,... Gradient Norm: 0.0559\n",
      "Epoch 261: avg loss training: 2.7964,... Gradient Norm: 0.0659\n",
      "Epoch 262: avg loss training: 2.7964,... Gradient Norm: 0.0475\n",
      "Epoch 263: avg loss training: 2.7963,... Gradient Norm: 0.0528\n",
      "Epoch 264: avg loss training: 2.7963,... Gradient Norm: 0.0816\n",
      "Epoch 265: avg loss training: 2.7963,... Gradient Norm: 0.0709\n",
      "Epoch 266: avg loss training: 2.7962,... Gradient Norm: 0.0430\n",
      "Epoch 267: avg loss training: 2.7962,... Gradient Norm: 0.0780\n",
      "Epoch 268: avg loss training: 2.7962,... Gradient Norm: 0.0766\n",
      "Epoch 269: avg loss training: 2.7962,... Gradient Norm: 0.0686\n",
      "Epoch 270: avg loss training: 2.7961,... Gradient Norm: 0.1184\n",
      "Epoch 271: avg loss training: 2.7961,... Gradient Norm: 0.0468\n",
      "Epoch 272: avg loss training: 2.7961,... Gradient Norm: 0.0538\n",
      "Epoch 273: avg loss training: 2.7960,... Gradient Norm: 0.0456\n",
      "Epoch 274: avg loss training: 2.7960,... Gradient Norm: 0.0719\n",
      "Epoch 275: avg loss training: 2.7960,... Gradient Norm: 0.0550\n",
      "Epoch 276: avg loss training: 2.7960,... Gradient Norm: 0.0840\n",
      "Epoch 277: avg loss training: 2.7959,... Gradient Norm: 0.0532\n",
      "Epoch 278: avg loss training: 2.7959,... Gradient Norm: 0.1347\n",
      "Epoch 279: avg loss training: 2.7959,... Gradient Norm: 0.0981\n",
      "Epoch 280: avg loss training: 2.7958,... Gradient Norm: 0.0754\n",
      "Epoch 281: avg loss training: 2.7958,... Gradient Norm: 0.1339\n",
      "Epoch 282: avg loss training: 2.7958,... Gradient Norm: 0.0520\n",
      "Epoch 283: avg loss training: 2.7958,... Gradient Norm: 0.0634\n",
      "Epoch 284: avg loss training: 2.7957,... Gradient Norm: 0.0596\n",
      "Epoch 285: avg loss training: 2.7957,... Gradient Norm: 0.0647\n",
      "Epoch 286: avg loss training: 2.7957,... Gradient Norm: 0.0751\n",
      "Epoch 287: avg loss training: 2.7957,... Gradient Norm: 0.0528\n",
      "Epoch 288: avg loss training: 2.7956,... Gradient Norm: 0.0470\n",
      "Epoch 289: avg loss training: 2.7956,... Gradient Norm: 0.0705\n",
      "Epoch 290: avg loss training: 2.7956,... Gradient Norm: 0.0446\n",
      "Epoch 291: avg loss training: 2.7956,... Gradient Norm: 0.0974\n",
      "Epoch 292: avg loss training: 2.7955,... Gradient Norm: 0.0875\n",
      "Epoch 293: avg loss training: 2.7955,... Gradient Norm: 0.0730\n",
      "Epoch 294: avg loss training: 2.7955,... Gradient Norm: 0.0784\n",
      "Epoch 295: avg loss training: 2.7955,... Gradient Norm: 0.0455\n",
      "Epoch 296: avg loss training: 2.7954,... Gradient Norm: 0.0760\n",
      "Epoch 297: avg loss training: 2.7954,... Gradient Norm: 0.0518\n",
      "Epoch 298: avg loss training: 2.7954,... Gradient Norm: 0.0520\n",
      "Epoch 299: avg loss training: 2.7954,... Gradient Norm: 0.0615\n",
      "Epoch 300: avg loss training: 2.7953,... Gradient Norm: 0.0823\n",
      "Epoch 301: avg loss training: 2.7953,... Gradient Norm: 0.0616\n",
      "Epoch 302: avg loss training: 2.7953,... Gradient Norm: 0.0849\n",
      "Epoch 303: avg loss training: 2.7953,... Gradient Norm: 0.1153\n",
      "Epoch 304: avg loss training: 2.7953,... Gradient Norm: 0.0508\n",
      "Epoch 305: avg loss training: 2.7952,... Gradient Norm: 0.1584\n",
      "Epoch 306: avg loss training: 2.7952,... Gradient Norm: 0.1645\n",
      "Epoch 307: avg loss training: 2.7952,... Gradient Norm: 0.0444\n",
      "Epoch 308: avg loss training: 2.7952,... Gradient Norm: 0.1421\n",
      "Epoch 309: avg loss training: 2.7952,... Gradient Norm: 0.1379\n",
      "Epoch 310: avg loss training: 2.7951,... Gradient Norm: 0.0605\n",
      "Epoch 311: avg loss training: 2.7951,... Gradient Norm: 0.1211\n",
      "Epoch 312: avg loss training: 2.7951,... Gradient Norm: 0.1066\n",
      "Epoch 313: avg loss training: 2.7951,... Gradient Norm: 0.0417\n",
      "Epoch 314: avg loss training: 2.7950,... Gradient Norm: 0.0762\n",
      "Epoch 315: avg loss training: 2.7950,... Gradient Norm: 0.0640\n",
      "Epoch 316: avg loss training: 2.7950,... Gradient Norm: 0.0646\n",
      "Epoch 317: avg loss training: 2.7950,... Gradient Norm: 0.0587\n",
      "Epoch 318: avg loss training: 2.7950,... Gradient Norm: 0.0484\n",
      "Epoch 319: avg loss training: 2.7949,... Gradient Norm: 0.0976\n",
      "Epoch 320: avg loss training: 2.7949,... Gradient Norm: 0.0903\n",
      "Epoch 321: avg loss training: 2.7949,... Gradient Norm: 0.0475\n",
      "Epoch 322: avg loss training: 2.7949,... Gradient Norm: 0.0882\n",
      "Epoch 323: avg loss training: 2.7949,... Gradient Norm: 0.0699\n",
      "Epoch 324: avg loss training: 2.7948,... Gradient Norm: 0.0635\n",
      "Epoch 325: avg loss training: 2.7948,... Gradient Norm: 0.1155\n",
      "Epoch 326: avg loss training: 2.7948,... Gradient Norm: 0.1010\n",
      "Epoch 327: avg loss training: 2.7948,... Gradient Norm: 0.0605\n",
      "Epoch 328: avg loss training: 2.7948,... Gradient Norm: 0.0773\n",
      "Epoch 329: avg loss training: 2.7947,... Gradient Norm: 0.0576\n",
      "Epoch 330: avg loss training: 2.7947,... Gradient Norm: 0.0914\n",
      "Epoch 331: avg loss training: 2.7947,... Gradient Norm: 0.1060\n",
      "Epoch 332: avg loss training: 2.7947,... Gradient Norm: 0.0703\n",
      "Epoch 333: avg loss training: 2.7947,... Gradient Norm: 0.0803\n",
      "Epoch 334: avg loss training: 2.7947,... Gradient Norm: 0.0488\n",
      "Epoch 335: avg loss training: 2.7946,... Gradient Norm: 0.0624\n",
      "Epoch 336: avg loss training: 2.7946,... Gradient Norm: 0.1029\n",
      "Epoch 337: avg loss training: 2.7946,... Gradient Norm: 0.0547\n",
      "Epoch 338: avg loss training: 2.7946,... Gradient Norm: 0.0674\n",
      "Epoch 339: avg loss training: 2.7946,... Gradient Norm: 0.0636\n",
      "Epoch 340: avg loss training: 2.7946,... Gradient Norm: 0.0470\n",
      "Epoch 341: avg loss training: 2.7945,... Gradient Norm: 0.1340\n",
      "Epoch 342: avg loss training: 2.7945,... Gradient Norm: 0.1117\n",
      "Epoch 343: avg loss training: 2.7945,... Gradient Norm: 0.0533\n",
      "Epoch 344: avg loss training: 2.7945,... Gradient Norm: 0.0915\n",
      "Epoch 345: avg loss training: 2.7945,... Gradient Norm: 0.0627\n",
      "Epoch 346: avg loss training: 2.7945,... Gradient Norm: 0.0632\n",
      "Epoch 347: avg loss training: 2.7944,... Gradient Norm: 0.0662\n",
      "Epoch 348: avg loss training: 2.7944,... Gradient Norm: 0.0596\n",
      "Epoch 349: avg loss training: 2.7944,... Gradient Norm: 0.0451\n",
      "Epoch 350: avg loss training: 2.7944,... Gradient Norm: 0.0412\n",
      "Epoch 351: avg loss training: 2.7944,... Gradient Norm: 0.0486\n",
      "Epoch 352: avg loss training: 2.7944,... Gradient Norm: 0.0870\n",
      "Epoch 353: avg loss training: 2.7944,... Gradient Norm: 0.0859\n",
      "Epoch 354: avg loss training: 2.7943,... Gradient Norm: 0.0651\n",
      "Epoch 355: avg loss training: 2.7943,... Gradient Norm: 0.0671\n",
      "Epoch 356: avg loss training: 2.7943,... Gradient Norm: 0.0599\n",
      "Epoch 357: avg loss training: 2.7943,... Gradient Norm: 0.0632\n",
      "Epoch 358: avg loss training: 2.7943,... Gradient Norm: 0.0469\n",
      "Epoch 359: avg loss training: 2.7943,... Gradient Norm: 0.0681\n",
      "Epoch 360: avg loss training: 2.7943,... Gradient Norm: 0.0612\n",
      "Epoch 361: avg loss training: 2.7942,... Gradient Norm: 0.0614\n",
      "Epoch 362: avg loss training: 2.7942,... Gradient Norm: 0.0582\n",
      "Epoch 363: avg loss training: 2.7942,... Gradient Norm: 0.0505\n",
      "Epoch 364: avg loss training: 2.7942,... Gradient Norm: 0.0625\n",
      "Epoch 365: avg loss training: 2.7942,... Gradient Norm: 0.0639\n",
      "Epoch 366: avg loss training: 2.7942,... Gradient Norm: 0.0567\n",
      "Epoch 367: avg loss training: 2.7942,... Gradient Norm: 0.0767\n",
      "Epoch 368: avg loss training: 2.7941,... Gradient Norm: 0.1040\n",
      "Epoch 369: avg loss training: 2.7941,... Gradient Norm: 0.0658\n",
      "Epoch 370: avg loss training: 2.7941,... Gradient Norm: 0.0466\n",
      "Epoch 371: avg loss training: 2.7941,... Gradient Norm: 0.0885\n",
      "Epoch 372: avg loss training: 2.7941,... Gradient Norm: 0.0867\n",
      "Epoch 373: avg loss training: 2.7941,... Gradient Norm: 0.0467\n",
      "Epoch 374: avg loss training: 2.7941,... Gradient Norm: 0.0829\n",
      "Epoch 375: avg loss training: 2.7941,... Gradient Norm: 0.0912\n",
      "Epoch 376: avg loss training: 2.7940,... Gradient Norm: 0.0717\n",
      "Epoch 377: avg loss training: 2.7940,... Gradient Norm: 0.0499\n",
      "Epoch 378: avg loss training: 2.7940,... Gradient Norm: 0.0459\n",
      "Epoch 379: avg loss training: 2.7940,... Gradient Norm: 0.0429\n",
      "Epoch 380: avg loss training: 2.7940,... Gradient Norm: 0.0590\n",
      "Epoch 381: avg loss training: 2.7940,... Gradient Norm: 0.0648\n",
      "Epoch 382: avg loss training: 2.7940,... Gradient Norm: 0.0484\n",
      "Epoch 383: avg loss training: 2.7940,... Gradient Norm: 0.0718\n",
      "Epoch 384: avg loss training: 2.7940,... Gradient Norm: 0.0765\n",
      "Epoch 385: avg loss training: 2.7939,... Gradient Norm: 0.0477\n",
      "Epoch 386: avg loss training: 2.7939,... Gradient Norm: 0.1030\n",
      "Epoch 387: avg loss training: 2.7939,... Gradient Norm: 0.1121\n",
      "Epoch 388: avg loss training: 2.7939,... Gradient Norm: 0.0804\n",
      "Epoch 389: avg loss training: 2.7939,... Gradient Norm: 0.0731\n",
      "Epoch 390: avg loss training: 2.7939,... Gradient Norm: 0.0663\n",
      "Epoch 391: avg loss training: 2.7939,... Gradient Norm: 0.0555\n",
      "Epoch 392: avg loss training: 2.7939,... Gradient Norm: 0.0475\n",
      "Epoch 393: avg loss training: 2.7939,... Gradient Norm: 0.0497\n",
      "Epoch 394: avg loss training: 2.7938,... Gradient Norm: 0.0723\n",
      "Epoch 395: avg loss training: 2.7938,... Gradient Norm: 0.0980\n",
      "Epoch 396: avg loss training: 2.7938,... Gradient Norm: 0.0966\n",
      "Epoch 397: avg loss training: 2.7938,... Gradient Norm: 0.0676\n",
      "Epoch 398: avg loss training: 2.7938,... Gradient Norm: 0.0431\n",
      "Epoch 399: avg loss training: 2.7938,... Gradient Norm: 0.0542\n",
      "Epoch 400: avg loss training: 2.7938,... Gradient Norm: 0.0731\n",
      "Epoch 401: avg loss training: 2.7938,... Gradient Norm: 0.0699\n",
      "Epoch 402: avg loss training: 2.7938,... Gradient Norm: 0.0556\n",
      "Epoch 403: avg loss training: 2.7938,... Gradient Norm: 0.0629\n",
      "Epoch 404: avg loss training: 2.7937,... Gradient Norm: 0.0867\n",
      "Epoch 405: avg loss training: 2.7937,... Gradient Norm: 0.0911\n",
      "Epoch 406: avg loss training: 2.7937,... Gradient Norm: 0.0630\n",
      "Epoch 407: avg loss training: 2.7937,... Gradient Norm: 0.0399\n",
      "Epoch 408: avg loss training: 2.7937,... Gradient Norm: 0.0459\n",
      "Epoch 409: avg loss training: 2.7937,... Gradient Norm: 0.0546\n",
      "Epoch 410: avg loss training: 2.7937,... Gradient Norm: 0.0548\n",
      "Epoch 411: avg loss training: 2.7937,... Gradient Norm: 0.0458\n",
      "Epoch 412: avg loss training: 2.7937,... Gradient Norm: 0.0796\n",
      "Epoch 413: avg loss training: 2.7937,... Gradient Norm: 0.0944\n",
      "Epoch 414: avg loss training: 2.7937,... Gradient Norm: 0.0854\n",
      "Epoch 415: avg loss training: 2.7936,... Gradient Norm: 0.0417\n",
      "Epoch 416: avg loss training: 2.7936,... Gradient Norm: 0.0560\n",
      "Epoch 417: avg loss training: 2.7936,... Gradient Norm: 0.0648\n",
      "Epoch 418: avg loss training: 2.7936,... Gradient Norm: 0.0639\n",
      "Epoch 419: avg loss training: 2.7936,... Gradient Norm: 0.0492\n",
      "Epoch 420: avg loss training: 2.7936,... Gradient Norm: 0.0502\n",
      "Epoch 421: avg loss training: 2.7936,... Gradient Norm: 0.0834\n",
      "Epoch 422: avg loss training: 2.7936,... Gradient Norm: 0.0887\n",
      "Epoch 423: avg loss training: 2.7936,... Gradient Norm: 0.0622\n",
      "Epoch 424: avg loss training: 2.7936,... Gradient Norm: 0.0496\n",
      "Epoch 425: avg loss training: 2.7936,... Gradient Norm: 0.0538\n",
      "Epoch 426: avg loss training: 2.7936,... Gradient Norm: 0.0510\n",
      "Epoch 427: avg loss training: 2.7935,... Gradient Norm: 0.0464\n",
      "Epoch 428: avg loss training: 2.7935,... Gradient Norm: 0.0514\n",
      "Epoch 429: avg loss training: 2.7935,... Gradient Norm: 0.0700\n",
      "Epoch 430: avg loss training: 2.7935,... Gradient Norm: 0.0535\n",
      "Epoch 431: avg loss training: 2.7935,... Gradient Norm: 0.0459\n",
      "Epoch 432: avg loss training: 2.7935,... Gradient Norm: 0.0460\n",
      "Epoch 433: avg loss training: 2.7935,... Gradient Norm: 0.0523\n",
      "Epoch 434: avg loss training: 2.7935,... Gradient Norm: 0.0513\n",
      "Epoch 435: avg loss training: 2.7935,... Gradient Norm: 0.0508\n",
      "Epoch 436: avg loss training: 2.7935,... Gradient Norm: 0.0646\n",
      "Epoch 437: avg loss training: 2.7935,... Gradient Norm: 0.0582\n",
      "Epoch 438: avg loss training: 2.7935,... Gradient Norm: 0.0473\n",
      "Epoch 439: avg loss training: 2.7935,... Gradient Norm: 0.0512\n",
      "Epoch 440: avg loss training: 2.7934,... Gradient Norm: 0.0494\n",
      "Epoch 441: avg loss training: 2.7934,... Gradient Norm: 0.0673\n",
      "Epoch 442: avg loss training: 2.7934,... Gradient Norm: 0.0666\n",
      "Epoch 443: avg loss training: 2.7934,... Gradient Norm: 0.0636\n",
      "Epoch 444: avg loss training: 2.7934,... Gradient Norm: 0.0468\n",
      "Epoch 445: avg loss training: 2.7934,... Gradient Norm: 0.0449\n",
      "Epoch 446: avg loss training: 2.7934,... Gradient Norm: 0.0476\n",
      "Epoch 447: avg loss training: 2.7934,... Gradient Norm: 0.0589\n",
      "Epoch 448: avg loss training: 2.7934,... Gradient Norm: 0.0592\n",
      "Epoch 449: avg loss training: 2.7934,... Gradient Norm: 0.0569\n",
      "Epoch 450: avg loss training: 2.7934,... Gradient Norm: 0.0547\n",
      "Epoch 451: avg loss training: 2.7934,... Gradient Norm: 0.0570\n",
      "Epoch 452: avg loss training: 2.7934,... Gradient Norm: 0.0642\n",
      "Epoch 453: avg loss training: 2.7934,... Gradient Norm: 0.0709\n",
      "Epoch 454: avg loss training: 2.7934,... Gradient Norm: 0.0700\n",
      "Epoch 455: avg loss training: 2.7933,... Gradient Norm: 0.0537\n",
      "Epoch 456: avg loss training: 2.7933,... Gradient Norm: 0.0579\n",
      "Epoch 457: avg loss training: 2.7933,... Gradient Norm: 0.0575\n",
      "Epoch 458: avg loss training: 2.7933,... Gradient Norm: 0.0465\n",
      "Epoch 459: avg loss training: 2.7933,... Gradient Norm: 0.0524\n",
      "Epoch 460: avg loss training: 2.7933,... Gradient Norm: 0.0482\n",
      "Epoch 461: avg loss training: 2.7933,... Gradient Norm: 0.0463\n",
      "Epoch 462: avg loss training: 2.7933,... Gradient Norm: 0.0716\n",
      "Epoch 463: avg loss training: 2.7933,... Gradient Norm: 0.0821\n",
      "Epoch 464: avg loss training: 2.7933,... Gradient Norm: 0.0852\n",
      "Epoch 465: avg loss training: 2.7933,... Gradient Norm: 0.0417\n",
      "Epoch 466: avg loss training: 2.7933,... Gradient Norm: 0.0453\n",
      "Epoch 467: avg loss training: 2.7933,... Gradient Norm: 0.0498\n",
      "Epoch 468: avg loss training: 2.7933,... Gradient Norm: 0.0412\n",
      "Epoch 469: avg loss training: 2.7933,... Gradient Norm: 0.0444\n",
      "Epoch 470: avg loss training: 2.7933,... Gradient Norm: 0.0718\n",
      "Epoch 471: avg loss training: 2.7933,... Gradient Norm: 0.0636\n",
      "Epoch 472: avg loss training: 2.7933,... Gradient Norm: 0.0498\n",
      "Epoch 473: avg loss training: 2.7932,... Gradient Norm: 0.0485\n",
      "Epoch 474: avg loss training: 2.7932,... Gradient Norm: 0.0495\n",
      "Epoch 475: avg loss training: 2.7932,... Gradient Norm: 0.0504\n",
      "Epoch 476: avg loss training: 2.7932,... Gradient Norm: 0.0416\n",
      "Epoch 477: avg loss training: 2.7932,... Gradient Norm: 0.0544\n",
      "Epoch 478: avg loss training: 2.7932,... Gradient Norm: 0.0602\n",
      "Epoch 479: avg loss training: 2.7932,... Gradient Norm: 0.0561\n",
      "Epoch 480: avg loss training: 2.7932,... Gradient Norm: 0.0614\n",
      "Epoch 481: avg loss training: 2.7932,... Gradient Norm: 0.0540\n",
      "Epoch 482: avg loss training: 2.7932,... Gradient Norm: 0.0397\n",
      "Epoch 483: avg loss training: 2.7932,... Gradient Norm: 0.0391\n",
      "Epoch 484: avg loss training: 2.7932,... Gradient Norm: 0.0440\n",
      "Epoch 485: avg loss training: 2.7932,... Gradient Norm: 0.0524\n",
      "Epoch 486: avg loss training: 2.7932,... Gradient Norm: 0.0455\n",
      "Epoch 487: avg loss training: 2.7932,... Gradient Norm: 0.0450\n",
      "Epoch 488: avg loss training: 2.7932,... Gradient Norm: 0.0510\n",
      "Epoch 489: avg loss training: 2.7932,... Gradient Norm: 0.0583\n",
      "Epoch 490: avg loss training: 2.7932,... Gradient Norm: 0.0544\n",
      "Epoch 491: avg loss training: 2.7932,... Gradient Norm: 0.0471\n",
      "Epoch 492: avg loss training: 2.7932,... Gradient Norm: 0.0387\n",
      "Epoch 493: avg loss training: 2.7932,... Gradient Norm: 0.0544\n",
      "Epoch 494: avg loss training: 2.7932,... Gradient Norm: 0.0406\n",
      "Epoch 495: avg loss training: 2.7932,... Gradient Norm: 0.0492\n",
      "Epoch 496: avg loss training: 2.7932,... Gradient Norm: 0.0643\n",
      "Epoch 497: avg loss training: 2.7931,... Gradient Norm: 0.0527\n",
      "Epoch 498: avg loss training: 2.7931,... Gradient Norm: 0.0384\n",
      "Epoch 499: avg loss training: 2.7931,... Gradient Norm: 0.0567\n",
      "Epoch 500: avg loss training: 2.7931,... Gradient Norm: 0.0558\n",
      "Epoch 501: avg loss training: 2.7931,... Gradient Norm: 0.0456\n",
      "Epoch 502: avg loss training: 2.7931,... Gradient Norm: 0.0474\n",
      "Epoch 503: avg loss training: 2.7931,... Gradient Norm: 0.0433\n",
      "Epoch 504: avg loss training: 2.7931,... Gradient Norm: 0.0515\n",
      "Epoch 505: avg loss training: 2.7931,... Gradient Norm: 0.0729\n",
      "Epoch 506: avg loss training: 2.7931,... Gradient Norm: 0.0430\n",
      "Epoch 507: avg loss training: 2.7931,... Gradient Norm: 0.0419\n",
      "Epoch 508: avg loss training: 2.7931,... Gradient Norm: 0.0425\n",
      "Epoch 509: avg loss training: 2.7931,... Gradient Norm: 0.0553\n",
      "Epoch 510: avg loss training: 2.7931,... Gradient Norm: 0.0635\n",
      "Epoch 511: avg loss training: 2.7931,... Gradient Norm: 0.0610\n",
      "Epoch 512: avg loss training: 2.7931,... Gradient Norm: 0.0592\n",
      "Epoch 513: avg loss training: 2.7931,... Gradient Norm: 0.0534\n",
      "Epoch 514: avg loss training: 2.7931,... Gradient Norm: 0.0544\n",
      "Epoch 515: avg loss training: 2.7931,... Gradient Norm: 0.0440\n",
      "Epoch 516: avg loss training: 2.7931,... Gradient Norm: 0.0382\n",
      "Epoch 517: avg loss training: 2.7931,... Gradient Norm: 0.0512\n",
      "Epoch 518: avg loss training: 2.7931,... Gradient Norm: 0.0403\n",
      "Epoch 519: avg loss training: 2.7931,... Gradient Norm: 0.0403\n",
      "Epoch 520: avg loss training: 2.7931,... Gradient Norm: 0.0459\n",
      "Epoch 521: avg loss training: 2.7931,... Gradient Norm: 0.0723\n",
      "Epoch 522: avg loss training: 2.7931,... Gradient Norm: 0.0771\n",
      "Epoch 523: avg loss training: 2.7931,... Gradient Norm: 0.0603\n",
      "Epoch 524: avg loss training: 2.7931,... Gradient Norm: 0.0442\n",
      "Epoch 525: avg loss training: 2.7931,... Gradient Norm: 0.0421\n",
      "Epoch 526: avg loss training: 2.7931,... Gradient Norm: 0.0409\n",
      "Epoch 527: avg loss training: 2.7931,... Gradient Norm: 0.0547\n",
      "Epoch 528: avg loss training: 2.7930,... Gradient Norm: 0.0467\n",
      "Epoch 529: avg loss training: 2.7930,... Gradient Norm: 0.0447\n",
      "Epoch 530: avg loss training: 2.7930,... Gradient Norm: 0.0409\n",
      "Epoch 531: avg loss training: 2.7930,... Gradient Norm: 0.0537\n",
      "Epoch 532: avg loss training: 2.7930,... Gradient Norm: 0.0563\n",
      "Epoch 533: avg loss training: 2.7930,... Gradient Norm: 0.0677\n",
      "Epoch 534: avg loss training: 2.7930,... Gradient Norm: 0.0552\n",
      "Epoch 535: avg loss training: 2.7930,... Gradient Norm: 0.0520\n",
      "Epoch 536: avg loss training: 2.7930,... Gradient Norm: 0.0533\n",
      "Epoch 537: avg loss training: 2.7930,... Gradient Norm: 0.0529\n",
      "Epoch 538: avg loss training: 2.7930,... Gradient Norm: 0.0504\n",
      "Epoch 539: avg loss training: 2.7930,... Gradient Norm: 0.0434\n",
      "Epoch 540: avg loss training: 2.7930,... Gradient Norm: 0.0511\n",
      "Epoch 541: avg loss training: 2.7930,... Gradient Norm: 0.0419\n",
      "Epoch 542: avg loss training: 2.7930,... Gradient Norm: 0.0431\n",
      "Epoch 543: avg loss training: 2.7930,... Gradient Norm: 0.0494\n",
      "Epoch 544: avg loss training: 2.7930,... Gradient Norm: 0.0558\n",
      "Epoch 545: avg loss training: 2.7930,... Gradient Norm: 0.0494\n",
      "Epoch 546: avg loss training: 2.7930,... Gradient Norm: 0.0570\n",
      "Epoch 547: avg loss training: 2.7930,... Gradient Norm: 0.0608\n",
      "Epoch 548: avg loss training: 2.7930,... Gradient Norm: 0.0618\n",
      "Epoch 549: avg loss training: 2.7930,... Gradient Norm: 0.0425\n",
      "Epoch 550: avg loss training: 2.7930,... Gradient Norm: 0.0409\n",
      "Epoch 551: avg loss training: 2.7930,... Gradient Norm: 0.0426\n",
      "Epoch 552: avg loss training: 2.7930,... Gradient Norm: 0.0399\n",
      "Epoch 553: avg loss training: 2.7930,... Gradient Norm: 0.0471\n",
      "Epoch 554: avg loss training: 2.7930,... Gradient Norm: 0.0565\n",
      "Epoch 555: avg loss training: 2.7930,... Gradient Norm: 0.0483\n",
      "Epoch 556: avg loss training: 2.7930,... Gradient Norm: 0.0418\n",
      "Epoch 557: avg loss training: 2.7930,... Gradient Norm: 0.0467\n",
      "Epoch 558: avg loss training: 2.7930,... Gradient Norm: 0.0606\n",
      "Epoch 559: avg loss training: 2.7930,... Gradient Norm: 0.0512\n",
      "Epoch 560: avg loss training: 2.7930,... Gradient Norm: 0.0604\n",
      "Epoch 561: avg loss training: 2.7930,... Gradient Norm: 0.0593\n",
      "Epoch 562: avg loss training: 2.7930,... Gradient Norm: 0.0578\n",
      "Epoch 563: avg loss training: 2.7930,... Gradient Norm: 0.0424\n",
      "Epoch 564: avg loss training: 2.7930,... Gradient Norm: 0.0497\n",
      "Epoch 565: avg loss training: 2.7930,... Gradient Norm: 0.0532\n",
      "Epoch 566: avg loss training: 2.7929,... Gradient Norm: 0.0508\n",
      "Epoch 567: avg loss training: 2.7929,... Gradient Norm: 0.0438\n",
      "Epoch 568: avg loss training: 2.7929,... Gradient Norm: 0.0480\n",
      "Epoch 569: avg loss training: 2.7929,... Gradient Norm: 0.0601\n",
      "Epoch 570: avg loss training: 2.7929,... Gradient Norm: 0.0549\n",
      "Epoch 571: avg loss training: 2.7929,... Gradient Norm: 0.0500\n",
      "Epoch 572: avg loss training: 2.7929,... Gradient Norm: 0.0449\n",
      "Epoch 573: avg loss training: 2.7929,... Gradient Norm: 0.0476\n",
      "Epoch 574: avg loss training: 2.7929,... Gradient Norm: 0.0507\n",
      "Epoch 575: avg loss training: 2.7929,... Gradient Norm: 0.0549\n",
      "Epoch 576: avg loss training: 2.7929,... Gradient Norm: 0.0577\n",
      "Epoch 577: avg loss training: 2.7929,... Gradient Norm: 0.0545\n",
      "Epoch 578: avg loss training: 2.7929,... Gradient Norm: 0.0436\n",
      "Epoch 579: avg loss training: 2.7929,... Gradient Norm: 0.0429\n",
      "Epoch 580: avg loss training: 2.7929,... Gradient Norm: 0.0450\n",
      "Epoch 581: avg loss training: 2.7929,... Gradient Norm: 0.0507\n",
      "Epoch 582: avg loss training: 2.7929,... Gradient Norm: 0.0444\n",
      "Epoch 583: avg loss training: 2.7929,... Gradient Norm: 0.0585\n",
      "Epoch 584: avg loss training: 2.7929,... Gradient Norm: 0.0617\n",
      "Epoch 585: avg loss training: 2.7929,... Gradient Norm: 0.0604\n",
      "Epoch 586: avg loss training: 2.7929,... Gradient Norm: 0.0518\n",
      "Epoch 587: avg loss training: 2.7929,... Gradient Norm: 0.0601\n",
      "Epoch 588: avg loss training: 2.7929,... Gradient Norm: 0.0609\n",
      "Epoch 589: avg loss training: 2.7929,... Gradient Norm: 0.0603\n",
      "Epoch 590: avg loss training: 2.7929,... Gradient Norm: 0.0555\n",
      "Epoch 591: avg loss training: 2.7929,... Gradient Norm: 0.0539\n",
      "Epoch 592: avg loss training: 2.7929,... Gradient Norm: 0.0563\n",
      "Epoch 593: avg loss training: 2.7929,... Gradient Norm: 0.0758\n",
      "Epoch 594: avg loss training: 2.7929,... Gradient Norm: 0.0744\n",
      "Epoch 595: avg loss training: 2.7929,... Gradient Norm: 0.0736\n",
      "Epoch 596: avg loss training: 2.7929,... Gradient Norm: 0.0489\n",
      "Epoch 597: avg loss training: 2.7929,... Gradient Norm: 0.0433\n",
      "Epoch 598: avg loss training: 2.7929,... Gradient Norm: 0.0417\n",
      "Epoch 599: avg loss training: 2.7929,... Gradient Norm: 0.0523\n",
      "Epoch 600: avg loss training: 2.7929,... Gradient Norm: 0.0480\n",
      "Epoch 601: avg loss training: 2.7929,... Gradient Norm: 0.0541\n",
      "Epoch 602: avg loss training: 2.7929,... Gradient Norm: 0.0493\n",
      "Epoch 603: avg loss training: 2.7929,... Gradient Norm: 0.0522\n",
      "Epoch 604: avg loss training: 2.7929,... Gradient Norm: 0.0508\n",
      "Epoch 605: avg loss training: 2.7929,... Gradient Norm: 0.0490\n",
      "Epoch 606: avg loss training: 2.7929,... Gradient Norm: 0.0569\n",
      "Epoch 607: avg loss training: 2.7929,... Gradient Norm: 0.0608\n",
      "Epoch 608: avg loss training: 2.7929,... Gradient Norm: 0.0598\n",
      "Epoch 609: avg loss training: 2.7929,... Gradient Norm: 0.0610\n",
      "Epoch 610: avg loss training: 2.7929,... Gradient Norm: 0.0445\n",
      "Epoch 611: avg loss training: 2.7929,... Gradient Norm: 0.0441\n",
      "Epoch 612: avg loss training: 2.7929,... Gradient Norm: 0.0592\n",
      "Epoch 613: avg loss training: 2.7929,... Gradient Norm: 0.0537\n",
      "Epoch 614: avg loss training: 2.7929,... Gradient Norm: 0.0542\n",
      "Epoch 615: avg loss training: 2.7929,... Gradient Norm: 0.0587\n",
      "Epoch 616: avg loss training: 2.7929,... Gradient Norm: 0.0533\n",
      "Epoch 617: avg loss training: 2.7929,... Gradient Norm: 0.0627\n",
      "Epoch 618: avg loss training: 2.7929,... Gradient Norm: 0.0690\n",
      "Epoch 619: avg loss training: 2.7929,... Gradient Norm: 0.0562\n",
      "Epoch 620: avg loss training: 2.7929,... Gradient Norm: 0.0496\n",
      "Epoch 621: avg loss training: 2.7928,... Gradient Norm: 0.0568\n",
      "Epoch 622: avg loss training: 2.7928,... Gradient Norm: 0.0536\n",
      "Epoch 623: avg loss training: 2.7928,... Gradient Norm: 0.0531\n",
      "Epoch 624: avg loss training: 2.7928,... Gradient Norm: 0.0481\n",
      "Epoch 625: avg loss training: 2.7928,... Gradient Norm: 0.0482\n",
      "Epoch 626: avg loss training: 2.7928,... Gradient Norm: 0.0507\n",
      "Epoch 627: avg loss training: 2.7928,... Gradient Norm: 0.0566\n",
      "Epoch 628: avg loss training: 2.7928,... Gradient Norm: 0.0588\n",
      "Epoch 629: avg loss training: 2.7928,... Gradient Norm: 0.0517\n",
      "Epoch 630: avg loss training: 2.7928,... Gradient Norm: 0.0531\n",
      "Epoch 631: avg loss training: 2.7928,... Gradient Norm: 0.0554\n",
      "Epoch 632: avg loss training: 2.7928,... Gradient Norm: 0.0548\n",
      "Epoch 633: avg loss training: 2.7928,... Gradient Norm: 0.0537\n",
      "Epoch 634: avg loss training: 2.7928,... Gradient Norm: 0.0542\n",
      "Epoch 635: avg loss training: 2.7928,... Gradient Norm: 0.0531\n",
      "Epoch 636: avg loss training: 2.7928,... Gradient Norm: 0.0571\n",
      "Epoch 637: avg loss training: 2.7928,... Gradient Norm: 0.0528\n",
      "Epoch 638: avg loss training: 2.7928,... Gradient Norm: 0.0416\n",
      "Epoch 639: avg loss training: 2.7928,... Gradient Norm: 0.0491\n",
      "Epoch 640: avg loss training: 2.7928,... Gradient Norm: 0.0628\n",
      "Epoch 641: avg loss training: 2.7928,... Gradient Norm: 0.0476\n",
      "Epoch 642: avg loss training: 2.7928,... Gradient Norm: 0.0489\n",
      "Epoch 643: avg loss training: 2.7928,... Gradient Norm: 0.0527\n",
      "Epoch 644: avg loss training: 2.7928,... Gradient Norm: 0.0515\n",
      "Epoch 645: avg loss training: 2.7928,... Gradient Norm: 0.0594\n",
      "Epoch 646: avg loss training: 2.7928,... Gradient Norm: 0.0494\n",
      "Epoch 647: avg loss training: 2.7928,... Gradient Norm: 0.0513\n",
      "Epoch 648: avg loss training: 2.7928,... Gradient Norm: 0.0543\n",
      "Epoch 649: avg loss training: 2.7928,... Gradient Norm: 0.0575\n",
      "Epoch 650: avg loss training: 2.7928,... Gradient Norm: 0.0628\n",
      "Epoch 651: avg loss training: 2.7928,... Gradient Norm: 0.0592\n",
      "Epoch 652: avg loss training: 2.7928,... Gradient Norm: 0.0560\n",
      "Epoch 653: avg loss training: 2.7928,... Gradient Norm: 0.0419\n",
      "Epoch 654: avg loss training: 2.7928,... Gradient Norm: 0.0532\n",
      "Epoch 655: avg loss training: 2.7928,... Gradient Norm: 0.0567\n",
      "Epoch 656: avg loss training: 2.7928,... Gradient Norm: 0.0573\n",
      "Epoch 657: avg loss training: 2.7928,... Gradient Norm: 0.0608\n",
      "Epoch 658: avg loss training: 2.7928,... Gradient Norm: 0.0542\n",
      "Epoch 659: avg loss training: 2.7928,... Gradient Norm: 0.0449\n",
      "Epoch 660: avg loss training: 2.7928,... Gradient Norm: 0.0433\n",
      "Epoch 661: avg loss training: 2.7928,... Gradient Norm: 0.0478\n",
      "Epoch 662: avg loss training: 2.7928,... Gradient Norm: 0.0544\n",
      "Epoch 663: avg loss training: 2.7928,... Gradient Norm: 0.0633\n",
      "Epoch 664: avg loss training: 2.7928,... Gradient Norm: 0.0605\n",
      "Epoch 665: avg loss training: 2.7928,... Gradient Norm: 0.0476\n",
      "Epoch 666: avg loss training: 2.7928,... Gradient Norm: 0.0488\n",
      "Epoch 667: avg loss training: 2.7928,... Gradient Norm: 0.0523\n",
      "Epoch 668: avg loss training: 2.7928,... Gradient Norm: 0.0598\n",
      "Epoch 669: avg loss training: 2.7928,... Gradient Norm: 0.0594\n",
      "Epoch 670: avg loss training: 2.7928,... Gradient Norm: 0.0594\n",
      "Epoch 671: avg loss training: 2.7928,... Gradient Norm: 0.0552\n",
      "Epoch 672: avg loss training: 2.7928,... Gradient Norm: 0.0535\n",
      "Epoch 673: avg loss training: 2.7928,... Gradient Norm: 0.0584\n",
      "Epoch 674: avg loss training: 2.7928,... Gradient Norm: 0.0589\n",
      "Epoch 675: avg loss training: 2.7928,... Gradient Norm: 0.0604\n",
      "Epoch 676: avg loss training: 2.7928,... Gradient Norm: 0.0535\n",
      "Epoch 677: avg loss training: 2.7928,... Gradient Norm: 0.0410\n",
      "Epoch 678: avg loss training: 2.7928,... Gradient Norm: 0.0505\n",
      "Epoch 679: avg loss training: 2.7928,... Gradient Norm: 0.0558\n",
      "Epoch 680: avg loss training: 2.7928,... Gradient Norm: 0.0607\n",
      "Epoch 681: avg loss training: 2.7928,... Gradient Norm: 0.0566\n",
      "Epoch 682: avg loss training: 2.7928,... Gradient Norm: 0.0543\n",
      "Epoch 683: avg loss training: 2.7928,... Gradient Norm: 0.0436\n",
      "Epoch 684: avg loss training: 2.7928,... Gradient Norm: 0.0438\n",
      "Epoch 685: avg loss training: 2.7928,... Gradient Norm: 0.0546\n",
      "Epoch 686: avg loss training: 2.7928,... Gradient Norm: 0.0550\n",
      "Epoch 687: avg loss training: 2.7928,... Gradient Norm: 0.0476\n",
      "Epoch 688: avg loss training: 2.7928,... Gradient Norm: 0.0582\n",
      "Epoch 689: avg loss training: 2.7928,... Gradient Norm: 0.0568\n",
      "Epoch 690: avg loss training: 2.7928,... Gradient Norm: 0.0525\n",
      "Epoch 691: avg loss training: 2.7928,... Gradient Norm: 0.0529\n",
      "Epoch 692: avg loss training: 2.7928,... Gradient Norm: 0.0515\n",
      "Epoch 693: avg loss training: 2.7928,... Gradient Norm: 0.0498\n",
      "Epoch 694: avg loss training: 2.7928,... Gradient Norm: 0.0497\n",
      "Epoch 695: avg loss training: 2.7928,... Gradient Norm: 0.0459\n",
      "Epoch 696: avg loss training: 2.7928,... Gradient Norm: 0.0581\n",
      "Epoch 697: avg loss training: 2.7928,... Gradient Norm: 0.0541\n",
      "Epoch 698: avg loss training: 2.7928,... Gradient Norm: 0.0554\n",
      "Epoch 699: avg loss training: 2.7928,... Gradient Norm: 0.0564\n",
      "Epoch 700: avg loss training: 2.7928,... Gradient Norm: 0.0508\n",
      "Epoch 701: avg loss training: 2.7928,... Gradient Norm: 0.0544\n",
      "Epoch 702: avg loss training: 2.7928,... Gradient Norm: 0.0580\n",
      "Epoch 703: avg loss training: 2.7928,... Gradient Norm: 0.0597\n",
      "Epoch 704: avg loss training: 2.7928,... Gradient Norm: 0.0474\n",
      "Epoch 705: avg loss training: 2.7928,... Gradient Norm: 0.0459\n",
      "Epoch 706: avg loss training: 2.7928,... Gradient Norm: 0.0520\n",
      "Epoch 707: avg loss training: 2.7928,... Gradient Norm: 0.0572\n",
      "Epoch 708: avg loss training: 2.7928,... Gradient Norm: 0.0467\n",
      "Epoch 709: avg loss training: 2.7928,... Gradient Norm: 0.0543\n",
      "Epoch 710: avg loss training: 2.7928,... Gradient Norm: 0.0535\n",
      "Epoch 711: avg loss training: 2.7928,... Gradient Norm: 0.0490\n",
      "Epoch 712: avg loss training: 2.7928,... Gradient Norm: 0.0462\n",
      "Epoch 713: avg loss training: 2.7928,... Gradient Norm: 0.0487\n",
      "Epoch 714: avg loss training: 2.7928,... Gradient Norm: 0.0580\n",
      "Epoch 715: avg loss training: 2.7928,... Gradient Norm: 0.0563\n",
      "Epoch 716: avg loss training: 2.7928,... Gradient Norm: 0.0518\n",
      "Epoch 717: avg loss training: 2.7928,... Gradient Norm: 0.0498\n",
      "Epoch 718: avg loss training: 2.7928,... Gradient Norm: 0.0465\n",
      "Epoch 719: avg loss training: 2.7928,... Gradient Norm: 0.0510\n",
      "Epoch 720: avg loss training: 2.7928,... Gradient Norm: 0.0660\n",
      "Epoch 721: avg loss training: 2.7928,... Gradient Norm: 0.0650\n",
      "Epoch 722: avg loss training: 2.7928,... Gradient Norm: 0.0503\n",
      "Epoch 723: avg loss training: 2.7928,... Gradient Norm: 0.0556\n",
      "Epoch 724: avg loss training: 2.7928,... Gradient Norm: 0.0454\n",
      "Epoch 725: avg loss training: 2.7928,... Gradient Norm: 0.0500\n",
      "Epoch 726: avg loss training: 2.7928,... Gradient Norm: 0.0500\n",
      "Epoch 727: avg loss training: 2.7927,... Gradient Norm: 0.0462\n",
      "Epoch 728: avg loss training: 2.7927,... Gradient Norm: 0.0465\n",
      "Epoch 729: avg loss training: 2.7927,... Gradient Norm: 0.0526\n",
      "Epoch 730: avg loss training: 2.7927,... Gradient Norm: 0.0569\n",
      "Epoch 731: avg loss training: 2.7927,... Gradient Norm: 0.0595\n",
      "Epoch 732: avg loss training: 2.7927,... Gradient Norm: 0.0545\n",
      "Epoch 733: avg loss training: 2.7927,... Gradient Norm: 0.0472\n",
      "Epoch 734: avg loss training: 2.7927,... Gradient Norm: 0.0507\n",
      "Epoch 735: avg loss training: 2.7927,... Gradient Norm: 0.0513\n",
      "Epoch 736: avg loss training: 2.7927,... Gradient Norm: 0.0529\n",
      "Epoch 737: avg loss training: 2.7927,... Gradient Norm: 0.0573\n",
      "Epoch 738: avg loss training: 2.7927,... Gradient Norm: 0.0548\n",
      "Epoch 739: avg loss training: 2.7927,... Gradient Norm: 0.0523\n",
      "Epoch 740: avg loss training: 2.7927,... Gradient Norm: 0.0515\n",
      "Epoch 741: avg loss training: 2.7927,... Gradient Norm: 0.0498\n",
      "Epoch 742: avg loss training: 2.7927,... Gradient Norm: 0.0526\n",
      "Epoch 743: avg loss training: 2.7927,... Gradient Norm: 0.0528\n",
      "Epoch 744: avg loss training: 2.7927,... Gradient Norm: 0.0466\n",
      "Epoch 745: avg loss training: 2.7927,... Gradient Norm: 0.0438\n",
      "Epoch 746: avg loss training: 2.7927,... Gradient Norm: 0.0460\n",
      "Epoch 747: avg loss training: 2.7927,... Gradient Norm: 0.0547\n",
      "Epoch 748: avg loss training: 2.7927,... Gradient Norm: 0.0571\n",
      "Epoch 749: avg loss training: 2.7927,... Gradient Norm: 0.0613\n",
      "Epoch 750: avg loss training: 2.7927,... Gradient Norm: 0.0643\n",
      "Epoch 751: avg loss training: 2.7927,... Gradient Norm: 0.0475\n",
      "Epoch 752: avg loss training: 2.7927,... Gradient Norm: 0.0470\n",
      "Epoch 753: avg loss training: 2.7927,... Gradient Norm: 0.0426\n",
      "Epoch 754: avg loss training: 2.7927,... Gradient Norm: 0.0554\n",
      "Epoch 755: avg loss training: 2.7927,... Gradient Norm: 0.0516\n",
      "Epoch 756: avg loss training: 2.7927,... Gradient Norm: 0.0525\n",
      "Epoch 757: avg loss training: 2.7927,... Gradient Norm: 0.0538\n",
      "Epoch 758: avg loss training: 2.7927,... Gradient Norm: 0.0495\n",
      "Epoch 759: avg loss training: 2.7927,... Gradient Norm: 0.0523\n",
      "Epoch 760: avg loss training: 2.7927,... Gradient Norm: 0.0524\n",
      "Epoch 761: avg loss training: 2.7927,... Gradient Norm: 0.0495\n",
      "Epoch 762: avg loss training: 2.7927,... Gradient Norm: 0.0513\n",
      "Epoch 763: avg loss training: 2.7927,... Gradient Norm: 0.0535\n",
      "Epoch 764: avg loss training: 2.7927,... Gradient Norm: 0.0506\n",
      "Epoch 765: avg loss training: 2.7927,... Gradient Norm: 0.0434\n",
      "Epoch 766: avg loss training: 2.7927,... Gradient Norm: 0.0492\n",
      "Epoch 767: avg loss training: 2.7927,... Gradient Norm: 0.0544\n",
      "Epoch 768: avg loss training: 2.7927,... Gradient Norm: 0.0547\n",
      "Epoch 769: avg loss training: 2.7927,... Gradient Norm: 0.0545\n",
      "Epoch 770: avg loss training: 2.7927,... Gradient Norm: 0.0467\n",
      "Epoch 771: avg loss training: 2.7927,... Gradient Norm: 0.0500\n",
      "Epoch 772: avg loss training: 2.7927,... Gradient Norm: 0.0490\n",
      "Epoch 773: avg loss training: 2.7927,... Gradient Norm: 0.0495\n",
      "Epoch 774: avg loss training: 2.7927,... Gradient Norm: 0.0483\n",
      "Epoch 775: avg loss training: 2.7927,... Gradient Norm: 0.0524\n",
      "Epoch 776: avg loss training: 2.7927,... Gradient Norm: 0.0512\n",
      "Epoch 777: avg loss training: 2.7927,... Gradient Norm: 0.0589\n",
      "Epoch 778: avg loss training: 2.7927,... Gradient Norm: 0.0475\n",
      "Epoch 779: avg loss training: 2.7927,... Gradient Norm: 0.0480\n",
      "Epoch 780: avg loss training: 2.7927,... Gradient Norm: 0.0461\n",
      "Epoch 781: avg loss training: 2.7927,... Gradient Norm: 0.0450\n",
      "Epoch 782: avg loss training: 2.7927,... Gradient Norm: 0.0438\n",
      "Epoch 783: avg loss training: 2.7927,... Gradient Norm: 0.0588\n",
      "Epoch 784: avg loss training: 2.7927,... Gradient Norm: 0.0660\n",
      "Epoch 785: avg loss training: 2.7927,... Gradient Norm: 0.0526\n",
      "Epoch 786: avg loss training: 2.7927,... Gradient Norm: 0.0489\n",
      "Epoch 787: avg loss training: 2.7927,... Gradient Norm: 0.0536\n",
      "Epoch 788: avg loss training: 2.7927,... Gradient Norm: 0.0495\n",
      "Epoch 789: avg loss training: 2.7927,... Gradient Norm: 0.0493\n",
      "Epoch 790: avg loss training: 2.7927,... Gradient Norm: 0.0496\n",
      "Epoch 791: avg loss training: 2.7927,... Gradient Norm: 0.0497\n",
      "Epoch 792: avg loss training: 2.7927,... Gradient Norm: 0.0539\n",
      "Epoch 793: avg loss training: 2.7927,... Gradient Norm: 0.0566\n",
      "Epoch 794: avg loss training: 2.7927,... Gradient Norm: 0.0428\n",
      "Epoch 795: avg loss training: 2.7927,... Gradient Norm: 0.0440\n",
      "Epoch 796: avg loss training: 2.7927,... Gradient Norm: 0.0459\n",
      "Epoch 797: avg loss training: 2.7927,... Gradient Norm: 0.0530\n",
      "Epoch 798: avg loss training: 2.7927,... Gradient Norm: 0.0478\n",
      "Epoch 799: avg loss training: 2.7927,... Gradient Norm: 0.0490\n",
      "Epoch 800: avg loss training: 2.7927,... Gradient Norm: 0.0485\n",
      "Epoch 801: avg loss training: 2.7927,... Gradient Norm: 0.0519\n",
      "Epoch 802: avg loss training: 2.7927,... Gradient Norm: 0.0517\n",
      "Epoch 803: avg loss training: 2.7927,... Gradient Norm: 0.0494\n",
      "Epoch 804: avg loss training: 2.7927,... Gradient Norm: 0.0508\n",
      "Epoch 805: avg loss training: 2.7927,... Gradient Norm: 0.0464\n",
      "Epoch 806: avg loss training: 2.7927,... Gradient Norm: 0.0481\n",
      "Epoch 807: avg loss training: 2.7927,... Gradient Norm: 0.0455\n",
      "Epoch 808: avg loss training: 2.7927,... Gradient Norm: 0.0432\n",
      "Epoch 809: avg loss training: 2.7927,... Gradient Norm: 0.0579\n",
      "Epoch 810: avg loss training: 2.7927,... Gradient Norm: 0.0604\n",
      "Epoch 811: avg loss training: 2.7927,... Gradient Norm: 0.0512\n",
      "Epoch 812: avg loss training: 2.7927,... Gradient Norm: 0.0462\n",
      "Epoch 813: avg loss training: 2.7927,... Gradient Norm: 0.0433\n",
      "Epoch 814: avg loss training: 2.7927,... Gradient Norm: 0.0561\n",
      "Epoch 815: avg loss training: 2.7927,... Gradient Norm: 0.0528\n",
      "Epoch 816: avg loss training: 2.7927,... Gradient Norm: 0.0467\n",
      "Epoch 817: avg loss training: 2.7927,... Gradient Norm: 0.0503\n",
      "Epoch 818: avg loss training: 2.7927,... Gradient Norm: 0.0559\n",
      "Epoch 819: avg loss training: 2.7927,... Gradient Norm: 0.0489\n",
      "Epoch 820: avg loss training: 2.7927,... Gradient Norm: 0.0445\n",
      "Epoch 821: avg loss training: 2.7927,... Gradient Norm: 0.0458\n",
      "Epoch 822: avg loss training: 2.7927,... Gradient Norm: 0.0488\n",
      "Epoch 823: avg loss training: 2.7927,... Gradient Norm: 0.0452\n",
      "Epoch 824: avg loss training: 2.7927,... Gradient Norm: 0.0548\n",
      "Epoch 825: avg loss training: 2.7927,... Gradient Norm: 0.0497\n",
      "Epoch 826: avg loss training: 2.7927,... Gradient Norm: 0.0520\n",
      "Epoch 827: avg loss training: 2.7927,... Gradient Norm: 0.0476\n",
      "Epoch 828: avg loss training: 2.7927,... Gradient Norm: 0.0526\n",
      "Epoch 829: avg loss training: 2.7927,... Gradient Norm: 0.0508\n",
      "Epoch 830: avg loss training: 2.7927,... Gradient Norm: 0.0466\n",
      "Epoch 831: avg loss training: 2.7927,... Gradient Norm: 0.0505\n",
      "Epoch 832: avg loss training: 2.7927,... Gradient Norm: 0.0537\n",
      "Epoch 833: avg loss training: 2.7927,... Gradient Norm: 0.0554\n",
      "Epoch 834: avg loss training: 2.7927,... Gradient Norm: 0.0480\n",
      "Epoch 835: avg loss training: 2.7927,... Gradient Norm: 0.0504\n",
      "Epoch 836: avg loss training: 2.7927,... Gradient Norm: 0.0463\n",
      "Epoch 837: avg loss training: 2.7927,... Gradient Norm: 0.0451\n",
      "Epoch 838: avg loss training: 2.7927,... Gradient Norm: 0.0581\n",
      "Epoch 839: avg loss training: 2.7927,... Gradient Norm: 0.0482\n",
      "Epoch 840: avg loss training: 2.7927,... Gradient Norm: 0.0450\n",
      "Epoch 841: avg loss training: 2.7927,... Gradient Norm: 0.0475\n",
      "Epoch 842: avg loss training: 2.7927,... Gradient Norm: 0.0528\n",
      "Epoch 843: avg loss training: 2.7927,... Gradient Norm: 0.0628\n",
      "Epoch 844: avg loss training: 2.7927,... Gradient Norm: 0.0528\n",
      "Epoch 845: avg loss training: 2.7927,... Gradient Norm: 0.0444\n",
      "Epoch 846: avg loss training: 2.7927,... Gradient Norm: 0.0451\n",
      "Epoch 847: avg loss training: 2.7927,... Gradient Norm: 0.0526\n",
      "Epoch 848: avg loss training: 2.7927,... Gradient Norm: 0.0482\n",
      "Epoch 849: avg loss training: 2.7927,... Gradient Norm: 0.0466\n",
      "Epoch 850: avg loss training: 2.7927,... Gradient Norm: 0.0476\n",
      "Epoch 851: avg loss training: 2.7927,... Gradient Norm: 0.0563\n",
      "Epoch 852: avg loss training: 2.7927,... Gradient Norm: 0.0535\n",
      "Epoch 853: avg loss training: 2.7927,... Gradient Norm: 0.0494\n",
      "Epoch 854: avg loss training: 2.7927,... Gradient Norm: 0.0486\n",
      "Epoch 855: avg loss training: 2.7927,... Gradient Norm: 0.0490\n",
      "Epoch 856: avg loss training: 2.7927,... Gradient Norm: 0.0482\n",
      "Epoch 857: avg loss training: 2.7927,... Gradient Norm: 0.0472\n",
      "Epoch 858: avg loss training: 2.7927,... Gradient Norm: 0.0509\n",
      "Epoch 859: avg loss training: 2.7927,... Gradient Norm: 0.0478\n",
      "Epoch 860: avg loss training: 2.7927,... Gradient Norm: 0.0472\n",
      "Epoch 861: avg loss training: 2.7927,... Gradient Norm: 0.0573\n",
      "Epoch 862: avg loss training: 2.7927,... Gradient Norm: 0.0504\n",
      "Epoch 863: avg loss training: 2.7927,... Gradient Norm: 0.0498\n",
      "Epoch 864: avg loss training: 2.7927,... Gradient Norm: 0.0479\n",
      "Epoch 865: avg loss training: 2.7927,... Gradient Norm: 0.0542\n",
      "Epoch 866: avg loss training: 2.7927,... Gradient Norm: 0.0486\n",
      "Epoch 867: avg loss training: 2.7927,... Gradient Norm: 0.0479\n",
      "Epoch 868: avg loss training: 2.7927,... Gradient Norm: 0.0547\n",
      "Epoch 869: avg loss training: 2.7927,... Gradient Norm: 0.0549\n",
      "Epoch 870: avg loss training: 2.7927,... Gradient Norm: 0.0457\n",
      "Epoch 871: avg loss training: 2.7927,... Gradient Norm: 0.0475\n",
      "Epoch 872: avg loss training: 2.7927,... Gradient Norm: 0.0496\n",
      "Epoch 873: avg loss training: 2.7927,... Gradient Norm: 0.0490\n",
      "Epoch 874: avg loss training: 2.7927,... Gradient Norm: 0.0515\n",
      "Epoch 875: avg loss training: 2.7927,... Gradient Norm: 0.0558\n",
      "Epoch 876: avg loss training: 2.7927,... Gradient Norm: 0.0509\n",
      "Epoch 877: avg loss training: 2.7927,... Gradient Norm: 0.0465\n",
      "Epoch 878: avg loss training: 2.7927,... Gradient Norm: 0.0519\n",
      "Epoch 879: avg loss training: 2.7927,... Gradient Norm: 0.0460\n",
      "Epoch 880: avg loss training: 2.7927,... Gradient Norm: 0.0495\n",
      "Epoch 881: avg loss training: 2.7927,... Gradient Norm: 0.0459\n",
      "Epoch 882: avg loss training: 2.7927,... Gradient Norm: 0.0488\n",
      "Epoch 883: avg loss training: 2.7927,... Gradient Norm: 0.0527\n",
      "Epoch 884: avg loss training: 2.7927,... Gradient Norm: 0.0470\n",
      "Epoch 885: avg loss training: 2.7927,... Gradient Norm: 0.0495\n",
      "Epoch 886: avg loss training: 2.7927,... Gradient Norm: 0.0492\n",
      "Epoch 887: avg loss training: 2.7927,... Gradient Norm: 0.0498\n",
      "Epoch 888: avg loss training: 2.7927,... Gradient Norm: 0.0594\n",
      "Epoch 889: avg loss training: 2.7927,... Gradient Norm: 0.0535\n",
      "Epoch 890: avg loss training: 2.7927,... Gradient Norm: 0.0448\n",
      "Epoch 891: avg loss training: 2.7927,... Gradient Norm: 0.0452\n",
      "Epoch 892: avg loss training: 2.7927,... Gradient Norm: 0.0472\n",
      "Epoch 893: avg loss training: 2.7927,... Gradient Norm: 0.0471\n",
      "Epoch 894: avg loss training: 2.7927,... Gradient Norm: 0.0538\n",
      "Epoch 895: avg loss training: 2.7927,... Gradient Norm: 0.0505\n",
      "Epoch 896: avg loss training: 2.7927,... Gradient Norm: 0.0600\n",
      "Epoch 897: avg loss training: 2.7927,... Gradient Norm: 0.0493\n",
      "Epoch 898: avg loss training: 2.7927,... Gradient Norm: 0.0531\n",
      "Epoch 899: avg loss training: 2.7927,... Gradient Norm: 0.0499\n",
      "Epoch 900: avg loss training: 2.7927,... Gradient Norm: 0.0442\n",
      "Epoch 901: avg loss training: 2.7927,... Gradient Norm: 0.0470\n",
      "Epoch 902: avg loss training: 2.7927,... Gradient Norm: 0.0529\n",
      "Epoch 903: avg loss training: 2.7927,... Gradient Norm: 0.0512\n",
      "Epoch 904: avg loss training: 2.7927,... Gradient Norm: 0.0481\n",
      "Epoch 905: avg loss training: 2.7927,... Gradient Norm: 0.0487\n",
      "Epoch 906: avg loss training: 2.7927,... Gradient Norm: 0.0507\n",
      "Epoch 907: avg loss training: 2.7927,... Gradient Norm: 0.0565\n",
      "Epoch 908: avg loss training: 2.7927,... Gradient Norm: 0.0615\n",
      "Epoch 909: avg loss training: 2.7927,... Gradient Norm: 0.0478\n",
      "Epoch 910: avg loss training: 2.7927,... Gradient Norm: 0.0463\n",
      "Epoch 911: avg loss training: 2.7927,... Gradient Norm: 0.0509\n",
      "Epoch 912: avg loss training: 2.7927,... Gradient Norm: 0.0513\n",
      "Epoch 913: avg loss training: 2.7927,... Gradient Norm: 0.0518\n",
      "Epoch 914: avg loss training: 2.7927,... Gradient Norm: 0.0548\n",
      "Epoch 915: avg loss training: 2.7927,... Gradient Norm: 0.0467\n",
      "Epoch 916: avg loss training: 2.7927,... Gradient Norm: 0.0490\n",
      "Epoch 917: avg loss training: 2.7927,... Gradient Norm: 0.0551\n",
      "Epoch 918: avg loss training: 2.7927,... Gradient Norm: 0.0539\n",
      "Epoch 919: avg loss training: 2.7927,... Gradient Norm: 0.0529\n",
      "Epoch 920: avg loss training: 2.7927,... Gradient Norm: 0.0494\n",
      "Epoch 921: avg loss training: 2.7927,... Gradient Norm: 0.0467\n",
      "Epoch 922: avg loss training: 2.7927,... Gradient Norm: 0.0583\n",
      "Epoch 923: avg loss training: 2.7927,... Gradient Norm: 0.0521\n",
      "Epoch 924: avg loss training: 2.7927,... Gradient Norm: 0.0463\n",
      "Epoch 925: avg loss training: 2.7927,... Gradient Norm: 0.0540\n",
      "Epoch 926: avg loss training: 2.7927,... Gradient Norm: 0.0506\n",
      "Epoch 927: avg loss training: 2.7927,... Gradient Norm: 0.0542\n",
      "Epoch 928: avg loss training: 2.7927,... Gradient Norm: 0.0551\n",
      "Epoch 929: avg loss training: 2.7927,... Gradient Norm: 0.0506\n",
      "Epoch 930: avg loss training: 2.7927,... Gradient Norm: 0.0458\n",
      "Epoch 931: avg loss training: 2.7927,... Gradient Norm: 0.0500\n",
      "Epoch 932: avg loss training: 2.7927,... Gradient Norm: 0.0521\n",
      "Epoch 933: avg loss training: 2.7927,... Gradient Norm: 0.0471\n",
      "Epoch 934: avg loss training: 2.7927,... Gradient Norm: 0.0489\n",
      "Epoch 935: avg loss training: 2.7927,... Gradient Norm: 0.0476\n",
      "Epoch 936: avg loss training: 2.7927,... Gradient Norm: 0.0506\n",
      "Epoch 937: avg loss training: 2.7927,... Gradient Norm: 0.0552\n",
      "Epoch 938: avg loss training: 2.7927,... Gradient Norm: 0.0487\n",
      "Epoch 939: avg loss training: 2.7927,... Gradient Norm: 0.0513\n",
      "Epoch 940: avg loss training: 2.7927,... Gradient Norm: 0.0462\n",
      "Epoch 941: avg loss training: 2.7927,... Gradient Norm: 0.0495\n",
      "Epoch 942: avg loss training: 2.7927,... Gradient Norm: 0.0449\n",
      "Epoch 943: avg loss training: 2.7927,... Gradient Norm: 0.0469\n",
      "Epoch 944: avg loss training: 2.7927,... Gradient Norm: 0.0526\n",
      "Epoch 945: avg loss training: 2.7927,... Gradient Norm: 0.0623\n",
      "Epoch 946: avg loss training: 2.7927,... Gradient Norm: 0.0464\n",
      "Epoch 947: avg loss training: 2.7927,... Gradient Norm: 0.0454\n",
      "Epoch 948: avg loss training: 2.7927,... Gradient Norm: 0.0507\n",
      "Epoch 949: avg loss training: 2.7927,... Gradient Norm: 0.0489\n",
      "Epoch 950: avg loss training: 2.7927,... Gradient Norm: 0.0532\n",
      "Epoch 951: avg loss training: 2.7927,... Gradient Norm: 0.0447\n",
      "Epoch 952: avg loss training: 2.7927,... Gradient Norm: 0.0496\n",
      "Epoch 953: avg loss training: 2.7927,... Gradient Norm: 0.0524\n",
      "Epoch 954: avg loss training: 2.7927,... Gradient Norm: 0.0586\n",
      "Epoch 955: avg loss training: 2.7927,... Gradient Norm: 0.0560\n",
      "Epoch 956: avg loss training: 2.7927,... Gradient Norm: 0.0555\n",
      "Epoch 957: avg loss training: 2.7927,... Gradient Norm: 0.0463\n",
      "Epoch 958: avg loss training: 2.7927,... Gradient Norm: 0.0453\n",
      "Epoch 959: avg loss training: 2.7927,... Gradient Norm: 0.0465\n",
      "Epoch 960: avg loss training: 2.7927,... Gradient Norm: 0.0515\n",
      "Epoch 961: avg loss training: 2.7927,... Gradient Norm: 0.0589\n",
      "Epoch 962: avg loss training: 2.7927,... Gradient Norm: 0.0573\n",
      "Epoch 963: avg loss training: 2.7927,... Gradient Norm: 0.0598\n",
      "Epoch 964: avg loss training: 2.7927,... Gradient Norm: 0.0525\n",
      "Epoch 965: avg loss training: 2.7927,... Gradient Norm: 0.0456\n",
      "Epoch 966: avg loss training: 2.7927,... Gradient Norm: 0.0429\n",
      "Epoch 967: avg loss training: 2.7927,... Gradient Norm: 0.0483\n",
      "Epoch 968: avg loss training: 2.7927,... Gradient Norm: 0.0461\n",
      "Epoch 969: avg loss training: 2.7927,... Gradient Norm: 0.0526\n",
      "Epoch 970: avg loss training: 2.7927,... Gradient Norm: 0.0462\n",
      "Epoch 971: avg loss training: 2.7927,... Gradient Norm: 0.0500\n",
      "Epoch 972: avg loss training: 2.7927,... Gradient Norm: 0.0533\n",
      "Epoch 973: avg loss training: 2.7927,... Gradient Norm: 0.0473\n",
      "Epoch 974: avg loss training: 2.7927,... Gradient Norm: 0.0510\n",
      "Epoch 975: avg loss training: 2.7927,... Gradient Norm: 0.0534\n",
      "Epoch 976: avg loss training: 2.7927,... Gradient Norm: 0.0554\n",
      "Epoch 977: avg loss training: 2.7927,... Gradient Norm: 0.0458\n",
      "Epoch 978: avg loss training: 2.7927,... Gradient Norm: 0.0487\n",
      "Epoch 979: avg loss training: 2.7927,... Gradient Norm: 0.0505\n",
      "Epoch 980: avg loss training: 2.7927,... Gradient Norm: 0.0467\n",
      "Epoch 981: avg loss training: 2.7927,... Gradient Norm: 0.0441\n",
      "Epoch 982: avg loss training: 2.7927,... Gradient Norm: 0.0510\n",
      "Epoch 983: avg loss training: 2.7927,... Gradient Norm: 0.0446\n",
      "Epoch 984: avg loss training: 2.7927,... Gradient Norm: 0.0476\n",
      "Epoch 985: avg loss training: 2.7927,... Gradient Norm: 0.0537\n",
      "Epoch 986: avg loss training: 2.7927,... Gradient Norm: 0.0569\n",
      "Epoch 987: avg loss training: 2.7927,... Gradient Norm: 0.0510\n",
      "Epoch 988: avg loss training: 2.7927,... Gradient Norm: 0.0536\n",
      "Epoch 989: avg loss training: 2.7927,... Gradient Norm: 0.0462\n",
      "Epoch 990: avg loss training: 2.7927,... Gradient Norm: 0.0465\n",
      "Epoch 991: avg loss training: 2.7927,... Gradient Norm: 0.0541\n",
      "Epoch 992: avg loss training: 2.7927,... Gradient Norm: 0.0474\n",
      "Epoch 993: avg loss training: 2.7927,... Gradient Norm: 0.0481\n",
      "Epoch 994: avg loss training: 2.7927,... Gradient Norm: 0.0500\n",
      "Epoch 995: avg loss training: 2.7927,... Gradient Norm: 0.0554\n",
      "Epoch 996: avg loss training: 2.7927,... Gradient Norm: 0.0530\n",
      "Epoch 997: avg loss training: 2.7927,... Gradient Norm: 0.0514\n",
      "Epoch 998: avg loss training: 2.7927,... Gradient Norm: 0.0559\n",
      "Epoch 999: avg loss training: 2.7927,... Gradient Norm: 0.0485\n",
      "Epoch 1000: avg loss training: 2.7927,... Gradient Norm: 0.0477\n",
      "final result: (0.028599294911795026, -0.02327558111054373) ********************\n"
     ]
    }
   ],
   "source": [
    "# plt.hist(y)\n",
    "# plt.show()\n",
    "# new, orig\n",
    "# wo std: final result: (0.022909228520449743, -0.0690273597669036) ********************\n",
    "# w std: final result: (0.028599294911795026, -0.02327558111054373) ********************\n",
    "\n",
    "dataset = SIM(100, preprocessor=StandardScaler(), double=True) # 1000 obs.\n",
    "x, y = dataset.cause.flatten().numpy(), dataset.effect.flatten().numpy()\n",
    "\n",
    "score_new, score_orig, model_x, model_y, new_x_f, new_x_r, f_forward, f_reverse= loci_w_marginal(\n",
    "    x, y, independence_test=False, neural_network=True, \n",
    "    return_function=True, n_steps=1000, marginal_loglik = True\n",
    ")\n",
    "\n",
    "print(f\"final result: {score_new, score_orig} ********************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1855ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0690273597669036\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = SIM(100, preprocessor=None, double=True) # 1000 obs.\n",
    "x, y = dataset.cause.flatten().numpy(), dataset.effect.flatten().numpy()\n",
    "# This function call shows all options of LOCI, we return_function to visualize the estimator\n",
    "score, f_forward, f_reverse = loci(\n",
    "    x, y, independence_test=False, neural_network=True, \n",
    "    return_function=True, n_steps=1000\n",
    ")\n",
    "print(score)\n",
    "plot_pair(x, y, f_forward, f_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f433d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: avg loss training: 3.5056,... Gradient Norm: 7.0747\n",
      "Epoch 2: avg loss training: 120.5641,... Gradient Norm: 1372.0607\n",
      "Epoch 3: avg loss training: 9.3453,... Gradient Norm: 79.3654\n",
      "Epoch 4: avg loss training: 32.6629,... Gradient Norm: 404.1977\n",
      "Epoch 5: avg loss training: 13.3740,... Gradient Norm: 118.2449\n",
      "Epoch 6: avg loss training: 10.6215,... Gradient Norm: 59.4674\n",
      "Epoch 7: avg loss training: 11.9939,... Gradient Norm: 100.9173\n",
      "Epoch 8: avg loss training: 7.2389,... Gradient Norm: 31.9358\n",
      "Epoch 9: avg loss training: 5.2544,... Gradient Norm: 24.8780\n",
      "Epoch 10: avg loss training: 4.0854,... Gradient Norm: 10.1568\n",
      "Epoch 11: avg loss training: 4.0477,... Gradient Norm: 10.2384\n",
      "Epoch 12: avg loss training: 4.4805,... Gradient Norm: 17.5710\n",
      "Epoch 13: avg loss training: 4.6511,... Gradient Norm: 19.1036\n",
      "Epoch 14: avg loss training: 4.4699,... Gradient Norm: 16.1959\n",
      "Epoch 15: avg loss training: 4.1469,... Gradient Norm: 11.4120\n",
      "Epoch 16: avg loss training: 3.9182,... Gradient Norm: 8.6553\n",
      "Epoch 17: avg loss training: 3.8537,... Gradient Norm: 8.1019\n",
      "Epoch 18: avg loss training: 3.6771,... Gradient Norm: 5.3468\n",
      "Epoch 19: avg loss training: 3.5645,... Gradient Norm: 6.2252\n",
      "Epoch 20: avg loss training: 3.4564,... Gradient Norm: 6.2304\n",
      "Epoch 21: avg loss training: 3.3507,... Gradient Norm: 3.7966\n",
      "Epoch 22: avg loss training: 3.3265,... Gradient Norm: 4.1842\n",
      "Epoch 23: avg loss training: 3.3367,... Gradient Norm: 6.5697\n",
      "Epoch 24: avg loss training: 3.2738,... Gradient Norm: 5.0784\n",
      "Epoch 25: avg loss training: 3.2087,... Gradient Norm: 2.3465\n",
      "Epoch 26: avg loss training: 3.1996,... Gradient Norm: 4.6886\n",
      "Epoch 27: avg loss training: 3.1679,... Gradient Norm: 4.7515\n",
      "Epoch 28: avg loss training: 3.1144,... Gradient Norm: 2.2922\n",
      "Epoch 29: avg loss training: 3.1048,... Gradient Norm: 4.6988\n",
      "Epoch 30: avg loss training: 3.0651,... Gradient Norm: 3.8921\n",
      "Epoch 31: avg loss training: 3.0181,... Gradient Norm: 1.3880\n",
      "Epoch 32: avg loss training: 3.0122,... Gradient Norm: 4.5835\n",
      "Epoch 33: avg loss training: 2.9877,... Gradient Norm: 3.6320\n",
      "Epoch 34: avg loss training: 2.9682,... Gradient Norm: 1.6226\n",
      "Epoch 35: avg loss training: 2.9696,... Gradient Norm: 3.3995\n",
      "Epoch 36: avg loss training: 2.9488,... Gradient Norm: 2.7017\n",
      "Epoch 37: avg loss training: 2.9300,... Gradient Norm: 1.2784\n",
      "Epoch 38: avg loss training: 2.9285,... Gradient Norm: 3.2920\n",
      "Epoch 39: avg loss training: 2.9086,... Gradient Norm: 1.9994\n",
      "Epoch 40: avg loss training: 2.8963,... Gradient Norm: 2.0613\n",
      "Epoch 41: avg loss training: 2.8948,... Gradient Norm: 3.3788\n",
      "Epoch 42: avg loss training: 2.8812,... Gradient Norm: 0.7712\n",
      "Epoch 43: avg loss training: 2.8839,... Gradient Norm: 3.5483\n",
      "Epoch 44: avg loss training: 2.8718,... Gradient Norm: 2.0637\n",
      "Epoch 45: avg loss training: 2.8657,... Gradient Norm: 1.9843\n",
      "Epoch 46: avg loss training: 2.8648,... Gradient Norm: 2.5941\n",
      "Epoch 47: avg loss training: 2.8591,... Gradient Norm: 0.7023\n",
      "Epoch 48: avg loss training: 2.8554,... Gradient Norm: 1.9514\n",
      "Epoch 49: avg loss training: 2.8494,... Gradient Norm: 1.0160\n",
      "Epoch 50: avg loss training: 2.8483,... Gradient Norm: 1.6001\n",
      "Epoch 51: avg loss training: 2.8433,... Gradient Norm: 1.2498\n",
      "Epoch 52: avg loss training: 2.8386,... Gradient Norm: 1.0948\n",
      "Epoch 53: avg loss training: 2.8373,... Gradient Norm: 1.5003\n",
      "Epoch 54: avg loss training: 2.8332,... Gradient Norm: 0.8668\n",
      "Epoch 55: avg loss training: 2.8302,... Gradient Norm: 1.4295\n",
      "Epoch 56: avg loss training: 2.8272,... Gradient Norm: 0.5788\n",
      "Epoch 57: avg loss training: 2.8268,... Gradient Norm: 1.6801\n",
      "Epoch 58: avg loss training: 2.8228,... Gradient Norm: 0.4239\n",
      "Epoch 59: avg loss training: 2.8220,... Gradient Norm: 1.3964\n",
      "Epoch 60: avg loss training: 2.8196,... Gradient Norm: 0.7053\n",
      "Epoch 61: avg loss training: 2.8175,... Gradient Norm: 0.9212\n",
      "Epoch 62: avg loss training: 2.8162,... Gradient Norm: 0.6967\n",
      "Epoch 63: avg loss training: 2.8150,... Gradient Norm: 0.8202\n",
      "Epoch 64: avg loss training: 2.8126,... Gradient Norm: 0.4905\n",
      "Epoch 65: avg loss training: 2.8117,... Gradient Norm: 0.7959\n",
      "Epoch 66: avg loss training: 2.8098,... Gradient Norm: 0.5206\n",
      "Epoch 67: avg loss training: 2.8086,... Gradient Norm: 0.8887\n",
      "Epoch 68: avg loss training: 2.8072,... Gradient Norm: 0.6729\n",
      "Epoch 69: avg loss training: 2.8054,... Gradient Norm: 0.8521\n",
      "Epoch 70: avg loss training: 2.8037,... Gradient Norm: 0.3527\n",
      "Epoch 71: avg loss training: 2.8024,... Gradient Norm: 0.9085\n",
      "Epoch 72: avg loss training: 2.8006,... Gradient Norm: 0.2456\n",
      "Epoch 73: avg loss training: 2.7995,... Gradient Norm: 0.7186\n",
      "Epoch 74: avg loss training: 2.7977,... Gradient Norm: 0.3221\n",
      "Epoch 75: avg loss training: 2.7965,... Gradient Norm: 0.6157\n",
      "Epoch 76: avg loss training: 2.7951,... Gradient Norm: 0.3686\n",
      "Epoch 77: avg loss training: 2.7939,... Gradient Norm: 0.3725\n",
      "Epoch 78: avg loss training: 2.7927,... Gradient Norm: 0.4890\n",
      "Epoch 79: avg loss training: 2.7914,... Gradient Norm: 0.2230\n",
      "Epoch 80: avg loss training: 2.7903,... Gradient Norm: 0.3254\n",
      "Epoch 81: avg loss training: 2.7891,... Gradient Norm: 0.2499\n",
      "Epoch 82: avg loss training: 2.7881,... Gradient Norm: 0.3613\n",
      "Epoch 83: avg loss training: 2.7869,... Gradient Norm: 0.1294\n",
      "Epoch 84: avg loss training: 2.7860,... Gradient Norm: 0.3033\n",
      "Epoch 85: avg loss training: 2.7849,... Gradient Norm: 0.1927\n",
      "Epoch 86: avg loss training: 2.7840,... Gradient Norm: 0.2912\n",
      "Epoch 87: avg loss training: 2.7831,... Gradient Norm: 0.2599\n",
      "Epoch 88: avg loss training: 2.7822,... Gradient Norm: 0.3253\n",
      "Epoch 89: avg loss training: 2.7814,... Gradient Norm: 0.2559\n",
      "Epoch 90: avg loss training: 2.7805,... Gradient Norm: 0.2835\n",
      "Epoch 91: avg loss training: 2.7798,... Gradient Norm: 0.4867\n",
      "Epoch 92: avg loss training: 2.7789,... Gradient Norm: 0.4557\n",
      "Epoch 93: avg loss training: 2.7781,... Gradient Norm: 0.3081\n",
      "Epoch 94: avg loss training: 2.7773,... Gradient Norm: 0.2346\n",
      "Epoch 95: avg loss training: 2.7767,... Gradient Norm: 0.4854\n",
      "Epoch 96: avg loss training: 2.7762,... Gradient Norm: 0.7547\n",
      "Epoch 97: avg loss training: 2.7756,... Gradient Norm: 1.1377\n",
      "Epoch 98: avg loss training: 2.7754,... Gradient Norm: 1.6426\n",
      "Epoch 99: avg loss training: 2.7756,... Gradient Norm: 2.3717\n",
      "Epoch 100: avg loss training: 2.7761,... Gradient Norm: 3.0778\n",
      "Epoch 101: avg loss training: 2.7760,... Gradient Norm: 3.1682\n",
      "Epoch 102: avg loss training: 2.7743,... Gradient Norm: 2.6888\n",
      "Epoch 103: avg loss training: 2.7726,... Gradient Norm: 1.6672\n",
      "Epoch 104: avg loss training: 2.7714,... Gradient Norm: 0.2956\n",
      "Epoch 105: avg loss training: 2.7712,... Gradient Norm: 0.9776\n",
      "Epoch 106: avg loss training: 2.7710,... Gradient Norm: 1.4923\n",
      "Epoch 107: avg loss training: 2.7703,... Gradient Norm: 1.3737\n",
      "Epoch 108: avg loss training: 2.7696,... Gradient Norm: 0.9121\n",
      "Epoch 109: avg loss training: 2.7689,... Gradient Norm: 0.4042\n",
      "Epoch 110: avg loss training: 2.7683,... Gradient Norm: 0.2825\n",
      "Epoch 111: avg loss training: 2.7681,... Gradient Norm: 0.9942\n",
      "Epoch 112: avg loss training: 2.7680,... Gradient Norm: 1.2823\n",
      "Epoch 113: avg loss training: 2.7673,... Gradient Norm: 0.8635\n",
      "Epoch 114: avg loss training: 2.7666,... Gradient Norm: 0.2111\n",
      "Epoch 115: avg loss training: 2.7662,... Gradient Norm: 0.7665\n",
      "Epoch 116: avg loss training: 2.7659,... Gradient Norm: 1.1595\n",
      "Epoch 117: avg loss training: 2.7653,... Gradient Norm: 1.2236\n",
      "Epoch 118: avg loss training: 2.7647,... Gradient Norm: 1.4325\n",
      "Epoch 119: avg loss training: 2.7646,... Gradient Norm: 1.9304\n",
      "Epoch 120: avg loss training: 2.7651,... Gradient Norm: 2.7317\n",
      "Epoch 121: avg loss training: 2.7676,... Gradient Norm: 4.3370\n",
      "Epoch 122: avg loss training: 2.7718,... Gradient Norm: 5.9231\n",
      "Epoch 123: avg loss training: 2.7724,... Gradient Norm: 6.5757\n",
      "Epoch 124: avg loss training: 2.7683,... Gradient Norm: 5.1594\n",
      "Epoch 125: avg loss training: 2.7620,... Gradient Norm: 1.9465\n",
      "Epoch 126: avg loss training: 2.7614,... Gradient Norm: 1.8043\n",
      "Epoch 127: avg loss training: 2.7655,... Gradient Norm: 4.5237\n",
      "Epoch 128: avg loss training: 2.7665,... Gradient Norm: 4.9602\n",
      "Epoch 129: avg loss training: 2.7614,... Gradient Norm: 2.8691\n",
      "Epoch 130: avg loss training: 2.7591,... Gradient Norm: 0.1712\n",
      "Epoch 131: avg loss training: 2.7602,... Gradient Norm: 2.1654\n",
      "Epoch 132: avg loss training: 2.7605,... Gradient Norm: 2.9672\n",
      "Epoch 133: avg loss training: 2.7595,... Gradient Norm: 2.3689\n",
      "Epoch 134: avg loss training: 2.7579,... Gradient Norm: 0.8024\n",
      "Epoch 135: avg loss training: 2.7578,... Gradient Norm: 1.2468\n",
      "Epoch 136: avg loss training: 2.7586,... Gradient Norm: 2.4808\n",
      "Epoch 137: avg loss training: 2.7582,... Gradient Norm: 2.4857\n",
      "Epoch 138: avg loss training: 2.7572,... Gradient Norm: 1.5025\n",
      "Epoch 139: avg loss training: 2.7562,... Gradient Norm: 0.3275\n",
      "Epoch 140: avg loss training: 2.7569,... Gradient Norm: 2.0227\n",
      "Epoch 141: avg loss training: 2.7576,... Gradient Norm: 3.0141\n",
      "Epoch 142: avg loss training: 2.7569,... Gradient Norm: 2.5209\n",
      "Epoch 143: avg loss training: 2.7553,... Gradient Norm: 0.9164\n",
      "Epoch 144: avg loss training: 2.7552,... Gradient Norm: 1.0519\n",
      "Epoch 145: avg loss training: 2.7559,... Gradient Norm: 2.3771\n",
      "Epoch 146: avg loss training: 2.7558,... Gradient Norm: 2.6084\n",
      "Epoch 147: avg loss training: 2.7548,... Gradient Norm: 1.6680\n",
      "Epoch 148: avg loss training: 2.7539,... Gradient Norm: 0.1046\n",
      "Epoch 149: avg loss training: 2.7542,... Gradient Norm: 1.5121\n",
      "Epoch 150: avg loss training: 2.7544,... Gradient Norm: 2.1264\n",
      "Epoch 151: avg loss training: 2.7539,... Gradient Norm: 1.6587\n",
      "Epoch 152: avg loss training: 2.7531,... Gradient Norm: 0.4793\n",
      "Epoch 153: avg loss training: 2.7530,... Gradient Norm: 0.9880\n",
      "Epoch 154: avg loss training: 2.7533,... Gradient Norm: 1.7127\n",
      "Epoch 155: avg loss training: 2.7529,... Gradient Norm: 1.4667\n",
      "Epoch 156: avg loss training: 2.7523,... Gradient Norm: 0.4904\n",
      "Epoch 157: avg loss training: 2.7522,... Gradient Norm: 0.6617\n",
      "Epoch 158: avg loss training: 2.7522,... Gradient Norm: 1.2081\n",
      "Epoch 159: avg loss training: 2.7518,... Gradient Norm: 0.8871\n",
      "Epoch 160: avg loss training: 2.7515,... Gradient Norm: 0.1596\n",
      "Epoch 161: avg loss training: 2.7513,... Gradient Norm: 0.4496\n",
      "Epoch 162: avg loss training: 2.7511,... Gradient Norm: 0.1078\n",
      "Epoch 163: avg loss training: 2.7510,... Gradient Norm: 0.7709\n",
      "Epoch 164: avg loss training: 2.7510,... Gradient Norm: 1.2025\n",
      "Epoch 165: avg loss training: 2.7506,... Gradient Norm: 0.8445\n",
      "Epoch 166: avg loss training: 2.7503,... Gradient Norm: 0.0940\n",
      "Epoch 167: avg loss training: 2.7503,... Gradient Norm: 0.5499\n",
      "Epoch 168: avg loss training: 2.7500,... Gradient Norm: 0.3380\n",
      "Epoch 169: avg loss training: 2.7498,... Gradient Norm: 0.3790\n",
      "Epoch 170: avg loss training: 2.7498,... Gradient Norm: 0.7784\n",
      "Epoch 171: avg loss training: 2.7496,... Gradient Norm: 0.8387\n",
      "Epoch 172: avg loss training: 2.7494,... Gradient Norm: 0.9404\n",
      "Epoch 173: avg loss training: 2.7493,... Gradient Norm: 0.9549\n",
      "Epoch 174: avg loss training: 2.7491,... Gradient Norm: 0.7361\n",
      "Epoch 175: avg loss training: 2.7489,... Gradient Norm: 0.2984\n",
      "Epoch 176: avg loss training: 2.7487,... Gradient Norm: 0.1743\n",
      "Epoch 177: avg loss training: 2.7486,... Gradient Norm: 0.2110\n",
      "Epoch 178: avg loss training: 2.7484,... Gradient Norm: 0.0793\n",
      "Epoch 179: avg loss training: 2.7482,... Gradient Norm: 0.2555\n",
      "Epoch 180: avg loss training: 2.7481,... Gradient Norm: 0.4096\n",
      "Epoch 181: avg loss training: 2.7480,... Gradient Norm: 0.5029\n",
      "Epoch 182: avg loss training: 2.7479,... Gradient Norm: 0.5109\n",
      "Epoch 183: avg loss training: 2.7477,... Gradient Norm: 0.3019\n",
      "Epoch 184: avg loss training: 2.7476,... Gradient Norm: 0.1127\n",
      "Epoch 185: avg loss training: 2.7475,... Gradient Norm: 0.3179\n",
      "Epoch 186: avg loss training: 2.7474,... Gradient Norm: 0.6306\n",
      "Epoch 187: avg loss training: 2.7473,... Gradient Norm: 0.8840\n",
      "Epoch 188: avg loss training: 2.7473,... Gradient Norm: 1.1753\n",
      "Epoch 189: avg loss training: 2.7473,... Gradient Norm: 1.3436\n",
      "Epoch 190: avg loss training: 2.7471,... Gradient Norm: 1.0493\n",
      "Epoch 191: avg loss training: 2.7468,... Gradient Norm: 0.3985\n",
      "Epoch 192: avg loss training: 2.7467,... Gradient Norm: 0.2685\n",
      "Epoch 193: avg loss training: 2.7467,... Gradient Norm: 0.8925\n",
      "Epoch 194: avg loss training: 2.7467,... Gradient Norm: 1.3523\n",
      "Epoch 195: avg loss training: 2.7467,... Gradient Norm: 1.6243\n",
      "Epoch 196: avg loss training: 2.7465,... Gradient Norm: 1.5593\n",
      "Epoch 197: avg loss training: 2.7464,... Gradient Norm: 1.2836\n",
      "Epoch 198: avg loss training: 2.7461,... Gradient Norm: 0.7187\n",
      "Epoch 199: avg loss training: 2.7459,... Gradient Norm: 0.3285\n",
      "Epoch 200: avg loss training: 2.7460,... Gradient Norm: 1.2414\n",
      "Epoch 201: avg loss training: 2.7461,... Gradient Norm: 1.7765\n",
      "Epoch 202: avg loss training: 2.7462,... Gradient Norm: 1.8625\n",
      "Epoch 203: avg loss training: 2.7457,... Gradient Norm: 1.2038\n",
      "Epoch 204: avg loss training: 2.7454,... Gradient Norm: 0.2560\n",
      "Epoch 205: avg loss training: 2.7454,... Gradient Norm: 0.7743\n",
      "Epoch 206: avg loss training: 2.7454,... Gradient Norm: 1.2279\n",
      "Epoch 207: avg loss training: 2.7453,... Gradient Norm: 1.2876\n",
      "Epoch 208: avg loss training: 2.7451,... Gradient Norm: 0.8851\n",
      "Epoch 209: avg loss training: 2.7449,... Gradient Norm: 0.2937\n",
      "Epoch 210: avg loss training: 2.7448,... Gradient Norm: 0.4389\n",
      "Epoch 211: avg loss training: 2.7447,... Gradient Norm: 0.9280\n",
      "Epoch 212: avg loss training: 2.7448,... Gradient Norm: 1.2560\n",
      "Epoch 213: avg loss training: 2.7446,... Gradient Norm: 1.4211\n",
      "Epoch 214: avg loss training: 2.7446,... Gradient Norm: 1.2927\n",
      "Epoch 215: avg loss training: 2.7443,... Gradient Norm: 0.7364\n",
      "Epoch 216: avg loss training: 2.7442,... Gradient Norm: 0.2499\n",
      "Epoch 217: avg loss training: 2.7442,... Gradient Norm: 0.7758\n",
      "Epoch 218: avg loss training: 2.7441,... Gradient Norm: 0.9170\n",
      "Epoch 219: avg loss training: 2.7439,... Gradient Norm: 0.7473\n",
      "Epoch 220: avg loss training: 2.7438,... Gradient Norm: 0.5729\n",
      "Epoch 221: avg loss training: 2.7437,... Gradient Norm: 0.6725\n",
      "Epoch 222: avg loss training: 2.7436,... Gradient Norm: 0.5808\n",
      "Epoch 223: avg loss training: 2.7435,... Gradient Norm: 0.2423\n",
      "Epoch 224: avg loss training: 2.7434,... Gradient Norm: 0.2434\n",
      "Epoch 225: avg loss training: 2.7433,... Gradient Norm: 0.8432\n",
      "Epoch 226: avg loss training: 2.7434,... Gradient Norm: 1.3623\n",
      "Epoch 227: avg loss training: 2.7433,... Gradient Norm: 1.4739\n",
      "Epoch 228: avg loss training: 2.7432,... Gradient Norm: 1.0566\n",
      "Epoch 229: avg loss training: 2.7430,... Gradient Norm: 0.2510\n",
      "Epoch 230: avg loss training: 2.7429,... Gradient Norm: 0.7329\n",
      "Epoch 231: avg loss training: 2.7429,... Gradient Norm: 1.1496\n",
      "Epoch 232: avg loss training: 2.7427,... Gradient Norm: 0.9748\n",
      "Epoch 233: avg loss training: 2.7426,... Gradient Norm: 0.6316\n",
      "Epoch 234: avg loss training: 2.7425,... Gradient Norm: 0.2093\n",
      "Epoch 235: avg loss training: 2.7424,... Gradient Norm: 0.3373\n",
      "Epoch 236: avg loss training: 2.7423,... Gradient Norm: 0.4514\n",
      "Epoch 237: avg loss training: 2.7422,... Gradient Norm: 0.6350\n",
      "Epoch 238: avg loss training: 2.7422,... Gradient Norm: 1.1243\n",
      "Epoch 239: avg loss training: 2.7423,... Gradient Norm: 1.5848\n",
      "Epoch 240: avg loss training: 2.7423,... Gradient Norm: 1.5288\n",
      "Epoch 241: avg loss training: 2.7420,... Gradient Norm: 0.7721\n",
      "Epoch 242: avg loss training: 2.7419,... Gradient Norm: 0.5064\n",
      "Epoch 243: avg loss training: 2.7420,... Gradient Norm: 1.4848\n",
      "Epoch 244: avg loss training: 2.7420,... Gradient Norm: 1.5348\n",
      "Epoch 245: avg loss training: 2.7416,... Gradient Norm: 0.6695\n",
      "Epoch 246: avg loss training: 2.7416,... Gradient Norm: 0.5668\n",
      "Epoch 247: avg loss training: 2.7417,... Gradient Norm: 1.3570\n",
      "Epoch 248: avg loss training: 2.7417,... Gradient Norm: 1.5182\n",
      "Epoch 249: avg loss training: 2.7414,... Gradient Norm: 0.9523\n",
      "Epoch 250: avg loss training: 2.7413,... Gradient Norm: 0.2757\n",
      "Epoch 251: avg loss training: 2.7413,... Gradient Norm: 0.8242\n",
      "Epoch 252: avg loss training: 2.7412,... Gradient Norm: 0.8865\n",
      "Epoch 253: avg loss training: 2.7410,... Gradient Norm: 0.3903\n",
      "Epoch 254: avg loss training: 2.7410,... Gradient Norm: 0.2757\n",
      "Epoch 255: avg loss training: 2.7409,... Gradient Norm: 0.6029\n",
      "Epoch 256: avg loss training: 2.7409,... Gradient Norm: 0.7792\n",
      "Epoch 257: avg loss training: 2.7408,... Gradient Norm: 0.7436\n",
      "Epoch 258: avg loss training: 2.7407,... Gradient Norm: 0.5152\n",
      "Epoch 259: avg loss training: 2.7406,... Gradient Norm: 0.2187\n",
      "Epoch 260: avg loss training: 2.7405,... Gradient Norm: 0.1875\n",
      "Epoch 261: avg loss training: 2.7405,... Gradient Norm: 0.2792\n",
      "Epoch 262: avg loss training: 2.7405,... Gradient Norm: 0.3434\n",
      "Epoch 263: avg loss training: 2.7404,... Gradient Norm: 0.1304\n",
      "Epoch 264: avg loss training: 2.7404,... Gradient Norm: 0.4866\n",
      "Epoch 265: avg loss training: 2.7403,... Gradient Norm: 0.9082\n",
      "Epoch 266: avg loss training: 2.7403,... Gradient Norm: 1.0347\n",
      "Epoch 267: avg loss training: 2.7403,... Gradient Norm: 0.6907\n",
      "Epoch 268: avg loss training: 2.7401,... Gradient Norm: 0.3575\n",
      "Epoch 269: avg loss training: 2.7402,... Gradient Norm: 0.8945\n",
      "Epoch 270: avg loss training: 2.7401,... Gradient Norm: 0.7444\n",
      "Epoch 271: avg loss training: 2.7399,... Gradient Norm: 0.3513\n",
      "Epoch 272: avg loss training: 2.7399,... Gradient Norm: 0.6120\n",
      "Epoch 273: avg loss training: 2.7400,... Gradient Norm: 1.0402\n",
      "Epoch 274: avg loss training: 2.7398,... Gradient Norm: 0.8075\n",
      "Epoch 275: avg loss training: 2.7397,... Gradient Norm: 0.2799\n",
      "Epoch 276: avg loss training: 2.7397,... Gradient Norm: 0.2308\n",
      "Epoch 277: avg loss training: 2.7396,... Gradient Norm: 0.3901\n",
      "Epoch 278: avg loss training: 2.7395,... Gradient Norm: 0.1668\n",
      "Epoch 279: avg loss training: 2.7395,... Gradient Norm: 0.2466\n",
      "Epoch 280: avg loss training: 2.7394,... Gradient Norm: 0.3396\n",
      "Epoch 281: avg loss training: 2.7394,... Gradient Norm: 0.2394\n",
      "Epoch 282: avg loss training: 2.7394,... Gradient Norm: 0.2734\n",
      "Epoch 283: avg loss training: 2.7393,... Gradient Norm: 0.1907\n",
      "Epoch 284: avg loss training: 2.7393,... Gradient Norm: 0.4567\n",
      "Epoch 285: avg loss training: 2.7392,... Gradient Norm: 0.3174\n",
      "Epoch 286: avg loss training: 2.7392,... Gradient Norm: 0.2734\n",
      "Epoch 287: avg loss training: 2.7392,... Gradient Norm: 0.6403\n",
      "Epoch 288: avg loss training: 2.7391,... Gradient Norm: 0.6211\n",
      "Epoch 289: avg loss training: 2.7390,... Gradient Norm: 0.1955\n",
      "Epoch 290: avg loss training: 2.7390,... Gradient Norm: 0.4798\n",
      "Epoch 291: avg loss training: 2.7389,... Gradient Norm: 0.5009\n",
      "Epoch 292: avg loss training: 2.7389,... Gradient Norm: 0.2230\n",
      "Epoch 293: avg loss training: 2.7388,... Gradient Norm: 0.2545\n",
      "Epoch 294: avg loss training: 2.7388,... Gradient Norm: 0.4019\n",
      "Epoch 295: avg loss training: 2.7388,... Gradient Norm: 0.4184\n",
      "Epoch 296: avg loss training: 2.7387,... Gradient Norm: 0.1850\n",
      "Epoch 297: avg loss training: 2.7387,... Gradient Norm: 0.4010\n",
      "Epoch 298: avg loss training: 2.7387,... Gradient Norm: 0.4428\n",
      "Epoch 299: avg loss training: 2.7386,... Gradient Norm: 0.3091\n",
      "Epoch 300: avg loss training: 2.7386,... Gradient Norm: 0.4024\n",
      "Epoch 301: avg loss training: 2.7385,... Gradient Norm: 0.4155\n",
      "Epoch 302: avg loss training: 2.7385,... Gradient Norm: 0.1699\n",
      "Epoch 303: avg loss training: 2.7384,... Gradient Norm: 0.2308\n",
      "Epoch 304: avg loss training: 2.7384,... Gradient Norm: 0.3434\n",
      "Epoch 305: avg loss training: 2.7383,... Gradient Norm: 0.4241\n",
      "Epoch 306: avg loss training: 2.7383,... Gradient Norm: 0.3529\n",
      "Epoch 307: avg loss training: 2.7382,... Gradient Norm: 0.1902\n",
      "Epoch 308: avg loss training: 2.7382,... Gradient Norm: 0.4979\n",
      "Epoch 309: avg loss training: 2.7382,... Gradient Norm: 0.8442\n",
      "Epoch 310: avg loss training: 2.7382,... Gradient Norm: 0.7420\n",
      "Epoch 311: avg loss training: 2.7381,... Gradient Norm: 0.3284\n",
      "Epoch 312: avg loss training: 2.7381,... Gradient Norm: 0.2659\n",
      "Epoch 313: avg loss training: 2.7381,... Gradient Norm: 0.4883\n",
      "Epoch 314: avg loss training: 2.7380,... Gradient Norm: 0.2882\n",
      "Epoch 315: avg loss training: 2.7380,... Gradient Norm: 0.3034\n",
      "Epoch 316: avg loss training: 2.7379,... Gradient Norm: 0.5125\n",
      "Epoch 317: avg loss training: 2.7379,... Gradient Norm: 0.3976\n",
      "Epoch 318: avg loss training: 2.7379,... Gradient Norm: 0.2174\n",
      "Epoch 319: avg loss training: 2.7378,... Gradient Norm: 0.4192\n",
      "Epoch 320: avg loss training: 2.7378,... Gradient Norm: 0.4515\n",
      "Epoch 321: avg loss training: 2.7378,... Gradient Norm: 0.1261\n",
      "Epoch 322: avg loss training: 2.7378,... Gradient Norm: 0.6474\n",
      "Epoch 323: avg loss training: 2.7377,... Gradient Norm: 0.5842\n",
      "Epoch 324: avg loss training: 2.7377,... Gradient Norm: 0.3568\n",
      "Epoch 325: avg loss training: 2.7377,... Gradient Norm: 0.9689\n",
      "Epoch 326: avg loss training: 2.7377,... Gradient Norm: 0.6049\n",
      "Epoch 327: avg loss training: 2.7376,... Gradient Norm: 0.3531\n",
      "Epoch 328: avg loss training: 2.7376,... Gradient Norm: 0.5118\n",
      "Epoch 329: avg loss training: 2.7376,... Gradient Norm: 0.3673\n",
      "Epoch 330: avg loss training: 2.7376,... Gradient Norm: 0.6128\n",
      "Epoch 331: avg loss training: 2.7375,... Gradient Norm: 0.3497\n",
      "Epoch 332: avg loss training: 2.7375,... Gradient Norm: 0.9271\n",
      "Epoch 333: avg loss training: 2.7374,... Gradient Norm: 0.3117\n",
      "Epoch 334: avg loss training: 2.7374,... Gradient Norm: 0.5657\n",
      "Epoch 335: avg loss training: 2.7374,... Gradient Norm: 0.3586\n",
      "Epoch 336: avg loss training: 2.7374,... Gradient Norm: 0.6589\n",
      "Epoch 337: avg loss training: 2.7374,... Gradient Norm: 0.8706\n",
      "Epoch 338: avg loss training: 2.7373,... Gradient Norm: 0.1931\n",
      "Epoch 339: avg loss training: 2.7373,... Gradient Norm: 0.9853\n",
      "Epoch 340: avg loss training: 2.7373,... Gradient Norm: 0.4683\n",
      "Epoch 341: avg loss training: 2.7373,... Gradient Norm: 0.6400\n",
      "Epoch 342: avg loss training: 2.7372,... Gradient Norm: 0.2781\n",
      "Epoch 343: avg loss training: 2.7372,... Gradient Norm: 0.4357\n",
      "Epoch 344: avg loss training: 2.7372,... Gradient Norm: 0.1505\n",
      "Epoch 345: avg loss training: 2.7371,... Gradient Norm: 0.4624\n",
      "Epoch 346: avg loss training: 2.7371,... Gradient Norm: 0.2625\n",
      "Epoch 347: avg loss training: 2.7371,... Gradient Norm: 0.5495\n",
      "Epoch 348: avg loss training: 2.7371,... Gradient Norm: 0.3120\n",
      "Epoch 349: avg loss training: 2.7370,... Gradient Norm: 0.3888\n",
      "Epoch 350: avg loss training: 2.7370,... Gradient Norm: 0.3190\n",
      "Epoch 351: avg loss training: 2.7370,... Gradient Norm: 0.2954\n",
      "Epoch 352: avg loss training: 2.7370,... Gradient Norm: 0.2786\n",
      "Epoch 353: avg loss training: 2.7370,... Gradient Norm: 0.2001\n",
      "Epoch 354: avg loss training: 2.7369,... Gradient Norm: 0.4721\n",
      "Epoch 355: avg loss training: 2.7369,... Gradient Norm: 0.1634\n",
      "Epoch 356: avg loss training: 2.7369,... Gradient Norm: 0.2619\n",
      "Epoch 357: avg loss training: 2.7369,... Gradient Norm: 0.3007\n",
      "Epoch 358: avg loss training: 2.7368,... Gradient Norm: 0.1175\n",
      "Epoch 359: avg loss training: 2.7368,... Gradient Norm: 0.1527\n",
      "Epoch 360: avg loss training: 2.7368,... Gradient Norm: 0.0962\n",
      "Epoch 361: avg loss training: 2.7368,... Gradient Norm: 0.3190\n",
      "Epoch 362: avg loss training: 2.7367,... Gradient Norm: 0.3370\n",
      "Epoch 363: avg loss training: 2.7367,... Gradient Norm: 0.1835\n",
      "Epoch 364: avg loss training: 2.7367,... Gradient Norm: 0.2637\n",
      "Epoch 365: avg loss training: 2.7367,... Gradient Norm: 0.1139\n",
      "Epoch 366: avg loss training: 2.7367,... Gradient Norm: 0.2394\n",
      "Epoch 367: avg loss training: 2.7367,... Gradient Norm: 0.1844\n",
      "Epoch 368: avg loss training: 2.7366,... Gradient Norm: 0.0848\n",
      "Epoch 369: avg loss training: 2.7366,... Gradient Norm: 0.2665\n",
      "Epoch 370: avg loss training: 2.7366,... Gradient Norm: 0.2097\n",
      "Epoch 371: avg loss training: 2.7366,... Gradient Norm: 0.2732\n",
      "Epoch 372: avg loss training: 2.7366,... Gradient Norm: 0.1521\n",
      "Epoch 373: avg loss training: 2.7365,... Gradient Norm: 0.4993\n",
      "Epoch 374: avg loss training: 2.7365,... Gradient Norm: 0.1952\n",
      "Epoch 375: avg loss training: 2.7365,... Gradient Norm: 0.2841\n",
      "Epoch 376: avg loss training: 2.7365,... Gradient Norm: 0.2197\n",
      "Epoch 377: avg loss training: 2.7365,... Gradient Norm: 0.3643\n",
      "Epoch 378: avg loss training: 2.7365,... Gradient Norm: 0.3266\n",
      "Epoch 379: avg loss training: 2.7365,... Gradient Norm: 0.3629\n",
      "Epoch 380: avg loss training: 2.7364,... Gradient Norm: 0.2768\n",
      "Epoch 381: avg loss training: 2.7364,... Gradient Norm: 0.2311\n",
      "Epoch 382: avg loss training: 2.7364,... Gradient Norm: 0.3592\n",
      "Epoch 383: avg loss training: 2.7364,... Gradient Norm: 0.2437\n",
      "Epoch 384: avg loss training: 2.7364,... Gradient Norm: 0.2973\n",
      "Epoch 385: avg loss training: 2.7364,... Gradient Norm: 0.2235\n",
      "Epoch 386: avg loss training: 2.7363,... Gradient Norm: 0.3798\n",
      "Epoch 387: avg loss training: 2.7363,... Gradient Norm: 0.2114\n",
      "Epoch 388: avg loss training: 2.7363,... Gradient Norm: 0.2289\n",
      "Epoch 389: avg loss training: 2.7363,... Gradient Norm: 0.2417\n",
      "Epoch 390: avg loss training: 2.7363,... Gradient Norm: 0.1433\n",
      "Epoch 391: avg loss training: 2.7363,... Gradient Norm: 0.0751\n",
      "Epoch 392: avg loss training: 2.7362,... Gradient Norm: 0.1325\n",
      "Epoch 393: avg loss training: 2.7362,... Gradient Norm: 0.1953\n",
      "Epoch 394: avg loss training: 2.7362,... Gradient Norm: 0.1469\n",
      "Epoch 395: avg loss training: 2.7362,... Gradient Norm: 0.1026\n",
      "Epoch 396: avg loss training: 2.7362,... Gradient Norm: 0.2482\n",
      "Epoch 397: avg loss training: 2.7362,... Gradient Norm: 0.2204\n",
      "Epoch 398: avg loss training: 2.7362,... Gradient Norm: 0.1122\n",
      "Epoch 399: avg loss training: 2.7361,... Gradient Norm: 0.1553\n",
      "Epoch 400: avg loss training: 2.7361,... Gradient Norm: 0.1998\n",
      "Epoch 401: avg loss training: 2.7361,... Gradient Norm: 0.2838\n",
      "Epoch 402: avg loss training: 2.7361,... Gradient Norm: 0.0770\n",
      "Epoch 403: avg loss training: 2.7361,... Gradient Norm: 0.4656\n",
      "Epoch 404: avg loss training: 2.7361,... Gradient Norm: 0.2346\n",
      "Epoch 405: avg loss training: 2.7361,... Gradient Norm: 0.2742\n",
      "Epoch 406: avg loss training: 2.7361,... Gradient Norm: 0.1734\n",
      "Epoch 407: avg loss training: 2.7360,... Gradient Norm: 0.2434\n",
      "Epoch 408: avg loss training: 2.7360,... Gradient Norm: 0.2872\n",
      "Epoch 409: avg loss training: 2.7360,... Gradient Norm: 0.2848\n",
      "Epoch 410: avg loss training: 2.7360,... Gradient Norm: 0.2661\n",
      "Epoch 411: avg loss training: 2.7360,... Gradient Norm: 0.3035\n",
      "Epoch 412: avg loss training: 2.7360,... Gradient Norm: 0.1590\n",
      "Epoch 413: avg loss training: 2.7360,... Gradient Norm: 0.1473\n",
      "Epoch 414: avg loss training: 2.7360,... Gradient Norm: 0.1965\n",
      "Epoch 415: avg loss training: 2.7359,... Gradient Norm: 0.1157\n",
      "Epoch 416: avg loss training: 2.7359,... Gradient Norm: 0.2205\n",
      "Epoch 417: avg loss training: 2.7359,... Gradient Norm: 0.1638\n",
      "Epoch 418: avg loss training: 2.7359,... Gradient Norm: 0.1771\n",
      "Epoch 419: avg loss training: 2.7359,... Gradient Norm: 0.1114\n",
      "Epoch 420: avg loss training: 2.7359,... Gradient Norm: 0.2160\n",
      "Epoch 421: avg loss training: 2.7359,... Gradient Norm: 0.1324\n",
      "Epoch 422: avg loss training: 2.7359,... Gradient Norm: 0.1499\n",
      "Epoch 423: avg loss training: 2.7358,... Gradient Norm: 0.2236\n",
      "Epoch 424: avg loss training: 2.7358,... Gradient Norm: 0.2584\n",
      "Epoch 425: avg loss training: 2.7358,... Gradient Norm: 0.2742\n",
      "Epoch 426: avg loss training: 2.7358,... Gradient Norm: 0.1068\n",
      "Epoch 427: avg loss training: 2.7358,... Gradient Norm: 0.3663\n",
      "Epoch 428: avg loss training: 2.7358,... Gradient Norm: 0.1793\n",
      "Epoch 429: avg loss training: 2.7358,... Gradient Norm: 0.1946\n",
      "Epoch 430: avg loss training: 2.7358,... Gradient Norm: 0.1057\n",
      "Epoch 431: avg loss training: 2.7358,... Gradient Norm: 0.3018\n",
      "Epoch 432: avg loss training: 2.7358,... Gradient Norm: 0.1063\n",
      "Epoch 433: avg loss training: 2.7357,... Gradient Norm: 0.2499\n",
      "Epoch 434: avg loss training: 2.7357,... Gradient Norm: 0.2205\n",
      "Epoch 435: avg loss training: 2.7357,... Gradient Norm: 0.2478\n",
      "Epoch 436: avg loss training: 2.7357,... Gradient Norm: 0.2672\n",
      "Epoch 437: avg loss training: 2.7357,... Gradient Norm: 0.1431\n",
      "Epoch 438: avg loss training: 2.7357,... Gradient Norm: 0.4079\n",
      "Epoch 439: avg loss training: 2.7357,... Gradient Norm: 0.1682\n",
      "Epoch 440: avg loss training: 2.7357,... Gradient Norm: 0.2730\n",
      "Epoch 441: avg loss training: 2.7357,... Gradient Norm: 0.2367\n",
      "Epoch 442: avg loss training: 2.7357,... Gradient Norm: 0.1504\n",
      "Epoch 443: avg loss training: 2.7356,... Gradient Norm: 0.1046\n",
      "Epoch 444: avg loss training: 2.7356,... Gradient Norm: 0.1016\n",
      "Epoch 445: avg loss training: 2.7356,... Gradient Norm: 0.2265\n",
      "Epoch 446: avg loss training: 2.7356,... Gradient Norm: 0.1511\n",
      "Epoch 447: avg loss training: 2.7356,... Gradient Norm: 0.0994\n",
      "Epoch 448: avg loss training: 2.7356,... Gradient Norm: 0.2760\n",
      "Epoch 449: avg loss training: 2.7356,... Gradient Norm: 0.1438\n",
      "Epoch 450: avg loss training: 2.7356,... Gradient Norm: 0.1684\n",
      "Epoch 451: avg loss training: 2.7356,... Gradient Norm: 0.2943\n",
      "Epoch 452: avg loss training: 2.7356,... Gradient Norm: 0.1713\n",
      "Epoch 453: avg loss training: 2.7356,... Gradient Norm: 0.2724\n",
      "Epoch 454: avg loss training: 2.7355,... Gradient Norm: 0.0871\n",
      "Epoch 455: avg loss training: 2.7355,... Gradient Norm: 0.3160\n",
      "Epoch 456: avg loss training: 2.7355,... Gradient Norm: 0.1321\n",
      "Epoch 457: avg loss training: 2.7355,... Gradient Norm: 0.3029\n",
      "Epoch 458: avg loss training: 2.7355,... Gradient Norm: 0.2889\n",
      "Epoch 459: avg loss training: 2.7355,... Gradient Norm: 0.2055\n",
      "Epoch 460: avg loss training: 2.7355,... Gradient Norm: 0.4121\n",
      "Epoch 461: avg loss training: 2.7355,... Gradient Norm: 0.1166\n",
      "Epoch 462: avg loss training: 2.7355,... Gradient Norm: 0.3355\n",
      "Epoch 463: avg loss training: 2.7355,... Gradient Norm: 0.3022\n",
      "Epoch 464: avg loss training: 2.7355,... Gradient Norm: 0.2624\n",
      "Epoch 465: avg loss training: 2.7355,... Gradient Norm: 0.2288\n",
      "Epoch 466: avg loss training: 2.7355,... Gradient Norm: 0.2309\n",
      "Epoch 467: avg loss training: 2.7354,... Gradient Norm: 0.2317\n",
      "Epoch 468: avg loss training: 2.7354,... Gradient Norm: 0.3145\n",
      "Epoch 469: avg loss training: 2.7354,... Gradient Norm: 0.1101\n",
      "Epoch 470: avg loss training: 2.7354,... Gradient Norm: 0.3257\n",
      "Epoch 471: avg loss training: 2.7354,... Gradient Norm: 0.1481\n",
      "Epoch 472: avg loss training: 2.7354,... Gradient Norm: 0.2054\n",
      "Epoch 473: avg loss training: 2.7354,... Gradient Norm: 0.1972\n",
      "Epoch 474: avg loss training: 2.7354,... Gradient Norm: 0.1927\n",
      "Epoch 475: avg loss training: 2.7354,... Gradient Norm: 0.1524\n",
      "Epoch 476: avg loss training: 2.7354,... Gradient Norm: 0.3977\n",
      "Epoch 477: avg loss training: 2.7354,... Gradient Norm: 0.1303\n",
      "Epoch 478: avg loss training: 2.7354,... Gradient Norm: 0.2821\n",
      "Epoch 479: avg loss training: 2.7354,... Gradient Norm: 0.1410\n",
      "Epoch 480: avg loss training: 2.7353,... Gradient Norm: 0.2348\n",
      "Epoch 481: avg loss training: 2.7353,... Gradient Norm: 0.2299\n",
      "Epoch 482: avg loss training: 2.7353,... Gradient Norm: 0.2698\n",
      "Epoch 483: avg loss training: 2.7353,... Gradient Norm: 0.2622\n",
      "Epoch 484: avg loss training: 2.7353,... Gradient Norm: 0.1051\n",
      "Epoch 485: avg loss training: 2.7353,... Gradient Norm: 0.3010\n",
      "Epoch 486: avg loss training: 2.7353,... Gradient Norm: 0.1380\n",
      "Epoch 487: avg loss training: 2.7353,... Gradient Norm: 0.1265\n",
      "Epoch 488: avg loss training: 2.7353,... Gradient Norm: 0.2055\n",
      "Epoch 489: avg loss training: 2.7353,... Gradient Norm: 0.1490\n",
      "Epoch 490: avg loss training: 2.7353,... Gradient Norm: 0.2956\n",
      "Epoch 491: avg loss training: 2.7353,... Gradient Norm: 0.2432\n",
      "Epoch 492: avg loss training: 2.7353,... Gradient Norm: 0.2301\n",
      "Epoch 493: avg loss training: 2.7353,... Gradient Norm: 0.2430\n",
      "Epoch 494: avg loss training: 2.7353,... Gradient Norm: 0.0889\n",
      "Epoch 495: avg loss training: 2.7353,... Gradient Norm: 0.3484\n",
      "Epoch 496: avg loss training: 2.7352,... Gradient Norm: 0.1009\n",
      "Epoch 497: avg loss training: 2.7352,... Gradient Norm: 0.2851\n",
      "Epoch 498: avg loss training: 2.7352,... Gradient Norm: 0.2542\n",
      "Epoch 499: avg loss training: 2.7352,... Gradient Norm: 0.1705\n",
      "Epoch 500: avg loss training: 2.7352,... Gradient Norm: 0.2118\n",
      "Epoch 501: avg loss training: 2.7352,... Gradient Norm: 0.1923\n",
      "Epoch 502: avg loss training: 2.7352,... Gradient Norm: 0.1061\n",
      "Epoch 503: avg loss training: 2.7352,... Gradient Norm: 0.2475\n",
      "Epoch 504: avg loss training: 2.7352,... Gradient Norm: 0.1578\n",
      "Epoch 505: avg loss training: 2.7352,... Gradient Norm: 0.1199\n",
      "Epoch 506: avg loss training: 2.7352,... Gradient Norm: 0.1101\n",
      "Epoch 507: avg loss training: 2.7352,... Gradient Norm: 0.2709\n",
      "Epoch 508: avg loss training: 2.7352,... Gradient Norm: 0.1002\n",
      "Epoch 509: avg loss training: 2.7352,... Gradient Norm: 0.2744\n",
      "Epoch 510: avg loss training: 2.7352,... Gradient Norm: 0.2238\n",
      "Epoch 511: avg loss training: 2.7352,... Gradient Norm: 0.1192\n",
      "Epoch 512: avg loss training: 2.7352,... Gradient Norm: 0.2031\n",
      "Epoch 513: avg loss training: 2.7351,... Gradient Norm: 0.1598\n",
      "Epoch 514: avg loss training: 2.7351,... Gradient Norm: 0.2144\n",
      "Epoch 515: avg loss training: 2.7351,... Gradient Norm: 0.1702\n",
      "Epoch 516: avg loss training: 2.7351,... Gradient Norm: 0.1883\n",
      "Epoch 517: avg loss training: 2.7351,... Gradient Norm: 0.1746\n",
      "Epoch 518: avg loss training: 2.7351,... Gradient Norm: 0.2013\n",
      "Epoch 519: avg loss training: 2.7351,... Gradient Norm: 0.2155\n",
      "Epoch 520: avg loss training: 2.7351,... Gradient Norm: 0.1803\n",
      "Epoch 521: avg loss training: 2.7351,... Gradient Norm: 0.0986\n",
      "Epoch 522: avg loss training: 2.7351,... Gradient Norm: 0.1430\n",
      "Epoch 523: avg loss training: 2.7351,... Gradient Norm: 0.1387\n",
      "Epoch 524: avg loss training: 2.7351,... Gradient Norm: 0.2356\n",
      "Epoch 525: avg loss training: 2.7351,... Gradient Norm: 0.2131\n",
      "Epoch 526: avg loss training: 2.7351,... Gradient Norm: 0.2380\n",
      "Epoch 527: avg loss training: 2.7351,... Gradient Norm: 0.2550\n",
      "Epoch 528: avg loss training: 2.7351,... Gradient Norm: 0.2903\n",
      "Epoch 529: avg loss training: 2.7351,... Gradient Norm: 0.2368\n",
      "Epoch 530: avg loss training: 2.7351,... Gradient Norm: 0.2247\n",
      "Epoch 531: avg loss training: 2.7351,... Gradient Norm: 0.1006\n",
      "Epoch 532: avg loss training: 2.7351,... Gradient Norm: 0.2710\n",
      "Epoch 533: avg loss training: 2.7351,... Gradient Norm: 0.2021\n",
      "Epoch 534: avg loss training: 2.7350,... Gradient Norm: 0.1358\n",
      "Epoch 535: avg loss training: 2.7350,... Gradient Norm: 0.2174\n",
      "Epoch 536: avg loss training: 2.7350,... Gradient Norm: 0.2062\n",
      "Epoch 537: avg loss training: 2.7350,... Gradient Norm: 0.1048\n",
      "Epoch 538: avg loss training: 2.7350,... Gradient Norm: 0.2123\n",
      "Epoch 539: avg loss training: 2.7350,... Gradient Norm: 0.2018\n",
      "Epoch 540: avg loss training: 2.7350,... Gradient Norm: 0.1356\n",
      "Epoch 541: avg loss training: 2.7350,... Gradient Norm: 0.1255\n",
      "Epoch 542: avg loss training: 2.7350,... Gradient Norm: 0.2511\n",
      "Epoch 543: avg loss training: 2.7350,... Gradient Norm: 0.1120\n",
      "Epoch 544: avg loss training: 2.7350,... Gradient Norm: 0.2707\n",
      "Epoch 545: avg loss training: 2.7350,... Gradient Norm: 0.1856\n",
      "Epoch 546: avg loss training: 2.7350,... Gradient Norm: 0.2623\n",
      "Epoch 547: avg loss training: 2.7350,... Gradient Norm: 0.2760\n",
      "Epoch 548: avg loss training: 2.7350,... Gradient Norm: 0.1902\n",
      "Epoch 549: avg loss training: 2.7350,... Gradient Norm: 0.2220\n",
      "Epoch 550: avg loss training: 2.7350,... Gradient Norm: 0.1529\n",
      "Epoch 551: avg loss training: 2.7350,... Gradient Norm: 0.1017\n",
      "Epoch 552: avg loss training: 2.7350,... Gradient Norm: 0.3185\n",
      "Epoch 553: avg loss training: 2.7350,... Gradient Norm: 0.2550\n",
      "Epoch 554: avg loss training: 2.7350,... Gradient Norm: 0.1954\n",
      "Epoch 555: avg loss training: 2.7350,... Gradient Norm: 0.2160\n",
      "Epoch 556: avg loss training: 2.7350,... Gradient Norm: 0.1635\n",
      "Epoch 557: avg loss training: 2.7350,... Gradient Norm: 0.2568\n",
      "Epoch 558: avg loss training: 2.7349,... Gradient Norm: 0.1440\n",
      "Epoch 559: avg loss training: 2.7349,... Gradient Norm: 0.1922\n",
      "Epoch 560: avg loss training: 2.7349,... Gradient Norm: 0.2892\n",
      "Epoch 561: avg loss training: 2.7349,... Gradient Norm: 0.1958\n",
      "Epoch 562: avg loss training: 2.7349,... Gradient Norm: 0.2169\n",
      "Epoch 563: avg loss training: 2.7349,... Gradient Norm: 0.2782\n",
      "Epoch 564: avg loss training: 2.7349,... Gradient Norm: 0.1143\n",
      "Epoch 565: avg loss training: 2.7349,... Gradient Norm: 0.1846\n",
      "Epoch 566: avg loss training: 2.7349,... Gradient Norm: 0.2273\n",
      "Epoch 567: avg loss training: 2.7349,... Gradient Norm: 0.1912\n",
      "Epoch 568: avg loss training: 2.7349,... Gradient Norm: 0.0927\n",
      "Epoch 569: avg loss training: 2.7349,... Gradient Norm: 0.1404\n",
      "Epoch 570: avg loss training: 2.7349,... Gradient Norm: 0.2226\n",
      "Epoch 571: avg loss training: 2.7349,... Gradient Norm: 0.1457\n",
      "Epoch 572: avg loss training: 2.7349,... Gradient Norm: 0.1371\n",
      "Epoch 573: avg loss training: 2.7349,... Gradient Norm: 0.1645\n",
      "Epoch 574: avg loss training: 2.7349,... Gradient Norm: 0.0535\n",
      "Epoch 575: avg loss training: 2.7349,... Gradient Norm: 0.1613\n",
      "Epoch 576: avg loss training: 2.7349,... Gradient Norm: 0.1274\n",
      "Epoch 577: avg loss training: 2.7349,... Gradient Norm: 0.2469\n",
      "Epoch 578: avg loss training: 2.7349,... Gradient Norm: 0.1572\n",
      "Epoch 579: avg loss training: 2.7349,... Gradient Norm: 0.2620\n",
      "Epoch 580: avg loss training: 2.7349,... Gradient Norm: 0.1762\n",
      "Epoch 581: avg loss training: 2.7349,... Gradient Norm: 0.3257\n",
      "Epoch 582: avg loss training: 2.7349,... Gradient Norm: 0.1541\n",
      "Epoch 583: avg loss training: 2.7349,... Gradient Norm: 0.2800\n",
      "Epoch 584: avg loss training: 2.7349,... Gradient Norm: 0.1705\n",
      "Epoch 585: avg loss training: 2.7349,... Gradient Norm: 0.1068\n",
      "Epoch 586: avg loss training: 2.7349,... Gradient Norm: 0.1242\n",
      "Epoch 587: avg loss training: 2.7349,... Gradient Norm: 0.1217\n",
      "Epoch 588: avg loss training: 2.7349,... Gradient Norm: 0.0756\n",
      "Epoch 589: avg loss training: 2.7348,... Gradient Norm: 0.1733\n",
      "Epoch 590: avg loss training: 2.7348,... Gradient Norm: 0.2310\n",
      "Epoch 591: avg loss training: 2.7348,... Gradient Norm: 0.2248\n",
      "Epoch 592: avg loss training: 2.7348,... Gradient Norm: 0.1093\n",
      "Epoch 593: avg loss training: 2.7348,... Gradient Norm: 0.0834\n",
      "Epoch 594: avg loss training: 2.7348,... Gradient Norm: 0.2374\n",
      "Epoch 595: avg loss training: 2.7348,... Gradient Norm: 0.2318\n",
      "Epoch 596: avg loss training: 2.7348,... Gradient Norm: 0.1538\n",
      "Epoch 597: avg loss training: 2.7348,... Gradient Norm: 0.3134\n",
      "Epoch 598: avg loss training: 2.7348,... Gradient Norm: 0.0792\n",
      "Epoch 599: avg loss training: 2.7348,... Gradient Norm: 0.1797\n",
      "Epoch 600: avg loss training: 2.7348,... Gradient Norm: 0.1351\n",
      "Epoch 601: avg loss training: 2.7348,... Gradient Norm: 0.2175\n",
      "Epoch 602: avg loss training: 2.7348,... Gradient Norm: 0.1871\n",
      "Epoch 603: avg loss training: 2.7348,... Gradient Norm: 0.2175\n",
      "Epoch 604: avg loss training: 2.7348,... Gradient Norm: 0.2464\n",
      "Epoch 605: avg loss training: 2.7348,... Gradient Norm: 0.2222\n",
      "Epoch 606: avg loss training: 2.7348,... Gradient Norm: 0.2738\n",
      "Epoch 607: avg loss training: 2.7348,... Gradient Norm: 0.1475\n",
      "Epoch 608: avg loss training: 2.7348,... Gradient Norm: 0.2365\n",
      "Epoch 609: avg loss training: 2.7348,... Gradient Norm: 0.3384\n",
      "Epoch 610: avg loss training: 2.7348,... Gradient Norm: 0.1375\n",
      "Epoch 611: avg loss training: 2.7348,... Gradient Norm: 0.2556\n",
      "Epoch 612: avg loss training: 2.7348,... Gradient Norm: 0.1845\n",
      "Epoch 613: avg loss training: 2.7348,... Gradient Norm: 0.1225\n",
      "Epoch 614: avg loss training: 2.7348,... Gradient Norm: 0.2432\n",
      "Epoch 615: avg loss training: 2.7348,... Gradient Norm: 0.1766\n",
      "Epoch 616: avg loss training: 2.7348,... Gradient Norm: 0.1974\n",
      "Epoch 617: avg loss training: 2.7348,... Gradient Norm: 0.1009\n",
      "Epoch 618: avg loss training: 2.7348,... Gradient Norm: 0.2481\n",
      "Epoch 619: avg loss training: 2.7348,... Gradient Norm: 0.2502\n",
      "Epoch 620: avg loss training: 2.7348,... Gradient Norm: 0.1521\n",
      "Epoch 621: avg loss training: 2.7348,... Gradient Norm: 0.2190\n",
      "Epoch 622: avg loss training: 2.7348,... Gradient Norm: 0.1701\n",
      "Epoch 623: avg loss training: 2.7348,... Gradient Norm: 0.2162\n",
      "Epoch 624: avg loss training: 2.7348,... Gradient Norm: 0.1610\n",
      "Epoch 625: avg loss training: 2.7348,... Gradient Norm: 0.2028\n",
      "Epoch 626: avg loss training: 2.7348,... Gradient Norm: 0.1123\n",
      "Epoch 627: avg loss training: 2.7348,... Gradient Norm: 0.2225\n",
      "Epoch 628: avg loss training: 2.7348,... Gradient Norm: 0.1151\n",
      "Epoch 629: avg loss training: 2.7348,... Gradient Norm: 0.1271\n",
      "Epoch 630: avg loss training: 2.7348,... Gradient Norm: 0.0781\n",
      "Epoch 631: avg loss training: 2.7348,... Gradient Norm: 0.1896\n",
      "Epoch 632: avg loss training: 2.7348,... Gradient Norm: 0.1401\n",
      "Epoch 633: avg loss training: 2.7348,... Gradient Norm: 0.2172\n",
      "Epoch 634: avg loss training: 2.7347,... Gradient Norm: 0.1532\n",
      "Epoch 635: avg loss training: 2.7347,... Gradient Norm: 0.3028\n",
      "Epoch 636: avg loss training: 2.7347,... Gradient Norm: 0.1017\n",
      "Epoch 637: avg loss training: 2.7347,... Gradient Norm: 0.2402\n",
      "Epoch 638: avg loss training: 2.7347,... Gradient Norm: 0.1248\n",
      "Epoch 639: avg loss training: 2.7347,... Gradient Norm: 0.2549\n",
      "Epoch 640: avg loss training: 2.7347,... Gradient Norm: 0.1337\n",
      "Epoch 641: avg loss training: 2.7347,... Gradient Norm: 0.1706\n",
      "Epoch 642: avg loss training: 2.7347,... Gradient Norm: 0.1457\n",
      "Epoch 643: avg loss training: 2.7347,... Gradient Norm: 0.0558\n",
      "Epoch 644: avg loss training: 2.7347,... Gradient Norm: 0.2244\n",
      "Epoch 645: avg loss training: 2.7347,... Gradient Norm: 0.1186\n",
      "Epoch 646: avg loss training: 2.7347,... Gradient Norm: 0.1479\n",
      "Epoch 647: avg loss training: 2.7347,... Gradient Norm: 0.1227\n",
      "Epoch 648: avg loss training: 2.7347,... Gradient Norm: 0.2521\n",
      "Epoch 649: avg loss training: 2.7347,... Gradient Norm: 0.1472\n",
      "Epoch 650: avg loss training: 2.7347,... Gradient Norm: 0.1180\n",
      "Epoch 651: avg loss training: 2.7347,... Gradient Norm: 0.2174\n",
      "Epoch 652: avg loss training: 2.7347,... Gradient Norm: 0.1402\n",
      "Epoch 653: avg loss training: 2.7347,... Gradient Norm: 0.1383\n",
      "Epoch 654: avg loss training: 2.7347,... Gradient Norm: 0.2285\n",
      "Epoch 655: avg loss training: 2.7347,... Gradient Norm: 0.1820\n",
      "Epoch 656: avg loss training: 2.7347,... Gradient Norm: 0.1151\n",
      "Epoch 657: avg loss training: 2.7347,... Gradient Norm: 0.1058\n",
      "Epoch 658: avg loss training: 2.7347,... Gradient Norm: 0.2305\n",
      "Epoch 659: avg loss training: 2.7347,... Gradient Norm: 0.1456\n",
      "Epoch 660: avg loss training: 2.7347,... Gradient Norm: 0.1499\n",
      "Epoch 661: avg loss training: 2.7347,... Gradient Norm: 0.2925\n",
      "Epoch 662: avg loss training: 2.7347,... Gradient Norm: 0.0730\n",
      "Epoch 663: avg loss training: 2.7347,... Gradient Norm: 0.1948\n",
      "Epoch 664: avg loss training: 2.7347,... Gradient Norm: 0.2099\n",
      "Epoch 665: avg loss training: 2.7347,... Gradient Norm: 0.0896\n",
      "Epoch 666: avg loss training: 2.7347,... Gradient Norm: 0.1041\n",
      "Epoch 667: avg loss training: 2.7347,... Gradient Norm: 0.2930\n",
      "Epoch 668: avg loss training: 2.7347,... Gradient Norm: 0.2351\n",
      "Epoch 669: avg loss training: 2.7347,... Gradient Norm: 0.1713\n",
      "Epoch 670: avg loss training: 2.7347,... Gradient Norm: 0.2620\n",
      "Epoch 671: avg loss training: 2.7347,... Gradient Norm: 0.1170\n",
      "Epoch 672: avg loss training: 2.7347,... Gradient Norm: 0.1648\n",
      "Epoch 673: avg loss training: 2.7347,... Gradient Norm: 0.2145\n",
      "Epoch 674: avg loss training: 2.7347,... Gradient Norm: 0.1116\n",
      "Epoch 675: avg loss training: 2.7347,... Gradient Norm: 0.2009\n",
      "Epoch 676: avg loss training: 2.7347,... Gradient Norm: 0.1685\n",
      "Epoch 677: avg loss training: 2.7347,... Gradient Norm: 0.1670\n",
      "Epoch 678: avg loss training: 2.7347,... Gradient Norm: 0.1580\n",
      "Epoch 679: avg loss training: 2.7347,... Gradient Norm: 0.2622\n",
      "Epoch 680: avg loss training: 2.7347,... Gradient Norm: 0.1563\n",
      "Epoch 681: avg loss training: 2.7347,... Gradient Norm: 0.1797\n",
      "Epoch 682: avg loss training: 2.7347,... Gradient Norm: 0.2617\n",
      "Epoch 683: avg loss training: 2.7347,... Gradient Norm: 0.0859\n",
      "Epoch 684: avg loss training: 2.7347,... Gradient Norm: 0.1872\n",
      "Epoch 685: avg loss training: 2.7347,... Gradient Norm: 0.1380\n",
      "Epoch 686: avg loss training: 2.7347,... Gradient Norm: 0.1384\n",
      "Epoch 687: avg loss training: 2.7347,... Gradient Norm: 0.1939\n",
      "Epoch 688: avg loss training: 2.7347,... Gradient Norm: 0.2384\n",
      "Epoch 689: avg loss training: 2.7347,... Gradient Norm: 0.1166\n",
      "Epoch 690: avg loss training: 2.7347,... Gradient Norm: 0.3431\n",
      "Epoch 691: avg loss training: 2.7347,... Gradient Norm: 0.0825\n",
      "Epoch 692: avg loss training: 2.7347,... Gradient Norm: 0.2280\n",
      "Epoch 693: avg loss training: 2.7347,... Gradient Norm: 0.2007\n",
      "Epoch 694: avg loss training: 2.7347,... Gradient Norm: 0.1065\n",
      "Epoch 695: avg loss training: 2.7347,... Gradient Norm: 0.0589\n",
      "Epoch 696: avg loss training: 2.7347,... Gradient Norm: 0.2211\n",
      "Epoch 697: avg loss training: 2.7347,... Gradient Norm: 0.2095\n",
      "Epoch 698: avg loss training: 2.7347,... Gradient Norm: 0.2005\n",
      "Epoch 699: avg loss training: 2.7347,... Gradient Norm: 0.1614\n",
      "Epoch 700: avg loss training: 2.7347,... Gradient Norm: 0.0796\n",
      "Epoch 701: avg loss training: 2.7347,... Gradient Norm: 0.3089\n",
      "Epoch 702: avg loss training: 2.7347,... Gradient Norm: 0.2723\n",
      "Epoch 703: avg loss training: 2.7347,... Gradient Norm: 0.1941\n",
      "Epoch 704: avg loss training: 2.7347,... Gradient Norm: 0.2272\n",
      "Epoch 705: avg loss training: 2.7346,... Gradient Norm: 0.2079\n",
      "Epoch 706: avg loss training: 2.7346,... Gradient Norm: 0.2019\n",
      "Epoch 707: avg loss training: 2.7346,... Gradient Norm: 0.1921\n",
      "Epoch 708: avg loss training: 2.7346,... Gradient Norm: 0.1434\n",
      "Epoch 709: avg loss training: 2.7346,... Gradient Norm: 0.0966\n",
      "Epoch 710: avg loss training: 2.7346,... Gradient Norm: 0.2641\n",
      "Epoch 711: avg loss training: 2.7346,... Gradient Norm: 0.1983\n",
      "Epoch 712: avg loss training: 2.7346,... Gradient Norm: 0.2157\n",
      "Epoch 713: avg loss training: 2.7346,... Gradient Norm: 0.2319\n",
      "Epoch 714: avg loss training: 2.7346,... Gradient Norm: 0.0937\n",
      "Epoch 715: avg loss training: 2.7346,... Gradient Norm: 0.2045\n",
      "Epoch 716: avg loss training: 2.7346,... Gradient Norm: 0.1989\n",
      "Epoch 717: avg loss training: 2.7346,... Gradient Norm: 0.1043\n",
      "Epoch 718: avg loss training: 2.7346,... Gradient Norm: 0.2580\n",
      "Epoch 719: avg loss training: 2.7346,... Gradient Norm: 0.0780\n",
      "Epoch 720: avg loss training: 2.7346,... Gradient Norm: 0.3243\n",
      "Epoch 721: avg loss training: 2.7346,... Gradient Norm: 0.1714\n",
      "Epoch 722: avg loss training: 2.7346,... Gradient Norm: 0.2775\n",
      "Epoch 723: avg loss training: 2.7346,... Gradient Norm: 0.2320\n",
      "Epoch 724: avg loss training: 2.7346,... Gradient Norm: 0.1117\n",
      "Epoch 725: avg loss training: 2.7346,... Gradient Norm: 0.3434\n",
      "Epoch 726: avg loss training: 2.7346,... Gradient Norm: 0.1926\n",
      "Epoch 727: avg loss training: 2.7346,... Gradient Norm: 0.1570\n",
      "Epoch 728: avg loss training: 2.7346,... Gradient Norm: 0.1765\n",
      "Epoch 729: avg loss training: 2.7346,... Gradient Norm: 0.2028\n",
      "Epoch 730: avg loss training: 2.7346,... Gradient Norm: 0.1451\n",
      "Epoch 731: avg loss training: 2.7346,... Gradient Norm: 0.0950\n",
      "Epoch 732: avg loss training: 2.7346,... Gradient Norm: 0.1277\n",
      "Epoch 733: avg loss training: 2.7346,... Gradient Norm: 0.2870\n",
      "Epoch 734: avg loss training: 2.7346,... Gradient Norm: 0.2085\n",
      "Epoch 735: avg loss training: 2.7346,... Gradient Norm: 0.2486\n",
      "Epoch 736: avg loss training: 2.7346,... Gradient Norm: 0.2347\n",
      "Epoch 737: avg loss training: 2.7346,... Gradient Norm: 0.2346\n",
      "Epoch 738: avg loss training: 2.7346,... Gradient Norm: 0.1965\n",
      "Epoch 739: avg loss training: 2.7346,... Gradient Norm: 0.1759\n",
      "Epoch 740: avg loss training: 2.7346,... Gradient Norm: 0.1612\n",
      "Epoch 741: avg loss training: 2.7346,... Gradient Norm: 0.1466\n",
      "Epoch 742: avg loss training: 2.7346,... Gradient Norm: 0.2753\n",
      "Epoch 743: avg loss training: 2.7346,... Gradient Norm: 0.1616\n",
      "Epoch 744: avg loss training: 2.7346,... Gradient Norm: 0.2498\n",
      "Epoch 745: avg loss training: 2.7346,... Gradient Norm: 0.1559\n",
      "Epoch 746: avg loss training: 2.7346,... Gradient Norm: 0.2499\n",
      "Epoch 747: avg loss training: 2.7346,... Gradient Norm: 0.2886\n",
      "Epoch 748: avg loss training: 2.7346,... Gradient Norm: 0.1716\n",
      "Epoch 749: avg loss training: 2.7346,... Gradient Norm: 0.1862\n",
      "Epoch 750: avg loss training: 2.7346,... Gradient Norm: 0.1110\n",
      "Epoch 751: avg loss training: 2.7346,... Gradient Norm: 0.1592\n",
      "Epoch 752: avg loss training: 2.7346,... Gradient Norm: 0.2440\n",
      "Epoch 753: avg loss training: 2.7346,... Gradient Norm: 0.1017\n",
      "Epoch 754: avg loss training: 2.7346,... Gradient Norm: 0.1561\n",
      "Epoch 755: avg loss training: 2.7346,... Gradient Norm: 0.1234\n",
      "Epoch 756: avg loss training: 2.7346,... Gradient Norm: 0.2871\n",
      "Epoch 757: avg loss training: 2.7346,... Gradient Norm: 0.0712\n",
      "Epoch 758: avg loss training: 2.7346,... Gradient Norm: 0.2102\n",
      "Epoch 759: avg loss training: 2.7346,... Gradient Norm: 0.1355\n",
      "Epoch 760: avg loss training: 2.7346,... Gradient Norm: 0.2220\n",
      "Epoch 761: avg loss training: 2.7346,... Gradient Norm: 0.1616\n",
      "Epoch 762: avg loss training: 2.7346,... Gradient Norm: 0.1512\n",
      "Epoch 763: avg loss training: 2.7346,... Gradient Norm: 0.1996\n",
      "Epoch 764: avg loss training: 2.7346,... Gradient Norm: 0.2534\n",
      "Epoch 765: avg loss training: 2.7346,... Gradient Norm: 0.1982\n",
      "Epoch 766: avg loss training: 2.7346,... Gradient Norm: 0.2095\n",
      "Epoch 767: avg loss training: 2.7346,... Gradient Norm: 0.0869\n",
      "Epoch 768: avg loss training: 2.7346,... Gradient Norm: 0.2078\n",
      "Epoch 769: avg loss training: 2.7346,... Gradient Norm: 0.2431\n",
      "Epoch 770: avg loss training: 2.7346,... Gradient Norm: 0.1457\n",
      "Epoch 771: avg loss training: 2.7346,... Gradient Norm: 0.2085\n",
      "Epoch 772: avg loss training: 2.7346,... Gradient Norm: 0.0993\n",
      "Epoch 773: avg loss training: 2.7346,... Gradient Norm: 0.1960\n",
      "Epoch 774: avg loss training: 2.7346,... Gradient Norm: 0.2159\n",
      "Epoch 775: avg loss training: 2.7346,... Gradient Norm: 0.1336\n",
      "Epoch 776: avg loss training: 2.7346,... Gradient Norm: 0.2260\n",
      "Epoch 777: avg loss training: 2.7346,... Gradient Norm: 0.1041\n",
      "Epoch 778: avg loss training: 2.7346,... Gradient Norm: 0.2901\n",
      "Epoch 779: avg loss training: 2.7346,... Gradient Norm: 0.2454\n",
      "Epoch 780: avg loss training: 2.7346,... Gradient Norm: 0.2223\n",
      "Epoch 781: avg loss training: 2.7346,... Gradient Norm: 0.2255\n",
      "Epoch 782: avg loss training: 2.7346,... Gradient Norm: 0.0879\n",
      "Epoch 783: avg loss training: 2.7346,... Gradient Norm: 0.1100\n",
      "Epoch 784: avg loss training: 2.7346,... Gradient Norm: 0.2539\n",
      "Epoch 785: avg loss training: 2.7346,... Gradient Norm: 0.2056\n",
      "Epoch 786: avg loss training: 2.7346,... Gradient Norm: 0.1261\n",
      "Epoch 787: avg loss training: 2.7346,... Gradient Norm: 0.2173\n",
      "Epoch 788: avg loss training: 2.7346,... Gradient Norm: 0.2058\n",
      "Epoch 789: avg loss training: 2.7346,... Gradient Norm: 0.0973\n",
      "Epoch 790: avg loss training: 2.7346,... Gradient Norm: 0.1456\n",
      "Epoch 791: avg loss training: 2.7346,... Gradient Norm: 0.2423\n",
      "Epoch 792: avg loss training: 2.7346,... Gradient Norm: 0.1276\n",
      "Epoch 793: avg loss training: 2.7346,... Gradient Norm: 0.2110\n",
      "Epoch 794: avg loss training: 2.7346,... Gradient Norm: 0.2043\n",
      "Epoch 795: avg loss training: 2.7346,... Gradient Norm: 0.2818\n",
      "Epoch 796: avg loss training: 2.7346,... Gradient Norm: 0.1349\n",
      "Epoch 797: avg loss training: 2.7346,... Gradient Norm: 0.2508\n",
      "Epoch 798: avg loss training: 2.7346,... Gradient Norm: 0.1868\n",
      "Epoch 799: avg loss training: 2.7346,... Gradient Norm: 0.1353\n",
      "Epoch 800: avg loss training: 2.7346,... Gradient Norm: 0.2600\n",
      "Epoch 801: avg loss training: 2.7346,... Gradient Norm: 0.1048\n",
      "Epoch 802: avg loss training: 2.7346,... Gradient Norm: 0.2001\n",
      "Epoch 803: avg loss training: 2.7346,... Gradient Norm: 0.1216\n",
      "Epoch 804: avg loss training: 2.7346,... Gradient Norm: 0.2186\n",
      "Epoch 805: avg loss training: 2.7346,... Gradient Norm: 0.2046\n",
      "Epoch 806: avg loss training: 2.7346,... Gradient Norm: 0.1761\n",
      "Epoch 807: avg loss training: 2.7346,... Gradient Norm: 0.1811\n",
      "Epoch 808: avg loss training: 2.7346,... Gradient Norm: 0.0936\n",
      "Epoch 809: avg loss training: 2.7346,... Gradient Norm: 0.1577\n",
      "Epoch 810: avg loss training: 2.7346,... Gradient Norm: 0.2446\n",
      "Epoch 811: avg loss training: 2.7346,... Gradient Norm: 0.2096\n",
      "Epoch 812: avg loss training: 2.7346,... Gradient Norm: 0.2521\n",
      "Epoch 813: avg loss training: 2.7346,... Gradient Norm: 0.2524\n",
      "Epoch 814: avg loss training: 2.7346,... Gradient Norm: 0.0672\n",
      "Epoch 815: avg loss training: 2.7346,... Gradient Norm: 0.2741\n",
      "Epoch 816: avg loss training: 2.7346,... Gradient Norm: 0.2637\n",
      "Epoch 817: avg loss training: 2.7346,... Gradient Norm: 0.1585\n",
      "Epoch 818: avg loss training: 2.7346,... Gradient Norm: 0.1935\n",
      "Epoch 819: avg loss training: 2.7346,... Gradient Norm: 0.1353\n",
      "Epoch 820: avg loss training: 2.7346,... Gradient Norm: 0.0699\n",
      "Epoch 821: avg loss training: 2.7346,... Gradient Norm: 0.2437\n",
      "Epoch 822: avg loss training: 2.7346,... Gradient Norm: 0.2067\n",
      "Epoch 823: avg loss training: 2.7346,... Gradient Norm: 0.1233\n",
      "Epoch 824: avg loss training: 2.7346,... Gradient Norm: 0.1416\n",
      "Epoch 825: avg loss training: 2.7346,... Gradient Norm: 0.1428\n",
      "Epoch 826: avg loss training: 2.7346,... Gradient Norm: 0.1310\n",
      "Epoch 827: avg loss training: 2.7346,... Gradient Norm: 0.1847\n",
      "Epoch 828: avg loss training: 2.7346,... Gradient Norm: 0.1558\n",
      "Epoch 829: avg loss training: 2.7346,... Gradient Norm: 0.0830\n",
      "Epoch 830: avg loss training: 2.7346,... Gradient Norm: 0.1158\n",
      "Epoch 831: avg loss training: 2.7346,... Gradient Norm: 0.1779\n",
      "Epoch 832: avg loss training: 2.7346,... Gradient Norm: 0.0738\n",
      "Epoch 833: avg loss training: 2.7346,... Gradient Norm: 0.3008\n",
      "Epoch 834: avg loss training: 2.7346,... Gradient Norm: 0.0912\n",
      "Epoch 835: avg loss training: 2.7346,... Gradient Norm: 0.1357\n",
      "Epoch 836: avg loss training: 2.7346,... Gradient Norm: 0.0777\n",
      "Epoch 837: avg loss training: 2.7346,... Gradient Norm: 0.2724\n",
      "Epoch 838: avg loss training: 2.7346,... Gradient Norm: 0.1809\n",
      "Epoch 839: avg loss training: 2.7346,... Gradient Norm: 0.1907\n",
      "Epoch 840: avg loss training: 2.7346,... Gradient Norm: 0.1951\n",
      "Epoch 841: avg loss training: 2.7346,... Gradient Norm: 0.1760\n",
      "Epoch 842: avg loss training: 2.7346,... Gradient Norm: 0.2173\n",
      "Epoch 843: avg loss training: 2.7346,... Gradient Norm: 0.1986\n",
      "Epoch 844: avg loss training: 2.7346,... Gradient Norm: 0.1723\n",
      "Epoch 845: avg loss training: 2.7346,... Gradient Norm: 0.2633\n",
      "Epoch 846: avg loss training: 2.7346,... Gradient Norm: 0.1538\n",
      "Epoch 847: avg loss training: 2.7346,... Gradient Norm: 0.1629\n",
      "Epoch 848: avg loss training: 2.7346,... Gradient Norm: 0.1619\n",
      "Epoch 849: avg loss training: 2.7346,... Gradient Norm: 0.1586\n",
      "Epoch 850: avg loss training: 2.7346,... Gradient Norm: 0.0905\n",
      "Epoch 851: avg loss training: 2.7346,... Gradient Norm: 0.1138\n",
      "Epoch 852: avg loss training: 2.7346,... Gradient Norm: 0.0839\n",
      "Epoch 853: avg loss training: 2.7346,... Gradient Norm: 0.1383\n",
      "Epoch 854: avg loss training: 2.7346,... Gradient Norm: 0.1279\n",
      "Epoch 855: avg loss training: 2.7346,... Gradient Norm: 0.1569\n",
      "Epoch 856: avg loss training: 2.7346,... Gradient Norm: 0.2293\n",
      "Epoch 857: avg loss training: 2.7346,... Gradient Norm: 0.1737\n",
      "Epoch 858: avg loss training: 2.7346,... Gradient Norm: 0.1909\n",
      "Epoch 859: avg loss training: 2.7346,... Gradient Norm: 0.1993\n",
      "Epoch 860: avg loss training: 2.7346,... Gradient Norm: 0.1579\n",
      "Epoch 861: avg loss training: 2.7346,... Gradient Norm: 0.2581\n",
      "Epoch 862: avg loss training: 2.7346,... Gradient Norm: 0.1465\n",
      "Epoch 863: avg loss training: 2.7346,... Gradient Norm: 0.1743\n",
      "Epoch 864: avg loss training: 2.7346,... Gradient Norm: 0.1615\n",
      "Epoch 865: avg loss training: 2.7346,... Gradient Norm: 0.2570\n",
      "Epoch 866: avg loss training: 2.7346,... Gradient Norm: 0.1109\n",
      "Epoch 867: avg loss training: 2.7346,... Gradient Norm: 0.1123\n",
      "Epoch 868: avg loss training: 2.7346,... Gradient Norm: 0.2008\n",
      "Epoch 869: avg loss training: 2.7346,... Gradient Norm: 0.1640\n",
      "Epoch 870: avg loss training: 2.7346,... Gradient Norm: 0.1031\n",
      "Epoch 871: avg loss training: 2.7346,... Gradient Norm: 0.1415\n",
      "Epoch 872: avg loss training: 2.7346,... Gradient Norm: 0.2031\n",
      "Epoch 873: avg loss training: 2.7346,... Gradient Norm: 0.0897\n",
      "Epoch 874: avg loss training: 2.7346,... Gradient Norm: 0.3191\n",
      "Epoch 875: avg loss training: 2.7346,... Gradient Norm: 0.2182\n",
      "Epoch 876: avg loss training: 2.7346,... Gradient Norm: 0.1838\n",
      "Epoch 877: avg loss training: 2.7346,... Gradient Norm: 0.1047\n",
      "Epoch 878: avg loss training: 2.7346,... Gradient Norm: 0.2897\n",
      "Epoch 879: avg loss training: 2.7346,... Gradient Norm: 0.1124\n",
      "Epoch 880: avg loss training: 2.7346,... Gradient Norm: 0.1755\n",
      "Epoch 881: avg loss training: 2.7346,... Gradient Norm: 0.1728\n",
      "Epoch 882: avg loss training: 2.7346,... Gradient Norm: 0.2098\n",
      "Epoch 883: avg loss training: 2.7346,... Gradient Norm: 0.2121\n",
      "Epoch 884: avg loss training: 2.7346,... Gradient Norm: 0.2078\n",
      "Epoch 885: avg loss training: 2.7346,... Gradient Norm: 0.2022\n",
      "Epoch 886: avg loss training: 2.7346,... Gradient Norm: 0.1141\n",
      "Epoch 887: avg loss training: 2.7346,... Gradient Norm: 0.2437\n",
      "Epoch 888: avg loss training: 2.7346,... Gradient Norm: 0.1329\n",
      "Epoch 889: avg loss training: 2.7346,... Gradient Norm: 0.1475\n",
      "Epoch 890: avg loss training: 2.7346,... Gradient Norm: 0.1614\n",
      "Epoch 891: avg loss training: 2.7346,... Gradient Norm: 0.1978\n",
      "Epoch 892: avg loss training: 2.7346,... Gradient Norm: 0.0861\n",
      "Epoch 893: avg loss training: 2.7346,... Gradient Norm: 0.1132\n",
      "Epoch 894: avg loss training: 2.7346,... Gradient Norm: 0.1414\n",
      "Epoch 895: avg loss training: 2.7346,... Gradient Norm: 0.1210\n",
      "Epoch 896: avg loss training: 2.7346,... Gradient Norm: 0.1405\n",
      "Epoch 897: avg loss training: 2.7346,... Gradient Norm: 0.1907\n",
      "Epoch 898: avg loss training: 2.7346,... Gradient Norm: 0.2521\n",
      "Epoch 899: avg loss training: 2.7346,... Gradient Norm: 0.1312\n",
      "Epoch 900: avg loss training: 2.7346,... Gradient Norm: 0.1760\n",
      "Epoch 901: avg loss training: 2.7346,... Gradient Norm: 0.2395\n",
      "Epoch 902: avg loss training: 2.7346,... Gradient Norm: 0.1141\n",
      "Epoch 903: avg loss training: 2.7346,... Gradient Norm: 0.2007\n",
      "Epoch 904: avg loss training: 2.7346,... Gradient Norm: 0.1861\n",
      "Epoch 905: avg loss training: 2.7346,... Gradient Norm: 0.1989\n",
      "Epoch 906: avg loss training: 2.7346,... Gradient Norm: 0.2412\n",
      "Epoch 907: avg loss training: 2.7346,... Gradient Norm: 0.1089\n",
      "Epoch 908: avg loss training: 2.7346,... Gradient Norm: 0.1822\n",
      "Epoch 909: avg loss training: 2.7346,... Gradient Norm: 0.1584\n",
      "Epoch 910: avg loss training: 2.7346,... Gradient Norm: 0.0778\n",
      "Epoch 911: avg loss training: 2.7346,... Gradient Norm: 0.1201\n",
      "Epoch 912: avg loss training: 2.7346,... Gradient Norm: 0.2214\n",
      "Epoch 913: avg loss training: 2.7346,... Gradient Norm: 0.2180\n",
      "Epoch 914: avg loss training: 2.7346,... Gradient Norm: 0.1053\n",
      "Epoch 915: avg loss training: 2.7346,... Gradient Norm: 0.1782\n",
      "Epoch 916: avg loss training: 2.7346,... Gradient Norm: 0.1512\n",
      "Epoch 917: avg loss training: 2.7346,... Gradient Norm: 0.1064\n",
      "Epoch 918: avg loss training: 2.7346,... Gradient Norm: 0.1974\n",
      "Epoch 919: avg loss training: 2.7346,... Gradient Norm: 0.1252\n",
      "Epoch 920: avg loss training: 2.7346,... Gradient Norm: 0.0956\n",
      "Epoch 921: avg loss training: 2.7346,... Gradient Norm: 0.2129\n",
      "Epoch 922: avg loss training: 2.7345,... Gradient Norm: 0.1116\n",
      "Epoch 923: avg loss training: 2.7345,... Gradient Norm: 0.0717\n",
      "Epoch 924: avg loss training: 2.7345,... Gradient Norm: 0.0658\n",
      "Epoch 925: avg loss training: 2.7345,... Gradient Norm: 0.2659\n",
      "Epoch 926: avg loss training: 2.7345,... Gradient Norm: 0.2474\n",
      "Epoch 927: avg loss training: 2.7345,... Gradient Norm: 0.2396\n",
      "Epoch 928: avg loss training: 2.7345,... Gradient Norm: 0.2185\n",
      "Epoch 929: avg loss training: 2.7345,... Gradient Norm: 0.1878\n",
      "Epoch 930: avg loss training: 2.7345,... Gradient Norm: 0.1514\n",
      "Epoch 931: avg loss training: 2.7345,... Gradient Norm: 0.1877\n",
      "Epoch 932: avg loss training: 2.7345,... Gradient Norm: 0.0725\n",
      "Epoch 933: avg loss training: 2.7345,... Gradient Norm: 0.1950\n",
      "Epoch 934: avg loss training: 2.7345,... Gradient Norm: 0.1208\n",
      "Epoch 935: avg loss training: 2.7345,... Gradient Norm: 0.1544\n",
      "Epoch 936: avg loss training: 2.7345,... Gradient Norm: 0.1275\n",
      "Epoch 937: avg loss training: 2.7345,... Gradient Norm: 0.2191\n",
      "Epoch 938: avg loss training: 2.7345,... Gradient Norm: 0.1998\n",
      "Epoch 939: avg loss training: 2.7345,... Gradient Norm: 0.1261\n",
      "Epoch 940: avg loss training: 2.7345,... Gradient Norm: 0.1819\n",
      "Epoch 941: avg loss training: 2.7345,... Gradient Norm: 0.2095\n",
      "Epoch 942: avg loss training: 2.7345,... Gradient Norm: 0.1239\n",
      "Epoch 943: avg loss training: 2.7345,... Gradient Norm: 0.1037\n",
      "Epoch 944: avg loss training: 2.7345,... Gradient Norm: 0.2986\n",
      "Epoch 945: avg loss training: 2.7345,... Gradient Norm: 0.1709\n",
      "Epoch 946: avg loss training: 2.7345,... Gradient Norm: 0.1405\n",
      "Epoch 947: avg loss training: 2.7345,... Gradient Norm: 0.2100\n",
      "Epoch 948: avg loss training: 2.7345,... Gradient Norm: 0.0772\n",
      "Epoch 949: avg loss training: 2.7345,... Gradient Norm: 0.1227\n",
      "Epoch 950: avg loss training: 2.7345,... Gradient Norm: 0.1623\n",
      "Epoch 951: avg loss training: 2.7345,... Gradient Norm: 0.2243\n",
      "Epoch 952: avg loss training: 2.7345,... Gradient Norm: 0.2459\n",
      "Epoch 953: avg loss training: 2.7345,... Gradient Norm: 0.1276\n",
      "Epoch 954: avg loss training: 2.7345,... Gradient Norm: 0.2057\n",
      "Epoch 955: avg loss training: 2.7345,... Gradient Norm: 0.0855\n",
      "Epoch 956: avg loss training: 2.7345,... Gradient Norm: 0.0981\n",
      "Epoch 957: avg loss training: 2.7345,... Gradient Norm: 0.1741\n",
      "Epoch 958: avg loss training: 2.7345,... Gradient Norm: 0.2214\n",
      "Epoch 959: avg loss training: 2.7345,... Gradient Norm: 0.2271\n",
      "Epoch 960: avg loss training: 2.7345,... Gradient Norm: 0.1471\n",
      "Epoch 961: avg loss training: 2.7345,... Gradient Norm: 0.1303\n",
      "Epoch 962: avg loss training: 2.7345,... Gradient Norm: 0.1522\n",
      "Epoch 963: avg loss training: 2.7345,... Gradient Norm: 0.0936\n",
      "Epoch 964: avg loss training: 2.7345,... Gradient Norm: 0.0939\n",
      "Epoch 965: avg loss training: 2.7345,... Gradient Norm: 0.1832\n",
      "Epoch 966: avg loss training: 2.7345,... Gradient Norm: 0.2185\n",
      "Epoch 967: avg loss training: 2.7345,... Gradient Norm: 0.0981\n",
      "Epoch 968: avg loss training: 2.7345,... Gradient Norm: 0.2423\n",
      "Epoch 969: avg loss training: 2.7345,... Gradient Norm: 0.1385\n",
      "Epoch 970: avg loss training: 2.7345,... Gradient Norm: 0.1151\n",
      "Epoch 971: avg loss training: 2.7345,... Gradient Norm: 0.0719\n",
      "Epoch 972: avg loss training: 2.7345,... Gradient Norm: 0.2055\n",
      "Epoch 973: avg loss training: 2.7345,... Gradient Norm: 0.0941\n",
      "Epoch 974: avg loss training: 2.7345,... Gradient Norm: 0.1507\n",
      "Epoch 975: avg loss training: 2.7345,... Gradient Norm: 0.1845\n",
      "Epoch 976: avg loss training: 2.7345,... Gradient Norm: 0.1703\n",
      "Epoch 977: avg loss training: 2.7345,... Gradient Norm: 0.1052\n",
      "Epoch 978: avg loss training: 2.7345,... Gradient Norm: 0.2698\n",
      "Epoch 979: avg loss training: 2.7345,... Gradient Norm: 0.1553\n",
      "Epoch 980: avg loss training: 2.7345,... Gradient Norm: 0.1942\n",
      "Epoch 981: avg loss training: 2.7345,... Gradient Norm: 0.1964\n",
      "Epoch 982: avg loss training: 2.7345,... Gradient Norm: 0.1897\n",
      "Epoch 983: avg loss training: 2.7345,... Gradient Norm: 0.2221\n",
      "Epoch 984: avg loss training: 2.7345,... Gradient Norm: 0.1049\n",
      "Epoch 985: avg loss training: 2.7345,... Gradient Norm: 0.1211\n",
      "Epoch 986: avg loss training: 2.7345,... Gradient Norm: 0.1698\n",
      "Epoch 987: avg loss training: 2.7345,... Gradient Norm: 0.2177\n",
      "Epoch 988: avg loss training: 2.7345,... Gradient Norm: 0.2178\n",
      "Epoch 989: avg loss training: 2.7345,... Gradient Norm: 0.2199\n",
      "Epoch 990: avg loss training: 2.7345,... Gradient Norm: 0.1943\n",
      "Epoch 991: avg loss training: 2.7345,... Gradient Norm: 0.2342\n",
      "Epoch 992: avg loss training: 2.7345,... Gradient Norm: 0.1624\n",
      "Epoch 993: avg loss training: 2.7345,... Gradient Norm: 0.1587\n",
      "Epoch 994: avg loss training: 2.7345,... Gradient Norm: 0.1127\n",
      "Epoch 995: avg loss training: 2.7345,... Gradient Norm: 0.1238\n",
      "Epoch 996: avg loss training: 2.7345,... Gradient Norm: 0.2180\n",
      "Epoch 997: avg loss training: 2.7345,... Gradient Norm: 0.1186\n",
      "Epoch 998: avg loss training: 2.7345,... Gradient Norm: 0.2304\n",
      "Epoch 999: avg loss training: 2.7345,... Gradient Norm: 0.0895\n",
      "Epoch 1000: avg loss training: 2.7345,... Gradient Norm: 0.2492\n",
      "Epoch 1: avg loss training: 3.5560,... Gradient Norm: 7.6769\n",
      "Epoch 2: avg loss training: 116.1941,... Gradient Norm: 1553.2389\n",
      "Epoch 3: avg loss training: 19.8902,... Gradient Norm: 178.2217\n",
      "Epoch 4: avg loss training: 9.2324,... Gradient Norm: 48.6483\n",
      "Epoch 5: avg loss training: 13.4934,... Gradient Norm: 99.9458\n",
      "Epoch 6: avg loss training: 7.7393,... Gradient Norm: 41.0065\n",
      "Epoch 7: avg loss training: 9.4162,... Gradient Norm: 85.3719\n",
      "Epoch 8: avg loss training: 6.2235,... Gradient Norm: 45.9193\n",
      "Epoch 9: avg loss training: 3.9792,... Gradient Norm: 5.7923\n",
      "Epoch 10: avg loss training: 4.6218,... Gradient Norm: 30.2393\n",
      "Epoch 11: avg loss training: 4.3769,... Gradient Norm: 26.5236\n",
      "Epoch 12: avg loss training: 4.2734,... Gradient Norm: 18.7816\n",
      "Epoch 13: avg loss training: 4.6997,... Gradient Norm: 24.3244\n",
      "Epoch 14: avg loss training: 4.3658,... Gradient Norm: 19.0068\n",
      "Epoch 15: avg loss training: 3.8803,... Gradient Norm: 7.6428\n",
      "Epoch 16: avg loss training: 3.7712,... Gradient Norm: 6.4225\n",
      "Epoch 17: avg loss training: 3.8754,... Gradient Norm: 12.1616\n",
      "Epoch 18: avg loss training: 3.7484,... Gradient Norm: 11.0372\n",
      "Epoch 19: avg loss training: 3.5227,... Gradient Norm: 6.7447\n",
      "Epoch 20: avg loss training: 3.4198,... Gradient Norm: 5.6765\n",
      "Epoch 21: avg loss training: 3.3887,... Gradient Norm: 5.9751\n",
      "Epoch 22: avg loss training: 3.3774,... Gradient Norm: 6.2791\n",
      "Epoch 23: avg loss training: 3.3934,... Gradient Norm: 8.7445\n",
      "Epoch 24: avg loss training: 3.3649,... Gradient Norm: 9.9045\n",
      "Epoch 25: avg loss training: 3.2508,... Gradient Norm: 7.4117\n",
      "Epoch 26: avg loss training: 3.1340,... Gradient Norm: 3.8911\n",
      "Epoch 27: avg loss training: 3.0791,... Gradient Norm: 3.2419\n",
      "Epoch 28: avg loss training: 3.0825,... Gradient Norm: 5.1832\n",
      "Epoch 29: avg loss training: 3.1056,... Gradient Norm: 6.2065\n",
      "Epoch 30: avg loss training: 3.0995,... Gradient Norm: 6.4018\n",
      "Epoch 31: avg loss training: 3.0558,... Gradient Norm: 4.2588\n",
      "Epoch 32: avg loss training: 3.0299,... Gradient Norm: 3.7147\n",
      "Epoch 33: avg loss training: 3.0145,... Gradient Norm: 2.6708\n",
      "Epoch 34: avg loss training: 3.0066,... Gradient Norm: 4.1881\n",
      "Epoch 35: avg loss training: 2.9887,... Gradient Norm: 2.8722\n",
      "Epoch 36: avg loss training: 2.9823,... Gradient Norm: 3.0083\n",
      "Epoch 37: avg loss training: 2.9818,... Gradient Norm: 2.1502\n",
      "Epoch 38: avg loss training: 2.9853,... Gradient Norm: 2.9278\n",
      "Epoch 39: avg loss training: 2.9776,... Gradient Norm: 2.1023\n",
      "Epoch 40: avg loss training: 2.9639,... Gradient Norm: 2.4933\n",
      "Epoch 41: avg loss training: 2.9453,... Gradient Norm: 1.9065\n",
      "Epoch 42: avg loss training: 2.9306,... Gradient Norm: 1.2399\n",
      "Epoch 43: avg loss training: 2.9271,... Gradient Norm: 2.0559\n",
      "Epoch 44: avg loss training: 2.9297,... Gradient Norm: 1.1423\n",
      "Epoch 45: avg loss training: 2.9356,... Gradient Norm: 2.5025\n",
      "Epoch 46: avg loss training: 2.9308,... Gradient Norm: 1.7264\n",
      "Epoch 47: avg loss training: 2.9216,... Gradient Norm: 1.9649\n",
      "Epoch 48: avg loss training: 2.9115,... Gradient Norm: 1.6965\n",
      "Epoch 49: avg loss training: 2.9053,... Gradient Norm: 0.9118\n",
      "Epoch 50: avg loss training: 2.9054,... Gradient Norm: 1.9151\n",
      "Epoch 51: avg loss training: 2.9053,... Gradient Norm: 0.8164\n",
      "Epoch 52: avg loss training: 2.9057,... Gradient Norm: 1.5865\n",
      "Epoch 53: avg loss training: 2.9027,... Gradient Norm: 1.3762\n",
      "Epoch 54: avg loss training: 2.8980,... Gradient Norm: 0.9531\n",
      "Epoch 55: avg loss training: 2.8946,... Gradient Norm: 1.4221\n",
      "Epoch 56: avg loss training: 2.8925,... Gradient Norm: 0.5900\n",
      "Epoch 57: avg loss training: 2.8927,... Gradient Norm: 1.2623\n",
      "Epoch 58: avg loss training: 2.8909,... Gradient Norm: 0.9071\n",
      "Epoch 59: avg loss training: 2.8879,... Gradient Norm: 0.7681\n",
      "Epoch 60: avg loss training: 2.8855,... Gradient Norm: 1.3379\n",
      "Epoch 61: avg loss training: 2.8835,... Gradient Norm: 0.6215\n",
      "Epoch 62: avg loss training: 2.8827,... Gradient Norm: 0.9623\n",
      "Epoch 63: avg loss training: 2.8811,... Gradient Norm: 1.1559\n",
      "Epoch 64: avg loss training: 2.8792,... Gradient Norm: 0.3917\n",
      "Epoch 65: avg loss training: 2.8780,... Gradient Norm: 0.7017\n",
      "Epoch 66: avg loss training: 2.8763,... Gradient Norm: 0.9839\n",
      "Epoch 67: avg loss training: 2.8746,... Gradient Norm: 0.3792\n",
      "Epoch 68: avg loss training: 2.8742,... Gradient Norm: 0.7461\n",
      "Epoch 69: avg loss training: 2.8734,... Gradient Norm: 0.8676\n",
      "Epoch 70: avg loss training: 2.8719,... Gradient Norm: 0.4505\n",
      "Epoch 71: avg loss training: 2.8709,... Gradient Norm: 0.4126\n",
      "Epoch 72: avg loss training: 2.8699,... Gradient Norm: 0.8659\n",
      "Epoch 73: avg loss training: 2.8684,... Gradient Norm: 0.7558\n",
      "Epoch 74: avg loss training: 2.8675,... Gradient Norm: 0.4064\n",
      "Epoch 75: avg loss training: 2.8669,... Gradient Norm: 0.2442\n",
      "Epoch 76: avg loss training: 2.8658,... Gradient Norm: 0.5274\n",
      "Epoch 77: avg loss training: 2.8652,... Gradient Norm: 0.7704\n",
      "Epoch 78: avg loss training: 2.8646,... Gradient Norm: 0.6911\n",
      "Epoch 79: avg loss training: 2.8637,... Gradient Norm: 0.5767\n",
      "Epoch 80: avg loss training: 2.8630,... Gradient Norm: 0.2197\n",
      "Epoch 81: avg loss training: 2.8626,... Gradient Norm: 0.3477\n",
      "Epoch 82: avg loss training: 2.8619,... Gradient Norm: 0.6670\n",
      "Epoch 83: avg loss training: 2.8612,... Gradient Norm: 0.7770\n",
      "Epoch 84: avg loss training: 2.8607,... Gradient Norm: 0.8919\n",
      "Epoch 85: avg loss training: 2.8601,... Gradient Norm: 0.7082\n",
      "Epoch 86: avg loss training: 2.8594,... Gradient Norm: 0.5266\n",
      "Epoch 87: avg loss training: 2.8588,... Gradient Norm: 0.2550\n",
      "Epoch 88: avg loss training: 2.8581,... Gradient Norm: 0.1372\n",
      "Epoch 89: avg loss training: 2.8576,... Gradient Norm: 0.4374\n",
      "Epoch 90: avg loss training: 2.8571,... Gradient Norm: 0.5973\n",
      "Epoch 91: avg loss training: 2.8566,... Gradient Norm: 0.7786\n",
      "Epoch 92: avg loss training: 2.8561,... Gradient Norm: 0.7917\n",
      "Epoch 93: avg loss training: 2.8556,... Gradient Norm: 0.7333\n",
      "Epoch 94: avg loss training: 2.8551,... Gradient Norm: 0.6515\n",
      "Epoch 95: avg loss training: 2.8546,... Gradient Norm: 0.5645\n",
      "Epoch 96: avg loss training: 2.8541,... Gradient Norm: 0.3607\n",
      "Epoch 97: avg loss training: 2.8537,... Gradient Norm: 0.2266\n",
      "Epoch 98: avg loss training: 2.8533,... Gradient Norm: 0.1546\n",
      "Epoch 99: avg loss training: 2.8528,... Gradient Norm: 0.1480\n",
      "Epoch 100: avg loss training: 2.8524,... Gradient Norm: 0.2527\n",
      "Epoch 101: avg loss training: 2.8521,... Gradient Norm: 0.4247\n",
      "Epoch 102: avg loss training: 2.8517,... Gradient Norm: 0.6530\n",
      "Epoch 103: avg loss training: 2.8515,... Gradient Norm: 0.8166\n",
      "Epoch 104: avg loss training: 2.8511,... Gradient Norm: 0.9279\n",
      "Epoch 105: avg loss training: 2.8508,... Gradient Norm: 0.9357\n",
      "Epoch 106: avg loss training: 2.8504,... Gradient Norm: 0.8740\n",
      "Epoch 107: avg loss training: 2.8500,... Gradient Norm: 0.7791\n",
      "Epoch 108: avg loss training: 2.8496,... Gradient Norm: 0.7780\n",
      "Epoch 109: avg loss training: 2.8493,... Gradient Norm: 0.8629\n",
      "Epoch 110: avg loss training: 2.8490,... Gradient Norm: 0.9706\n",
      "Epoch 111: avg loss training: 2.8487,... Gradient Norm: 1.1036\n",
      "Epoch 112: avg loss training: 2.8484,... Gradient Norm: 1.1890\n",
      "Epoch 113: avg loss training: 2.8481,... Gradient Norm: 1.2293\n",
      "Epoch 114: avg loss training: 2.8478,... Gradient Norm: 1.1889\n",
      "Epoch 115: avg loss training: 2.8475,... Gradient Norm: 1.0766\n",
      "Epoch 116: avg loss training: 2.8471,... Gradient Norm: 0.8388\n",
      "Epoch 117: avg loss training: 2.8467,... Gradient Norm: 0.5389\n",
      "Epoch 118: avg loss training: 2.8463,... Gradient Norm: 0.1402\n",
      "Epoch 119: avg loss training: 2.8461,... Gradient Norm: 0.3120\n",
      "Epoch 120: avg loss training: 2.8459,... Gradient Norm: 0.7120\n",
      "Epoch 121: avg loss training: 2.8457,... Gradient Norm: 0.9944\n",
      "Epoch 122: avg loss training: 2.8455,... Gradient Norm: 1.1009\n",
      "Epoch 123: avg loss training: 2.8453,... Gradient Norm: 1.0007\n",
      "Epoch 124: avg loss training: 2.8449,... Gradient Norm: 0.7924\n",
      "Epoch 125: avg loss training: 2.8446,... Gradient Norm: 0.3861\n",
      "Epoch 126: avg loss training: 2.8444,... Gradient Norm: 0.2295\n",
      "Epoch 127: avg loss training: 2.8442,... Gradient Norm: 0.6784\n",
      "Epoch 128: avg loss training: 2.8441,... Gradient Norm: 0.8653\n",
      "Epoch 129: avg loss training: 2.8438,... Gradient Norm: 0.6997\n",
      "Epoch 130: avg loss training: 2.8436,... Gradient Norm: 0.3400\n",
      "Epoch 131: avg loss training: 2.8433,... Gradient Norm: 0.1096\n",
      "Epoch 132: avg loss training: 2.8432,... Gradient Norm: 0.3946\n",
      "Epoch 133: avg loss training: 2.8430,... Gradient Norm: 0.6896\n",
      "Epoch 134: avg loss training: 2.8429,... Gradient Norm: 0.8488\n",
      "Epoch 135: avg loss training: 2.8427,... Gradient Norm: 0.8239\n",
      "Epoch 136: avg loss training: 2.8425,... Gradient Norm: 0.6061\n",
      "Epoch 137: avg loss training: 2.8423,... Gradient Norm: 0.2625\n",
      "Epoch 138: avg loss training: 2.8421,... Gradient Norm: 0.1449\n",
      "Epoch 139: avg loss training: 2.8420,... Gradient Norm: 0.3106\n",
      "Epoch 140: avg loss training: 2.8418,... Gradient Norm: 0.2598\n",
      "Epoch 141: avg loss training: 2.8416,... Gradient Norm: 0.1362\n",
      "Epoch 142: avg loss training: 2.8415,... Gradient Norm: 0.1497\n",
      "Epoch 143: avg loss training: 2.8413,... Gradient Norm: 0.1154\n",
      "Epoch 144: avg loss training: 2.8412,... Gradient Norm: 0.1750\n",
      "Epoch 145: avg loss training: 2.8411,... Gradient Norm: 0.2845\n",
      "Epoch 146: avg loss training: 2.8409,... Gradient Norm: 0.2609\n",
      "Epoch 147: avg loss training: 2.8408,... Gradient Norm: 0.1580\n",
      "Epoch 148: avg loss training: 2.8407,... Gradient Norm: 0.4162\n",
      "Epoch 149: avg loss training: 2.8406,... Gradient Norm: 0.7561\n",
      "Epoch 150: avg loss training: 2.8406,... Gradient Norm: 0.9437\n",
      "Epoch 151: avg loss training: 2.8405,... Gradient Norm: 0.9384\n",
      "Epoch 152: avg loss training: 2.8403,... Gradient Norm: 0.7405\n",
      "Epoch 153: avg loss training: 2.8401,... Gradient Norm: 0.2999\n",
      "Epoch 154: avg loss training: 2.8399,... Gradient Norm: 0.1362\n",
      "Epoch 155: avg loss training: 2.8399,... Gradient Norm: 0.3994\n",
      "Epoch 156: avg loss training: 2.8399,... Gradient Norm: 0.5795\n",
      "Epoch 157: avg loss training: 2.8397,... Gradient Norm: 0.6402\n",
      "Epoch 158: avg loss training: 2.8395,... Gradient Norm: 0.5011\n",
      "Epoch 159: avg loss training: 2.8395,... Gradient Norm: 0.2143\n",
      "Epoch 160: avg loss training: 2.8393,... Gradient Norm: 0.5185\n",
      "Epoch 161: avg loss training: 2.8393,... Gradient Norm: 0.7704\n",
      "Epoch 162: avg loss training: 2.8391,... Gradient Norm: 0.5044\n",
      "Epoch 163: avg loss training: 2.8389,... Gradient Norm: 0.1234\n",
      "Epoch 164: avg loss training: 2.8389,... Gradient Norm: 0.5341\n",
      "Epoch 165: avg loss training: 2.8388,... Gradient Norm: 0.5129\n",
      "Epoch 166: avg loss training: 2.8387,... Gradient Norm: 0.1880\n",
      "Epoch 167: avg loss training: 2.8387,... Gradient Norm: 0.6790\n",
      "Epoch 168: avg loss training: 2.8387,... Gradient Norm: 0.9256\n",
      "Epoch 169: avg loss training: 2.8384,... Gradient Norm: 0.5630\n",
      "Epoch 170: avg loss training: 2.8383,... Gradient Norm: 0.1153\n",
      "Epoch 171: avg loss training: 2.8383,... Gradient Norm: 0.5711\n",
      "Epoch 172: avg loss training: 2.8382,... Gradient Norm: 0.5819\n",
      "Epoch 173: avg loss training: 2.8382,... Gradient Norm: 0.3012\n",
      "Epoch 174: avg loss training: 2.8380,... Gradient Norm: 0.2103\n",
      "Epoch 175: avg loss training: 2.8381,... Gradient Norm: 0.4486\n",
      "Epoch 176: avg loss training: 2.8378,... Gradient Norm: 0.4352\n",
      "Epoch 177: avg loss training: 2.8379,... Gradient Norm: 0.2436\n",
      "Epoch 178: avg loss training: 2.8377,... Gradient Norm: 0.2929\n",
      "Epoch 179: avg loss training: 2.8376,... Gradient Norm: 0.3510\n",
      "Epoch 180: avg loss training: 2.8374,... Gradient Norm: 0.1923\n",
      "Epoch 181: avg loss training: 2.8374,... Gradient Norm: 0.3389\n",
      "Epoch 182: avg loss training: 2.8372,... Gradient Norm: 0.0903\n",
      "Epoch 183: avg loss training: 2.8373,... Gradient Norm: 0.3388\n",
      "Epoch 184: avg loss training: 2.8371,... Gradient Norm: 0.1406\n",
      "Epoch 185: avg loss training: 2.8370,... Gradient Norm: 0.2828\n",
      "Epoch 186: avg loss training: 2.8369,... Gradient Norm: 0.3208\n",
      "Epoch 187: avg loss training: 2.8368,... Gradient Norm: 0.0776\n",
      "Epoch 188: avg loss training: 2.8368,... Gradient Norm: 0.2324\n",
      "Epoch 189: avg loss training: 2.8366,... Gradient Norm: 0.1678\n",
      "Epoch 190: avg loss training: 2.8366,... Gradient Norm: 0.2750\n",
      "Epoch 191: avg loss training: 2.8365,... Gradient Norm: 0.1248\n",
      "Epoch 192: avg loss training: 2.8364,... Gradient Norm: 0.1573\n",
      "Epoch 193: avg loss training: 2.8363,... Gradient Norm: 0.0674\n",
      "Epoch 194: avg loss training: 2.8363,... Gradient Norm: 0.2108\n",
      "Epoch 195: avg loss training: 2.8361,... Gradient Norm: 0.1110\n",
      "Epoch 196: avg loss training: 2.8360,... Gradient Norm: 0.1683\n",
      "Epoch 197: avg loss training: 2.8359,... Gradient Norm: 0.1298\n",
      "Epoch 198: avg loss training: 2.8358,... Gradient Norm: 0.1033\n",
      "Epoch 199: avg loss training: 2.8358,... Gradient Norm: 0.1406\n",
      "Epoch 200: avg loss training: 2.8356,... Gradient Norm: 0.0885\n",
      "Epoch 201: avg loss training: 2.8356,... Gradient Norm: 0.2406\n",
      "Epoch 202: avg loss training: 2.8354,... Gradient Norm: 0.1394\n",
      "Epoch 203: avg loss training: 2.8354,... Gradient Norm: 0.1743\n",
      "Epoch 204: avg loss training: 2.8353,... Gradient Norm: 0.2970\n",
      "Epoch 205: avg loss training: 2.8352,... Gradient Norm: 0.3948\n",
      "Epoch 206: avg loss training: 2.8350,... Gradient Norm: 0.0897\n",
      "Epoch 207: avg loss training: 2.8350,... Gradient Norm: 0.4749\n",
      "Epoch 208: avg loss training: 2.8350,... Gradient Norm: 0.5013\n",
      "Epoch 209: avg loss training: 2.8348,... Gradient Norm: 0.2719\n",
      "Epoch 210: avg loss training: 2.8348,... Gradient Norm: 0.2496\n",
      "Epoch 211: avg loss training: 2.8347,... Gradient Norm: 0.5174\n",
      "Epoch 212: avg loss training: 2.8346,... Gradient Norm: 0.2034\n",
      "Epoch 213: avg loss training: 2.8345,... Gradient Norm: 0.6164\n",
      "Epoch 214: avg loss training: 2.8345,... Gradient Norm: 0.7590\n",
      "Epoch 215: avg loss training: 2.8343,... Gradient Norm: 0.1642\n",
      "Epoch 216: avg loss training: 2.8343,... Gradient Norm: 0.6215\n",
      "Epoch 217: avg loss training: 2.8342,... Gradient Norm: 0.4522\n",
      "Epoch 218: avg loss training: 2.8341,... Gradient Norm: 0.4374\n",
      "Epoch 219: avg loss training: 2.8341,... Gradient Norm: 0.6983\n",
      "Epoch 220: avg loss training: 2.8339,... Gradient Norm: 0.0684\n",
      "Epoch 221: avg loss training: 2.8340,... Gradient Norm: 0.6337\n",
      "Epoch 222: avg loss training: 2.8338,... Gradient Norm: 0.2807\n",
      "Epoch 223: avg loss training: 2.8338,... Gradient Norm: 0.3771\n",
      "Epoch 224: avg loss training: 2.8338,... Gradient Norm: 0.4808\n",
      "Epoch 225: avg loss training: 2.8336,... Gradient Norm: 0.0904\n",
      "Epoch 226: avg loss training: 2.8336,... Gradient Norm: 0.4599\n",
      "Epoch 227: avg loss training: 2.8335,... Gradient Norm: 0.2827\n",
      "Epoch 228: avg loss training: 2.8334,... Gradient Norm: 0.2955\n",
      "Epoch 229: avg loss training: 2.8334,... Gradient Norm: 0.4351\n",
      "Epoch 230: avg loss training: 2.8333,... Gradient Norm: 0.1009\n",
      "Epoch 231: avg loss training: 2.8333,... Gradient Norm: 0.4929\n",
      "Epoch 232: avg loss training: 2.8332,... Gradient Norm: 0.3564\n",
      "Epoch 233: avg loss training: 2.8331,... Gradient Norm: 0.1423\n",
      "Epoch 234: avg loss training: 2.8331,... Gradient Norm: 0.3300\n",
      "Epoch 235: avg loss training: 2.8330,... Gradient Norm: 0.1579\n",
      "Epoch 236: avg loss training: 2.8329,... Gradient Norm: 0.1816\n",
      "Epoch 237: avg loss training: 2.8329,... Gradient Norm: 0.3899\n",
      "Epoch 238: avg loss training: 2.8328,... Gradient Norm: 0.1511\n",
      "Epoch 239: avg loss training: 2.8328,... Gradient Norm: 0.2330\n",
      "Epoch 240: avg loss training: 2.8327,... Gradient Norm: 0.2450\n",
      "Epoch 241: avg loss training: 2.8326,... Gradient Norm: 0.1164\n",
      "Epoch 242: avg loss training: 2.8326,... Gradient Norm: 0.1648\n",
      "Epoch 243: avg loss training: 2.8325,... Gradient Norm: 0.0920\n",
      "Epoch 244: avg loss training: 2.8325,... Gradient Norm: 0.1225\n",
      "Epoch 245: avg loss training: 2.8324,... Gradient Norm: 0.1891\n",
      "Epoch 246: avg loss training: 2.8324,... Gradient Norm: 0.2276\n",
      "Epoch 247: avg loss training: 2.8323,... Gradient Norm: 0.1757\n",
      "Epoch 248: avg loss training: 2.8323,... Gradient Norm: 0.1842\n",
      "Epoch 249: avg loss training: 2.8322,... Gradient Norm: 0.1466\n",
      "Epoch 250: avg loss training: 2.8322,... Gradient Norm: 0.2084\n",
      "Epoch 251: avg loss training: 2.8321,... Gradient Norm: 0.2109\n",
      "Epoch 252: avg loss training: 2.8321,... Gradient Norm: 0.1236\n",
      "Epoch 253: avg loss training: 2.8320,... Gradient Norm: 0.2002\n",
      "Epoch 254: avg loss training: 2.8320,... Gradient Norm: 0.2509\n",
      "Epoch 255: avg loss training: 2.8319,... Gradient Norm: 0.0886\n",
      "Epoch 256: avg loss training: 2.8319,... Gradient Norm: 0.4498\n",
      "Epoch 257: avg loss training: 2.8318,... Gradient Norm: 0.3416\n",
      "Epoch 258: avg loss training: 2.8318,... Gradient Norm: 0.1604\n",
      "Epoch 259: avg loss training: 2.8317,... Gradient Norm: 0.2662\n",
      "Epoch 260: avg loss training: 2.8318,... Gradient Norm: 0.1696\n",
      "Epoch 261: avg loss training: 2.8317,... Gradient Norm: 0.1917\n",
      "Epoch 262: avg loss training: 2.8317,... Gradient Norm: 0.2741\n",
      "Epoch 263: avg loss training: 2.8316,... Gradient Norm: 0.1054\n",
      "Epoch 264: avg loss training: 2.8315,... Gradient Norm: 0.2714\n",
      "Epoch 265: avg loss training: 2.8315,... Gradient Norm: 0.1639\n",
      "Epoch 266: avg loss training: 2.8315,... Gradient Norm: 0.3661\n",
      "Epoch 267: avg loss training: 2.8314,... Gradient Norm: 0.3481\n",
      "Epoch 268: avg loss training: 2.8314,... Gradient Norm: 0.2323\n",
      "Epoch 269: avg loss training: 2.8313,... Gradient Norm: 0.3808\n",
      "Epoch 270: avg loss training: 2.8313,... Gradient Norm: 0.3555\n",
      "Epoch 271: avg loss training: 2.8313,... Gradient Norm: 0.6236\n",
      "Epoch 272: avg loss training: 2.8312,... Gradient Norm: 0.2390\n",
      "Epoch 273: avg loss training: 2.8312,... Gradient Norm: 0.7829\n",
      "Epoch 274: avg loss training: 2.8311,... Gradient Norm: 0.1926\n",
      "Epoch 275: avg loss training: 2.8311,... Gradient Norm: 0.5622\n",
      "Epoch 276: avg loss training: 2.8310,... Gradient Norm: 0.2017\n",
      "Epoch 277: avg loss training: 2.8310,... Gradient Norm: 0.7794\n",
      "Epoch 278: avg loss training: 2.8309,... Gradient Norm: 0.2217\n",
      "Epoch 279: avg loss training: 2.8309,... Gradient Norm: 0.4989\n",
      "Epoch 280: avg loss training: 2.8308,... Gradient Norm: 0.3596\n",
      "Epoch 281: avg loss training: 2.8308,... Gradient Norm: 0.1977\n",
      "Epoch 282: avg loss training: 2.8307,... Gradient Norm: 0.1579\n",
      "Epoch 283: avg loss training: 2.8307,... Gradient Norm: 0.2027\n",
      "Epoch 284: avg loss training: 2.8306,... Gradient Norm: 0.3813\n",
      "Epoch 285: avg loss training: 2.8306,... Gradient Norm: 0.2166\n",
      "Epoch 286: avg loss training: 2.8305,... Gradient Norm: 0.1496\n",
      "Epoch 287: avg loss training: 2.8305,... Gradient Norm: 0.1026\n",
      "Epoch 288: avg loss training: 2.8305,... Gradient Norm: 0.1402\n",
      "Epoch 289: avg loss training: 2.8304,... Gradient Norm: 0.3126\n",
      "Epoch 290: avg loss training: 2.8304,... Gradient Norm: 0.3882\n",
      "Epoch 291: avg loss training: 2.8304,... Gradient Norm: 0.1780\n",
      "Epoch 292: avg loss training: 2.8304,... Gradient Norm: 0.4937\n",
      "Epoch 293: avg loss training: 2.8303,... Gradient Norm: 0.2528\n",
      "Epoch 294: avg loss training: 2.8303,... Gradient Norm: 0.3005\n",
      "Epoch 295: avg loss training: 2.8302,... Gradient Norm: 0.3561\n",
      "Epoch 296: avg loss training: 2.8302,... Gradient Norm: 0.1582\n",
      "Epoch 297: avg loss training: 2.8301,... Gradient Norm: 0.2472\n",
      "Epoch 298: avg loss training: 2.8301,... Gradient Norm: 0.1782\n",
      "Epoch 299: avg loss training: 2.8301,... Gradient Norm: 0.3367\n",
      "Epoch 300: avg loss training: 2.8300,... Gradient Norm: 0.3211\n",
      "Epoch 301: avg loss training: 2.8300,... Gradient Norm: 0.2199\n",
      "Epoch 302: avg loss training: 2.8299,... Gradient Norm: 0.1011\n",
      "Epoch 303: avg loss training: 2.8299,... Gradient Norm: 0.1582\n",
      "Epoch 304: avg loss training: 2.8299,... Gradient Norm: 0.1068\n",
      "Epoch 305: avg loss training: 2.8298,... Gradient Norm: 0.3564\n",
      "Epoch 306: avg loss training: 2.8298,... Gradient Norm: 0.3820\n",
      "Epoch 307: avg loss training: 2.8298,... Gradient Norm: 0.2121\n",
      "Epoch 308: avg loss training: 2.8298,... Gradient Norm: 0.5947\n",
      "Epoch 309: avg loss training: 2.8297,... Gradient Norm: 0.2647\n",
      "Epoch 310: avg loss training: 2.8297,... Gradient Norm: 0.2869\n",
      "Epoch 311: avg loss training: 2.8296,... Gradient Norm: 0.5443\n",
      "Epoch 312: avg loss training: 2.8296,... Gradient Norm: 0.2791\n",
      "Epoch 313: avg loss training: 2.8296,... Gradient Norm: 0.2194\n",
      "Epoch 314: avg loss training: 2.8295,... Gradient Norm: 0.4348\n",
      "Epoch 315: avg loss training: 2.8295,... Gradient Norm: 0.3109\n",
      "Epoch 316: avg loss training: 2.8295,... Gradient Norm: 0.0741\n",
      "Epoch 317: avg loss training: 2.8295,... Gradient Norm: 0.2824\n",
      "Epoch 318: avg loss training: 2.8294,... Gradient Norm: 0.1538\n",
      "Epoch 319: avg loss training: 2.8294,... Gradient Norm: 0.1863\n",
      "Epoch 320: avg loss training: 2.8293,... Gradient Norm: 0.1764\n",
      "Epoch 321: avg loss training: 2.8293,... Gradient Norm: 0.3638\n",
      "Epoch 322: avg loss training: 2.8293,... Gradient Norm: 0.4482\n",
      "Epoch 323: avg loss training: 2.8293,... Gradient Norm: 0.2294\n",
      "Epoch 324: avg loss training: 2.8293,... Gradient Norm: 0.7541\n",
      "Epoch 325: avg loss training: 2.8292,... Gradient Norm: 0.2962\n",
      "Epoch 326: avg loss training: 2.8292,... Gradient Norm: 0.4419\n",
      "Epoch 327: avg loss training: 2.8292,... Gradient Norm: 0.1746\n",
      "Epoch 328: avg loss training: 2.8293,... Gradient Norm: 0.8575\n",
      "Epoch 329: avg loss training: 2.8291,... Gradient Norm: 0.4812\n",
      "Epoch 330: avg loss training: 2.8292,... Gradient Norm: 0.6291\n",
      "Epoch 331: avg loss training: 2.8290,... Gradient Norm: 0.3420\n",
      "Epoch 332: avg loss training: 2.8291,... Gradient Norm: 0.7061\n",
      "Epoch 333: avg loss training: 2.8290,... Gradient Norm: 0.4094\n",
      "Epoch 334: avg loss training: 2.8290,... Gradient Norm: 0.5339\n",
      "Epoch 335: avg loss training: 2.8290,... Gradient Norm: 0.1639\n",
      "Epoch 336: avg loss training: 2.8290,... Gradient Norm: 0.7127\n",
      "Epoch 337: avg loss training: 2.8289,... Gradient Norm: 0.2499\n",
      "Epoch 338: avg loss training: 2.8289,... Gradient Norm: 0.4041\n",
      "Epoch 339: avg loss training: 2.8289,... Gradient Norm: 0.2760\n",
      "Epoch 340: avg loss training: 2.8289,... Gradient Norm: 0.4748\n",
      "Epoch 341: avg loss training: 2.8289,... Gradient Norm: 0.3051\n",
      "Epoch 342: avg loss training: 2.8288,... Gradient Norm: 0.2563\n",
      "Epoch 343: avg loss training: 2.8288,... Gradient Norm: 0.4267\n",
      "Epoch 344: avg loss training: 2.8288,... Gradient Norm: 0.2362\n",
      "Epoch 345: avg loss training: 2.8288,... Gradient Norm: 0.2905\n",
      "Epoch 346: avg loss training: 2.8288,... Gradient Norm: 0.3339\n",
      "Epoch 347: avg loss training: 2.8287,... Gradient Norm: 0.5150\n",
      "Epoch 348: avg loss training: 2.8287,... Gradient Norm: 0.2596\n",
      "Epoch 349: avg loss training: 2.8287,... Gradient Norm: 0.4050\n",
      "Epoch 350: avg loss training: 2.8287,... Gradient Norm: 0.2149\n",
      "Epoch 351: avg loss training: 2.8286,... Gradient Norm: 0.5170\n",
      "Epoch 352: avg loss training: 2.8286,... Gradient Norm: 0.1052\n",
      "Epoch 353: avg loss training: 2.8286,... Gradient Norm: 0.2762\n",
      "Epoch 354: avg loss training: 2.8286,... Gradient Norm: 0.2728\n",
      "Epoch 355: avg loss training: 2.8286,... Gradient Norm: 0.3344\n",
      "Epoch 356: avg loss training: 2.8285,... Gradient Norm: 0.3137\n",
      "Epoch 357: avg loss training: 2.8285,... Gradient Norm: 0.2478\n",
      "Epoch 358: avg loss training: 2.8285,... Gradient Norm: 0.3249\n",
      "Epoch 359: avg loss training: 2.8285,... Gradient Norm: 0.0745\n",
      "Epoch 360: avg loss training: 2.8285,... Gradient Norm: 0.5269\n",
      "Epoch 361: avg loss training: 2.8284,... Gradient Norm: 0.1415\n",
      "Epoch 362: avg loss training: 2.8284,... Gradient Norm: 0.2479\n",
      "Epoch 363: avg loss training: 2.8284,... Gradient Norm: 0.2142\n",
      "Epoch 364: avg loss training: 2.8284,... Gradient Norm: 0.1097\n",
      "Epoch 365: avg loss training: 2.8284,... Gradient Norm: 0.4433\n",
      "Epoch 366: avg loss training: 2.8284,... Gradient Norm: 0.1885\n",
      "Epoch 367: avg loss training: 2.8284,... Gradient Norm: 0.3387\n",
      "Epoch 368: avg loss training: 2.8283,... Gradient Norm: 0.3098\n",
      "Epoch 369: avg loss training: 2.8283,... Gradient Norm: 0.1224\n",
      "Epoch 370: avg loss training: 2.8283,... Gradient Norm: 0.5027\n",
      "Epoch 371: avg loss training: 2.8283,... Gradient Norm: 0.1064\n",
      "Epoch 372: avg loss training: 2.8283,... Gradient Norm: 0.1877\n",
      "Epoch 373: avg loss training: 2.8282,... Gradient Norm: 0.2652\n",
      "Epoch 374: avg loss training: 2.8282,... Gradient Norm: 0.1363\n",
      "Epoch 375: avg loss training: 2.8282,... Gradient Norm: 0.3079\n",
      "Epoch 376: avg loss training: 2.8282,... Gradient Norm: 0.0695\n",
      "Epoch 377: avg loss training: 2.8282,... Gradient Norm: 0.1135\n",
      "Epoch 378: avg loss training: 2.8282,... Gradient Norm: 0.1302\n",
      "Epoch 379: avg loss training: 2.8282,... Gradient Norm: 0.3099\n",
      "Epoch 380: avg loss training: 2.8281,... Gradient Norm: 0.2351\n",
      "Epoch 381: avg loss training: 2.8281,... Gradient Norm: 0.1727\n",
      "Epoch 382: avg loss training: 2.8281,... Gradient Norm: 0.2037\n",
      "Epoch 383: avg loss training: 2.8281,... Gradient Norm: 0.1304\n",
      "Epoch 384: avg loss training: 2.8281,... Gradient Norm: 0.1280\n",
      "Epoch 385: avg loss training: 2.8281,... Gradient Norm: 0.2670\n",
      "Epoch 386: avg loss training: 2.8281,... Gradient Norm: 0.1729\n",
      "Epoch 387: avg loss training: 2.8280,... Gradient Norm: 0.1879\n",
      "Epoch 388: avg loss training: 2.8280,... Gradient Norm: 0.2169\n",
      "Epoch 389: avg loss training: 2.8280,... Gradient Norm: 0.4059\n",
      "Epoch 390: avg loss training: 2.8280,... Gradient Norm: 0.1304\n",
      "Epoch 391: avg loss training: 2.8280,... Gradient Norm: 0.2211\n",
      "Epoch 392: avg loss training: 2.8280,... Gradient Norm: 0.2052\n",
      "Epoch 393: avg loss training: 2.8280,... Gradient Norm: 0.1792\n",
      "Epoch 394: avg loss training: 2.8280,... Gradient Norm: 0.4379\n",
      "Epoch 395: avg loss training: 2.8280,... Gradient Norm: 0.2788\n",
      "Epoch 396: avg loss training: 2.8279,... Gradient Norm: 0.2031\n",
      "Epoch 397: avg loss training: 2.8279,... Gradient Norm: 0.1633\n",
      "Epoch 398: avg loss training: 2.8279,... Gradient Norm: 0.1023\n",
      "Epoch 399: avg loss training: 2.8279,... Gradient Norm: 0.3143\n",
      "Epoch 400: avg loss training: 2.8279,... Gradient Norm: 0.0900\n",
      "Epoch 401: avg loss training: 2.8279,... Gradient Norm: 0.1461\n",
      "Epoch 402: avg loss training: 2.8278,... Gradient Norm: 0.1504\n",
      "Epoch 403: avg loss training: 2.8278,... Gradient Norm: 0.2748\n",
      "Epoch 404: avg loss training: 2.8278,... Gradient Norm: 0.1693\n",
      "Epoch 405: avg loss training: 2.8278,... Gradient Norm: 0.1437\n",
      "Epoch 406: avg loss training: 2.8278,... Gradient Norm: 0.1936\n",
      "Epoch 407: avg loss training: 2.8278,... Gradient Norm: 0.3176\n",
      "Epoch 408: avg loss training: 2.8278,... Gradient Norm: 0.1199\n",
      "Epoch 409: avg loss training: 2.8278,... Gradient Norm: 0.2223\n",
      "Epoch 410: avg loss training: 2.8278,... Gradient Norm: 0.1603\n",
      "Epoch 411: avg loss training: 2.8277,... Gradient Norm: 0.2937\n",
      "Epoch 412: avg loss training: 2.8277,... Gradient Norm: 0.2277\n",
      "Epoch 413: avg loss training: 2.8277,... Gradient Norm: 0.1442\n",
      "Epoch 414: avg loss training: 2.8277,... Gradient Norm: 0.1202\n",
      "Epoch 415: avg loss training: 2.8277,... Gradient Norm: 0.2057\n",
      "Epoch 416: avg loss training: 2.8277,... Gradient Norm: 0.3143\n",
      "Epoch 417: avg loss training: 2.8277,... Gradient Norm: 0.2692\n",
      "Epoch 418: avg loss training: 2.8277,... Gradient Norm: 0.2891\n",
      "Epoch 419: avg loss training: 2.8277,... Gradient Norm: 0.2630\n",
      "Epoch 420: avg loss training: 2.8277,... Gradient Norm: 0.1674\n",
      "Epoch 421: avg loss training: 2.8276,... Gradient Norm: 0.3341\n",
      "Epoch 422: avg loss training: 2.8276,... Gradient Norm: 0.2995\n",
      "Epoch 423: avg loss training: 2.8276,... Gradient Norm: 0.1226\n",
      "Epoch 424: avg loss training: 2.8276,... Gradient Norm: 0.3056\n",
      "Epoch 425: avg loss training: 2.8276,... Gradient Norm: 0.2246\n",
      "Epoch 426: avg loss training: 2.8276,... Gradient Norm: 0.1034\n",
      "Epoch 427: avg loss training: 2.8276,... Gradient Norm: 0.1641\n",
      "Epoch 428: avg loss training: 2.8276,... Gradient Norm: 0.4917\n",
      "Epoch 429: avg loss training: 2.8276,... Gradient Norm: 0.1367\n",
      "Epoch 430: avg loss training: 2.8276,... Gradient Norm: 0.1462\n",
      "Epoch 431: avg loss training: 2.8276,... Gradient Norm: 0.2383\n",
      "Epoch 432: avg loss training: 2.8275,... Gradient Norm: 0.1718\n",
      "Epoch 433: avg loss training: 2.8275,... Gradient Norm: 0.3477\n",
      "Epoch 434: avg loss training: 2.8275,... Gradient Norm: 0.2542\n",
      "Epoch 435: avg loss training: 2.8275,... Gradient Norm: 0.1427\n",
      "Epoch 436: avg loss training: 2.8275,... Gradient Norm: 0.2096\n",
      "Epoch 437: avg loss training: 2.8275,... Gradient Norm: 0.1365\n",
      "Epoch 438: avg loss training: 2.8275,... Gradient Norm: 0.2263\n",
      "Epoch 439: avg loss training: 2.8275,... Gradient Norm: 0.3666\n",
      "Epoch 440: avg loss training: 2.8275,... Gradient Norm: 0.0948\n",
      "Epoch 441: avg loss training: 2.8275,... Gradient Norm: 0.2525\n",
      "Epoch 442: avg loss training: 2.8274,... Gradient Norm: 0.0803\n",
      "Epoch 443: avg loss training: 2.8274,... Gradient Norm: 0.3433\n",
      "Epoch 444: avg loss training: 2.8274,... Gradient Norm: 0.2426\n",
      "Epoch 445: avg loss training: 2.8274,... Gradient Norm: 0.1337\n",
      "Epoch 446: avg loss training: 2.8274,... Gradient Norm: 0.1449\n",
      "Epoch 447: avg loss training: 2.8274,... Gradient Norm: 0.2981\n",
      "Epoch 448: avg loss training: 2.8274,... Gradient Norm: 0.2844\n",
      "Epoch 449: avg loss training: 2.8274,... Gradient Norm: 0.1217\n",
      "Epoch 450: avg loss training: 2.8274,... Gradient Norm: 0.3026\n",
      "Epoch 451: avg loss training: 2.8274,... Gradient Norm: 0.2799\n",
      "Epoch 452: avg loss training: 2.8274,... Gradient Norm: 0.1596\n",
      "Epoch 453: avg loss training: 2.8274,... Gradient Norm: 0.4526\n",
      "Epoch 454: avg loss training: 2.8274,... Gradient Norm: 0.1184\n",
      "Epoch 455: avg loss training: 2.8274,... Gradient Norm: 0.3277\n",
      "Epoch 456: avg loss training: 2.8273,... Gradient Norm: 0.1619\n",
      "Epoch 457: avg loss training: 2.8273,... Gradient Norm: 0.3435\n",
      "Epoch 458: avg loss training: 2.8273,... Gradient Norm: 0.1313\n",
      "Epoch 459: avg loss training: 2.8273,... Gradient Norm: 0.1658\n",
      "Epoch 460: avg loss training: 2.8273,... Gradient Norm: 0.3087\n",
      "Epoch 461: avg loss training: 2.8273,... Gradient Norm: 0.1569\n",
      "Epoch 462: avg loss training: 2.8273,... Gradient Norm: 0.4355\n",
      "Epoch 463: avg loss training: 2.8273,... Gradient Norm: 0.1977\n",
      "Epoch 464: avg loss training: 2.8273,... Gradient Norm: 0.2690\n",
      "Epoch 465: avg loss training: 2.8273,... Gradient Norm: 0.1429\n",
      "Epoch 466: avg loss training: 2.8273,... Gradient Norm: 0.3502\n",
      "Epoch 467: avg loss training: 2.8273,... Gradient Norm: 0.1973\n",
      "Epoch 468: avg loss training: 2.8273,... Gradient Norm: 0.3132\n",
      "Epoch 469: avg loss training: 2.8272,... Gradient Norm: 0.1706\n",
      "Epoch 470: avg loss training: 2.8273,... Gradient Norm: 0.3399\n",
      "Epoch 471: avg loss training: 2.8272,... Gradient Norm: 0.2412\n",
      "Epoch 472: avg loss training: 2.8272,... Gradient Norm: 0.3579\n",
      "Epoch 473: avg loss training: 2.8272,... Gradient Norm: 0.1132\n",
      "Epoch 474: avg loss training: 2.8272,... Gradient Norm: 0.3704\n",
      "Epoch 475: avg loss training: 2.8272,... Gradient Norm: 0.1462\n",
      "Epoch 476: avg loss training: 2.8272,... Gradient Norm: 0.1769\n",
      "Epoch 477: avg loss training: 2.8272,... Gradient Norm: 0.0787\n",
      "Epoch 478: avg loss training: 2.8272,... Gradient Norm: 0.1022\n",
      "Epoch 479: avg loss training: 2.8272,... Gradient Norm: 0.1098\n",
      "Epoch 480: avg loss training: 2.8272,... Gradient Norm: 0.2162\n",
      "Epoch 481: avg loss training: 2.8272,... Gradient Norm: 0.2100\n",
      "Epoch 482: avg loss training: 2.8272,... Gradient Norm: 0.1298\n",
      "Epoch 483: avg loss training: 2.8272,... Gradient Norm: 0.2938\n",
      "Epoch 484: avg loss training: 2.8272,... Gradient Norm: 0.2436\n",
      "Epoch 485: avg loss training: 2.8271,... Gradient Norm: 0.1624\n",
      "Epoch 486: avg loss training: 2.8271,... Gradient Norm: 0.3070\n",
      "Epoch 487: avg loss training: 2.8271,... Gradient Norm: 0.2273\n",
      "Epoch 488: avg loss training: 2.8271,... Gradient Norm: 0.1425\n",
      "Epoch 489: avg loss training: 2.8271,... Gradient Norm: 0.1181\n",
      "Epoch 490: avg loss training: 2.8271,... Gradient Norm: 0.3118\n",
      "Epoch 491: avg loss training: 2.8271,... Gradient Norm: 0.2363\n",
      "Epoch 492: avg loss training: 2.8271,... Gradient Norm: 0.1664\n",
      "Epoch 493: avg loss training: 2.8271,... Gradient Norm: 0.1545\n",
      "Epoch 494: avg loss training: 2.8271,... Gradient Norm: 0.2650\n",
      "Epoch 495: avg loss training: 2.8271,... Gradient Norm: 0.2769\n",
      "Epoch 496: avg loss training: 2.8271,... Gradient Norm: 0.2044\n",
      "Epoch 497: avg loss training: 2.8271,... Gradient Norm: 0.2032\n",
      "Epoch 498: avg loss training: 2.8271,... Gradient Norm: 0.1641\n",
      "Epoch 499: avg loss training: 2.8271,... Gradient Norm: 0.1312\n",
      "Epoch 500: avg loss training: 2.8271,... Gradient Norm: 0.2177\n",
      "Epoch 501: avg loss training: 2.8271,... Gradient Norm: 0.2786\n",
      "Epoch 502: avg loss training: 2.8271,... Gradient Norm: 0.2295\n",
      "Epoch 503: avg loss training: 2.8270,... Gradient Norm: 0.1830\n",
      "Epoch 504: avg loss training: 2.8270,... Gradient Norm: 0.1133\n",
      "Epoch 505: avg loss training: 2.8270,... Gradient Norm: 0.1163\n",
      "Epoch 506: avg loss training: 2.8270,... Gradient Norm: 0.3682\n",
      "Epoch 507: avg loss training: 2.8270,... Gradient Norm: 0.1587\n",
      "Epoch 508: avg loss training: 2.8270,... Gradient Norm: 0.1145\n",
      "Epoch 509: avg loss training: 2.8270,... Gradient Norm: 0.1159\n",
      "Epoch 510: avg loss training: 2.8270,... Gradient Norm: 0.1278\n",
      "Epoch 511: avg loss training: 2.8270,... Gradient Norm: 0.2816\n",
      "Epoch 512: avg loss training: 2.8270,... Gradient Norm: 0.3184\n",
      "Epoch 513: avg loss training: 2.8270,... Gradient Norm: 0.1959\n",
      "Epoch 514: avg loss training: 2.8270,... Gradient Norm: 0.2433\n",
      "Epoch 515: avg loss training: 2.8270,... Gradient Norm: 0.1802\n",
      "Epoch 516: avg loss training: 2.8270,... Gradient Norm: 0.1352\n",
      "Epoch 517: avg loss training: 2.8270,... Gradient Norm: 0.3201\n",
      "Epoch 518: avg loss training: 2.8270,... Gradient Norm: 0.2818\n",
      "Epoch 519: avg loss training: 2.8270,... Gradient Norm: 0.1245\n",
      "Epoch 520: avg loss training: 2.8270,... Gradient Norm: 0.1656\n",
      "Epoch 521: avg loss training: 2.8270,... Gradient Norm: 0.1764\n",
      "Epoch 522: avg loss training: 2.8270,... Gradient Norm: 0.2264\n",
      "Epoch 523: avg loss training: 2.8270,... Gradient Norm: 0.0749\n",
      "Epoch 524: avg loss training: 2.8270,... Gradient Norm: 0.3933\n",
      "Epoch 525: avg loss training: 2.8270,... Gradient Norm: 0.2667\n",
      "Epoch 526: avg loss training: 2.8269,... Gradient Norm: 0.1336\n",
      "Epoch 527: avg loss training: 2.8269,... Gradient Norm: 0.1700\n",
      "Epoch 528: avg loss training: 2.8269,... Gradient Norm: 0.1742\n",
      "Epoch 529: avg loss training: 2.8269,... Gradient Norm: 0.1556\n",
      "Epoch 530: avg loss training: 2.8269,... Gradient Norm: 0.1334\n",
      "Epoch 531: avg loss training: 2.8269,... Gradient Norm: 0.4194\n",
      "Epoch 532: avg loss training: 2.8269,... Gradient Norm: 0.3758\n",
      "Epoch 533: avg loss training: 2.8269,... Gradient Norm: 0.2686\n",
      "Epoch 534: avg loss training: 2.8269,... Gradient Norm: 0.1843\n",
      "Epoch 535: avg loss training: 2.8269,... Gradient Norm: 0.1910\n",
      "Epoch 536: avg loss training: 2.8269,... Gradient Norm: 0.1958\n",
      "Epoch 537: avg loss training: 2.8269,... Gradient Norm: 0.1394\n",
      "Epoch 538: avg loss training: 2.8269,... Gradient Norm: 0.4039\n",
      "Epoch 539: avg loss training: 2.8269,... Gradient Norm: 0.3200\n",
      "Epoch 540: avg loss training: 2.8269,... Gradient Norm: 0.2397\n",
      "Epoch 541: avg loss training: 2.8269,... Gradient Norm: 0.1595\n",
      "Epoch 542: avg loss training: 2.8269,... Gradient Norm: 0.1618\n",
      "Epoch 543: avg loss training: 2.8269,... Gradient Norm: 0.1323\n",
      "Epoch 544: avg loss training: 2.8269,... Gradient Norm: 0.1834\n",
      "Epoch 545: avg loss training: 2.8269,... Gradient Norm: 0.3308\n",
      "Epoch 546: avg loss training: 2.8269,... Gradient Norm: 0.3865\n",
      "Epoch 547: avg loss training: 2.8269,... Gradient Norm: 0.1645\n",
      "Epoch 548: avg loss training: 2.8269,... Gradient Norm: 0.2046\n",
      "Epoch 549: avg loss training: 2.8269,... Gradient Norm: 0.2659\n",
      "Epoch 550: avg loss training: 2.8269,... Gradient Norm: 0.1666\n",
      "Epoch 551: avg loss training: 2.8269,... Gradient Norm: 0.3075\n",
      "Epoch 552: avg loss training: 2.8268,... Gradient Norm: 0.3137\n",
      "Epoch 553: avg loss training: 2.8268,... Gradient Norm: 0.2249\n",
      "Epoch 554: avg loss training: 2.8268,... Gradient Norm: 0.2162\n",
      "Epoch 555: avg loss training: 2.8268,... Gradient Norm: 0.1547\n",
      "Epoch 556: avg loss training: 2.8268,... Gradient Norm: 0.1445\n",
      "Epoch 557: avg loss training: 2.8268,... Gradient Norm: 0.1564\n",
      "Epoch 558: avg loss training: 2.8268,... Gradient Norm: 0.3276\n",
      "Epoch 559: avg loss training: 2.8268,... Gradient Norm: 0.0755\n",
      "Epoch 560: avg loss training: 2.8268,... Gradient Norm: 0.1865\n",
      "Epoch 561: avg loss training: 2.8268,... Gradient Norm: 0.1587\n",
      "Epoch 562: avg loss training: 2.8268,... Gradient Norm: 0.2359\n",
      "Epoch 563: avg loss training: 2.8268,... Gradient Norm: 0.1831\n",
      "Epoch 564: avg loss training: 2.8268,... Gradient Norm: 0.1320\n",
      "Epoch 565: avg loss training: 2.8268,... Gradient Norm: 0.2663\n",
      "Epoch 566: avg loss training: 2.8268,... Gradient Norm: 0.1302\n",
      "Epoch 567: avg loss training: 2.8268,... Gradient Norm: 0.1092\n",
      "Epoch 568: avg loss training: 2.8268,... Gradient Norm: 0.2177\n",
      "Epoch 569: avg loss training: 2.8268,... Gradient Norm: 0.1187\n",
      "Epoch 570: avg loss training: 2.8268,... Gradient Norm: 0.1330\n",
      "Epoch 571: avg loss training: 2.8268,... Gradient Norm: 0.2136\n",
      "Epoch 572: avg loss training: 2.8268,... Gradient Norm: 0.3384\n",
      "Epoch 573: avg loss training: 2.8268,... Gradient Norm: 0.1003\n",
      "Epoch 574: avg loss training: 2.8268,... Gradient Norm: 0.2518\n",
      "Epoch 575: avg loss training: 2.8268,... Gradient Norm: 0.1373\n",
      "Epoch 576: avg loss training: 2.8268,... Gradient Norm: 0.3368\n",
      "Epoch 577: avg loss training: 2.8268,... Gradient Norm: 0.3093\n",
      "Epoch 578: avg loss training: 2.8268,... Gradient Norm: 0.1087\n",
      "Epoch 579: avg loss training: 2.8268,... Gradient Norm: 0.1863\n",
      "Epoch 580: avg loss training: 2.8268,... Gradient Norm: 0.3250\n",
      "Epoch 581: avg loss training: 2.8268,... Gradient Norm: 0.3307\n",
      "Epoch 582: avg loss training: 2.8268,... Gradient Norm: 0.2775\n",
      "Epoch 583: avg loss training: 2.8268,... Gradient Norm: 0.1628\n",
      "Epoch 584: avg loss training: 2.8268,... Gradient Norm: 0.4192\n",
      "Epoch 585: avg loss training: 2.8268,... Gradient Norm: 0.1577\n",
      "Epoch 586: avg loss training: 2.8268,... Gradient Norm: 0.3783\n",
      "Epoch 587: avg loss training: 2.8267,... Gradient Norm: 0.2926\n",
      "Epoch 588: avg loss training: 2.8267,... Gradient Norm: 0.3912\n",
      "Epoch 589: avg loss training: 2.8267,... Gradient Norm: 0.3953\n",
      "Epoch 590: avg loss training: 2.8267,... Gradient Norm: 0.1454\n",
      "Epoch 591: avg loss training: 2.8267,... Gradient Norm: 0.3812\n",
      "Epoch 592: avg loss training: 2.8267,... Gradient Norm: 0.2607\n",
      "Epoch 593: avg loss training: 2.8267,... Gradient Norm: 0.1568\n",
      "Epoch 594: avg loss training: 2.8267,... Gradient Norm: 0.1589\n",
      "Epoch 595: avg loss training: 2.8267,... Gradient Norm: 0.3959\n",
      "Epoch 596: avg loss training: 2.8267,... Gradient Norm: 0.2831\n",
      "Epoch 597: avg loss training: 2.8267,... Gradient Norm: 0.3400\n",
      "Epoch 598: avg loss training: 2.8267,... Gradient Norm: 0.1293\n",
      "Epoch 599: avg loss training: 2.8267,... Gradient Norm: 0.1571\n",
      "Epoch 600: avg loss training: 2.8267,... Gradient Norm: 0.1498\n",
      "Epoch 601: avg loss training: 2.8267,... Gradient Norm: 0.0819\n",
      "Epoch 602: avg loss training: 2.8267,... Gradient Norm: 0.1975\n",
      "Epoch 603: avg loss training: 2.8267,... Gradient Norm: 0.2967\n",
      "Epoch 604: avg loss training: 2.8267,... Gradient Norm: 0.3176\n",
      "Epoch 605: avg loss training: 2.8267,... Gradient Norm: 0.1379\n",
      "Epoch 606: avg loss training: 2.8267,... Gradient Norm: 0.1518\n",
      "Epoch 607: avg loss training: 2.8267,... Gradient Norm: 0.2931\n",
      "Epoch 608: avg loss training: 2.8267,... Gradient Norm: 0.1263\n",
      "Epoch 609: avg loss training: 2.8267,... Gradient Norm: 0.1321\n",
      "Epoch 610: avg loss training: 2.8267,... Gradient Norm: 0.0851\n",
      "Epoch 611: avg loss training: 2.8267,... Gradient Norm: 0.3047\n",
      "Epoch 612: avg loss training: 2.8267,... Gradient Norm: 0.3089\n",
      "Epoch 613: avg loss training: 2.8267,... Gradient Norm: 0.1796\n",
      "Epoch 614: avg loss training: 2.8267,... Gradient Norm: 0.2018\n",
      "Epoch 615: avg loss training: 2.8267,... Gradient Norm: 0.1431\n",
      "Epoch 616: avg loss training: 2.8267,... Gradient Norm: 0.1568\n",
      "Epoch 617: avg loss training: 2.8267,... Gradient Norm: 0.1502\n",
      "Epoch 618: avg loss training: 2.8267,... Gradient Norm: 0.1443\n",
      "Epoch 619: avg loss training: 2.8267,... Gradient Norm: 0.3653\n",
      "Epoch 620: avg loss training: 2.8267,... Gradient Norm: 0.2625\n",
      "Epoch 621: avg loss training: 2.8267,... Gradient Norm: 0.2205\n",
      "Epoch 622: avg loss training: 2.8267,... Gradient Norm: 0.1505\n",
      "Epoch 623: avg loss training: 2.8267,... Gradient Norm: 0.1679\n",
      "Epoch 624: avg loss training: 2.8267,... Gradient Norm: 0.1274\n",
      "Epoch 625: avg loss training: 2.8267,... Gradient Norm: 0.1021\n",
      "Epoch 626: avg loss training: 2.8267,... Gradient Norm: 0.3167\n",
      "Epoch 627: avg loss training: 2.8267,... Gradient Norm: 0.3103\n",
      "Epoch 628: avg loss training: 2.8267,... Gradient Norm: 0.3818\n",
      "Epoch 629: avg loss training: 2.8267,... Gradient Norm: 0.1508\n",
      "Epoch 630: avg loss training: 2.8267,... Gradient Norm: 0.1549\n",
      "Epoch 631: avg loss training: 2.8267,... Gradient Norm: 0.1823\n",
      "Epoch 632: avg loss training: 2.8267,... Gradient Norm: 0.3479\n",
      "Epoch 633: avg loss training: 2.8267,... Gradient Norm: 0.1893\n",
      "Epoch 634: avg loss training: 2.8267,... Gradient Norm: 0.3875\n",
      "Epoch 635: avg loss training: 2.8267,... Gradient Norm: 0.3889\n",
      "Epoch 636: avg loss training: 2.8267,... Gradient Norm: 0.1212\n",
      "Epoch 637: avg loss training: 2.8267,... Gradient Norm: 0.2587\n",
      "Epoch 638: avg loss training: 2.8266,... Gradient Norm: 0.2122\n",
      "Epoch 639: avg loss training: 2.8266,... Gradient Norm: 0.1325\n",
      "Epoch 640: avg loss training: 2.8266,... Gradient Norm: 0.1688\n",
      "Epoch 641: avg loss training: 2.8266,... Gradient Norm: 0.3587\n",
      "Epoch 642: avg loss training: 2.8266,... Gradient Norm: 0.2625\n",
      "Epoch 643: avg loss training: 2.8266,... Gradient Norm: 0.2722\n",
      "Epoch 644: avg loss training: 2.8266,... Gradient Norm: 0.1426\n",
      "Epoch 645: avg loss training: 2.8266,... Gradient Norm: 0.3957\n",
      "Epoch 646: avg loss training: 2.8266,... Gradient Norm: 0.1002\n",
      "Epoch 647: avg loss training: 2.8266,... Gradient Norm: 0.2879\n",
      "Epoch 648: avg loss training: 2.8266,... Gradient Norm: 0.1689\n",
      "Epoch 649: avg loss training: 2.8266,... Gradient Norm: 0.1412\n",
      "Epoch 650: avg loss training: 2.8266,... Gradient Norm: 0.1472\n",
      "Epoch 651: avg loss training: 2.8266,... Gradient Norm: 0.3051\n",
      "Epoch 652: avg loss training: 2.8266,... Gradient Norm: 0.2696\n",
      "Epoch 653: avg loss training: 2.8266,... Gradient Norm: 0.1289\n",
      "Epoch 654: avg loss training: 2.8266,... Gradient Norm: 0.1416\n",
      "Epoch 655: avg loss training: 2.8266,... Gradient Norm: 0.2150\n",
      "Epoch 656: avg loss training: 2.8266,... Gradient Norm: 0.1134\n",
      "Epoch 657: avg loss training: 2.8266,... Gradient Norm: 0.3544\n",
      "Epoch 658: avg loss training: 2.8266,... Gradient Norm: 0.1037\n",
      "Epoch 659: avg loss training: 2.8266,... Gradient Norm: 0.3102\n",
      "Epoch 660: avg loss training: 2.8266,... Gradient Norm: 0.1103\n",
      "Epoch 661: avg loss training: 2.8266,... Gradient Norm: 0.3088\n",
      "Epoch 662: avg loss training: 2.8266,... Gradient Norm: 0.2190\n",
      "Epoch 663: avg loss training: 2.8266,... Gradient Norm: 0.0949\n",
      "Epoch 664: avg loss training: 2.8266,... Gradient Norm: 0.3095\n",
      "Epoch 665: avg loss training: 2.8266,... Gradient Norm: 0.2808\n",
      "Epoch 666: avg loss training: 2.8266,... Gradient Norm: 0.1428\n",
      "Epoch 667: avg loss training: 2.8266,... Gradient Norm: 0.1592\n",
      "Epoch 668: avg loss training: 2.8266,... Gradient Norm: 0.1061\n",
      "Epoch 669: avg loss training: 2.8266,... Gradient Norm: 0.2965\n",
      "Epoch 670: avg loss training: 2.8266,... Gradient Norm: 0.1687\n",
      "Epoch 671: avg loss training: 2.8266,... Gradient Norm: 0.3336\n",
      "Epoch 672: avg loss training: 2.8266,... Gradient Norm: 0.1619\n",
      "Epoch 673: avg loss training: 2.8266,... Gradient Norm: 0.1605\n",
      "Epoch 674: avg loss training: 2.8266,... Gradient Norm: 0.3667\n",
      "Epoch 675: avg loss training: 2.8266,... Gradient Norm: 0.1297\n",
      "Epoch 676: avg loss training: 2.8266,... Gradient Norm: 0.3859\n",
      "Epoch 677: avg loss training: 2.8266,... Gradient Norm: 0.0639\n",
      "Epoch 678: avg loss training: 2.8266,... Gradient Norm: 0.2582\n",
      "Epoch 679: avg loss training: 2.8266,... Gradient Norm: 0.1203\n",
      "Epoch 680: avg loss training: 2.8266,... Gradient Norm: 0.1656\n",
      "Epoch 681: avg loss training: 2.8266,... Gradient Norm: 0.1579\n",
      "Epoch 682: avg loss training: 2.8266,... Gradient Norm: 0.2503\n",
      "Epoch 683: avg loss training: 2.8266,... Gradient Norm: 0.2661\n",
      "Epoch 684: avg loss training: 2.8266,... Gradient Norm: 0.1448\n",
      "Epoch 685: avg loss training: 2.8266,... Gradient Norm: 0.1864\n",
      "Epoch 686: avg loss training: 2.8266,... Gradient Norm: 0.1320\n",
      "Epoch 687: avg loss training: 2.8266,... Gradient Norm: 0.2529\n",
      "Epoch 688: avg loss training: 2.8266,... Gradient Norm: 0.2268\n",
      "Epoch 689: avg loss training: 2.8266,... Gradient Norm: 0.1418\n",
      "Epoch 690: avg loss training: 2.8266,... Gradient Norm: 0.2935\n",
      "Epoch 691: avg loss training: 2.8266,... Gradient Norm: 0.1951\n",
      "Epoch 692: avg loss training: 2.8266,... Gradient Norm: 0.2428\n",
      "Epoch 693: avg loss training: 2.8266,... Gradient Norm: 0.1423\n",
      "Epoch 694: avg loss training: 2.8266,... Gradient Norm: 0.3482\n",
      "Epoch 695: avg loss training: 2.8266,... Gradient Norm: 0.1967\n",
      "Epoch 696: avg loss training: 2.8266,... Gradient Norm: 0.1280\n",
      "Epoch 697: avg loss training: 2.8266,... Gradient Norm: 0.1051\n",
      "Epoch 698: avg loss training: 2.8266,... Gradient Norm: 0.2484\n",
      "Epoch 699: avg loss training: 2.8266,... Gradient Norm: 0.2404\n",
      "Epoch 700: avg loss training: 2.8266,... Gradient Norm: 0.1319\n",
      "Epoch 701: avg loss training: 2.8266,... Gradient Norm: 0.1644\n",
      "Epoch 702: avg loss training: 2.8266,... Gradient Norm: 0.2267\n",
      "Epoch 703: avg loss training: 2.8266,... Gradient Norm: 0.2895\n",
      "Epoch 704: avg loss training: 2.8266,... Gradient Norm: 0.1493\n",
      "Epoch 705: avg loss training: 2.8266,... Gradient Norm: 0.1593\n",
      "Epoch 706: avg loss training: 2.8266,... Gradient Norm: 0.1574\n",
      "Epoch 707: avg loss training: 2.8266,... Gradient Norm: 0.2784\n",
      "Epoch 708: avg loss training: 2.8266,... Gradient Norm: 0.1209\n",
      "Epoch 709: avg loss training: 2.8266,... Gradient Norm: 0.1359\n",
      "Epoch 710: avg loss training: 2.8266,... Gradient Norm: 0.0955\n",
      "Epoch 711: avg loss training: 2.8266,... Gradient Norm: 0.2424\n",
      "Epoch 712: avg loss training: 2.8266,... Gradient Norm: 0.1665\n",
      "Epoch 713: avg loss training: 2.8266,... Gradient Norm: 0.0935\n",
      "Epoch 714: avg loss training: 2.8266,... Gradient Norm: 0.1216\n",
      "Epoch 715: avg loss training: 2.8266,... Gradient Norm: 0.3873\n",
      "Epoch 716: avg loss training: 2.8266,... Gradient Norm: 0.3226\n",
      "Epoch 717: avg loss training: 2.8266,... Gradient Norm: 0.1557\n",
      "Epoch 718: avg loss training: 2.8266,... Gradient Norm: 0.1207\n",
      "Epoch 719: avg loss training: 2.8266,... Gradient Norm: 0.1174\n",
      "Epoch 720: avg loss training: 2.8266,... Gradient Norm: 0.2579\n",
      "Epoch 721: avg loss training: 2.8266,... Gradient Norm: 0.2898\n",
      "Epoch 722: avg loss training: 2.8266,... Gradient Norm: 0.1701\n",
      "Epoch 723: avg loss training: 2.8266,... Gradient Norm: 0.1471\n",
      "Epoch 724: avg loss training: 2.8266,... Gradient Norm: 0.1904\n",
      "Epoch 725: avg loss training: 2.8266,... Gradient Norm: 0.2922\n",
      "Epoch 726: avg loss training: 2.8266,... Gradient Norm: 0.3001\n",
      "Epoch 727: avg loss training: 2.8266,... Gradient Norm: 0.3803\n",
      "Epoch 728: avg loss training: 2.8266,... Gradient Norm: 0.1377\n",
      "Epoch 729: avg loss training: 2.8266,... Gradient Norm: 0.2873\n",
      "Epoch 730: avg loss training: 2.8266,... Gradient Norm: 0.1859\n",
      "Epoch 731: avg loss training: 2.8266,... Gradient Norm: 0.1575\n",
      "Epoch 732: avg loss training: 2.8265,... Gradient Norm: 0.1188\n",
      "Epoch 733: avg loss training: 2.8265,... Gradient Norm: 0.2145\n",
      "Epoch 734: avg loss training: 2.8265,... Gradient Norm: 0.3204\n",
      "Epoch 735: avg loss training: 2.8265,... Gradient Norm: 0.3373\n",
      "Epoch 736: avg loss training: 2.8265,... Gradient Norm: 0.1735\n",
      "Epoch 737: avg loss training: 2.8265,... Gradient Norm: 0.2226\n",
      "Epoch 738: avg loss training: 2.8265,... Gradient Norm: 0.1397\n",
      "Epoch 739: avg loss training: 2.8265,... Gradient Norm: 0.1773\n",
      "Epoch 740: avg loss training: 2.8265,... Gradient Norm: 0.1416\n",
      "Epoch 741: avg loss training: 2.8265,... Gradient Norm: 0.2977\n",
      "Epoch 742: avg loss training: 2.8265,... Gradient Norm: 0.2922\n",
      "Epoch 743: avg loss training: 2.8265,... Gradient Norm: 0.3517\n",
      "Epoch 744: avg loss training: 2.8265,... Gradient Norm: 0.1469\n",
      "Epoch 745: avg loss training: 2.8265,... Gradient Norm: 0.1598\n",
      "Epoch 746: avg loss training: 2.8265,... Gradient Norm: 0.3000\n",
      "Epoch 747: avg loss training: 2.8265,... Gradient Norm: 0.1363\n",
      "Epoch 748: avg loss training: 2.8265,... Gradient Norm: 0.1640\n",
      "Epoch 749: avg loss training: 2.8265,... Gradient Norm: 0.1092\n",
      "Epoch 750: avg loss training: 2.8265,... Gradient Norm: 0.2866\n",
      "Epoch 751: avg loss training: 2.8265,... Gradient Norm: 0.2829\n",
      "Epoch 752: avg loss training: 2.8265,... Gradient Norm: 0.1409\n",
      "Epoch 753: avg loss training: 2.8265,... Gradient Norm: 0.1826\n",
      "Epoch 754: avg loss training: 2.8265,... Gradient Norm: 0.1358\n",
      "Epoch 755: avg loss training: 2.8265,... Gradient Norm: 0.1166\n",
      "Epoch 756: avg loss training: 2.8265,... Gradient Norm: 0.1835\n",
      "Epoch 757: avg loss training: 2.8265,... Gradient Norm: 0.1046\n",
      "Epoch 758: avg loss training: 2.8265,... Gradient Norm: 0.3242\n",
      "Epoch 759: avg loss training: 2.8265,... Gradient Norm: 0.0857\n",
      "Epoch 760: avg loss training: 2.8265,... Gradient Norm: 0.1874\n",
      "Epoch 761: avg loss training: 2.8265,... Gradient Norm: 0.1241\n",
      "Epoch 762: avg loss training: 2.8265,... Gradient Norm: 0.1319\n",
      "Epoch 763: avg loss training: 2.8265,... Gradient Norm: 0.1069\n",
      "Epoch 764: avg loss training: 2.8265,... Gradient Norm: 0.1784\n",
      "Epoch 765: avg loss training: 2.8265,... Gradient Norm: 0.2959\n",
      "Epoch 766: avg loss training: 2.8265,... Gradient Norm: 0.1510\n",
      "Epoch 767: avg loss training: 2.8265,... Gradient Norm: 0.1368\n",
      "Epoch 768: avg loss training: 2.8265,... Gradient Norm: 0.2063\n",
      "Epoch 769: avg loss training: 2.8265,... Gradient Norm: 0.1853\n",
      "Epoch 770: avg loss training: 2.8265,... Gradient Norm: 0.3209\n",
      "Epoch 771: avg loss training: 2.8265,... Gradient Norm: 0.3628\n",
      "Epoch 772: avg loss training: 2.8265,... Gradient Norm: 0.2690\n",
      "Epoch 773: avg loss training: 2.8265,... Gradient Norm: 0.1865\n",
      "Epoch 774: avg loss training: 2.8265,... Gradient Norm: 0.1501\n",
      "Epoch 775: avg loss training: 2.8265,... Gradient Norm: 0.1572\n",
      "Epoch 776: avg loss training: 2.8265,... Gradient Norm: 0.1140\n",
      "Epoch 777: avg loss training: 2.8265,... Gradient Norm: 0.2816\n",
      "Epoch 778: avg loss training: 2.8265,... Gradient Norm: 0.3750\n",
      "Epoch 779: avg loss training: 2.8265,... Gradient Norm: 0.3218\n",
      "Epoch 780: avg loss training: 2.8265,... Gradient Norm: 0.2701\n",
      "Epoch 781: avg loss training: 2.8265,... Gradient Norm: 0.1392\n",
      "Epoch 782: avg loss training: 2.8265,... Gradient Norm: 0.1406\n",
      "Epoch 783: avg loss training: 2.8265,... Gradient Norm: 0.1141\n",
      "Epoch 784: avg loss training: 2.8265,... Gradient Norm: 0.3179\n",
      "Epoch 785: avg loss training: 2.8265,... Gradient Norm: 0.0704\n",
      "Epoch 786: avg loss training: 2.8265,... Gradient Norm: 0.4011\n",
      "Epoch 787: avg loss training: 2.8265,... Gradient Norm: 0.3322\n",
      "Epoch 788: avg loss training: 2.8265,... Gradient Norm: 0.2359\n",
      "Epoch 789: avg loss training: 2.8265,... Gradient Norm: 0.2703\n",
      "Epoch 790: avg loss training: 2.8265,... Gradient Norm: 0.1374\n",
      "Epoch 791: avg loss training: 2.8265,... Gradient Norm: 0.1379\n",
      "Epoch 792: avg loss training: 2.8265,... Gradient Norm: 0.1351\n",
      "Epoch 793: avg loss training: 2.8265,... Gradient Norm: 0.1250\n",
      "Epoch 794: avg loss training: 2.8265,... Gradient Norm: 0.2183\n",
      "Epoch 795: avg loss training: 2.8265,... Gradient Norm: 0.3001\n",
      "Epoch 796: avg loss training: 2.8265,... Gradient Norm: 0.3954\n",
      "Epoch 797: avg loss training: 2.8265,... Gradient Norm: 0.1393\n",
      "Epoch 798: avg loss training: 2.8265,... Gradient Norm: 0.1929\n",
      "Epoch 799: avg loss training: 2.8265,... Gradient Norm: 0.2422\n",
      "Epoch 800: avg loss training: 2.8265,... Gradient Norm: 0.1267\n",
      "Epoch 801: avg loss training: 2.8265,... Gradient Norm: 0.1281\n",
      "Epoch 802: avg loss training: 2.8265,... Gradient Norm: 0.1279\n",
      "Epoch 803: avg loss training: 2.8265,... Gradient Norm: 0.1735\n",
      "Epoch 804: avg loss training: 2.8265,... Gradient Norm: 0.3537\n",
      "Epoch 805: avg loss training: 2.8265,... Gradient Norm: 0.3012\n",
      "Epoch 806: avg loss training: 2.8265,... Gradient Norm: 0.2806\n",
      "Epoch 807: avg loss training: 2.8265,... Gradient Norm: 0.1350\n",
      "Epoch 808: avg loss training: 2.8265,... Gradient Norm: 0.1334\n",
      "Epoch 809: avg loss training: 2.8265,... Gradient Norm: 0.1224\n",
      "Epoch 810: avg loss training: 2.8265,... Gradient Norm: 0.2796\n",
      "Epoch 811: avg loss training: 2.8265,... Gradient Norm: 0.3067\n",
      "Epoch 812: avg loss training: 2.8265,... Gradient Norm: 0.3793\n",
      "Epoch 813: avg loss training: 2.8265,... Gradient Norm: 0.1163\n",
      "Epoch 814: avg loss training: 2.8265,... Gradient Norm: 0.2874\n",
      "Epoch 815: avg loss training: 2.8265,... Gradient Norm: 0.1443\n",
      "Epoch 816: avg loss training: 2.8265,... Gradient Norm: 0.1367\n",
      "Epoch 817: avg loss training: 2.8265,... Gradient Norm: 0.1217\n",
      "Epoch 818: avg loss training: 2.8265,... Gradient Norm: 0.2570\n",
      "Epoch 819: avg loss training: 2.8265,... Gradient Norm: 0.1038\n",
      "Epoch 820: avg loss training: 2.8265,... Gradient Norm: 0.4028\n",
      "Epoch 821: avg loss training: 2.8265,... Gradient Norm: 0.3061\n",
      "Epoch 822: avg loss training: 2.8265,... Gradient Norm: 0.2673\n",
      "Epoch 823: avg loss training: 2.8265,... Gradient Norm: 0.1687\n",
      "Epoch 824: avg loss training: 2.8265,... Gradient Norm: 0.1464\n",
      "Epoch 825: avg loss training: 2.8265,... Gradient Norm: 0.1161\n",
      "Epoch 826: avg loss training: 2.8265,... Gradient Norm: 0.1583\n",
      "Epoch 827: avg loss training: 2.8265,... Gradient Norm: 0.0822\n",
      "Epoch 828: avg loss training: 2.8265,... Gradient Norm: 0.3052\n",
      "Epoch 829: avg loss training: 2.8265,... Gradient Norm: 0.1085\n",
      "Epoch 830: avg loss training: 2.8265,... Gradient Norm: 0.2041\n",
      "Epoch 831: avg loss training: 2.8265,... Gradient Norm: 0.1400\n",
      "Epoch 832: avg loss training: 2.8265,... Gradient Norm: 0.1415\n",
      "Epoch 833: avg loss training: 2.8265,... Gradient Norm: 0.1526\n",
      "Epoch 834: avg loss training: 2.8265,... Gradient Norm: 0.2704\n",
      "Epoch 835: avg loss training: 2.8265,... Gradient Norm: 0.3679\n",
      "Epoch 836: avg loss training: 2.8265,... Gradient Norm: 0.1462\n",
      "Epoch 837: avg loss training: 2.8265,... Gradient Norm: 0.2268\n",
      "Epoch 838: avg loss training: 2.8265,... Gradient Norm: 0.1429\n",
      "Epoch 839: avg loss training: 2.8265,... Gradient Norm: 0.1022\n",
      "Epoch 840: avg loss training: 2.8265,... Gradient Norm: 0.1656\n",
      "Epoch 841: avg loss training: 2.8265,... Gradient Norm: 0.1617\n",
      "Epoch 842: avg loss training: 2.8265,... Gradient Norm: 0.1332\n",
      "Epoch 843: avg loss training: 2.8265,... Gradient Norm: 0.0911\n",
      "Epoch 844: avg loss training: 2.8265,... Gradient Norm: 0.2837\n",
      "Epoch 845: avg loss training: 2.8265,... Gradient Norm: 0.2553\n",
      "Epoch 846: avg loss training: 2.8265,... Gradient Norm: 0.1795\n",
      "Epoch 847: avg loss training: 2.8265,... Gradient Norm: 0.1464\n",
      "Epoch 848: avg loss training: 2.8265,... Gradient Norm: 0.1159\n",
      "Epoch 849: avg loss training: 2.8265,... Gradient Norm: 0.1142\n",
      "Epoch 850: avg loss training: 2.8265,... Gradient Norm: 0.1221\n",
      "Epoch 851: avg loss training: 2.8265,... Gradient Norm: 0.3503\n",
      "Epoch 852: avg loss training: 2.8265,... Gradient Norm: 0.1843\n",
      "Epoch 853: avg loss training: 2.8265,... Gradient Norm: 0.1527\n",
      "Epoch 854: avg loss training: 2.8265,... Gradient Norm: 0.1094\n",
      "Epoch 855: avg loss training: 2.8265,... Gradient Norm: 0.1043\n",
      "Epoch 856: avg loss training: 2.8265,... Gradient Norm: 0.2653\n",
      "Epoch 857: avg loss training: 2.8265,... Gradient Norm: 0.1848\n",
      "Epoch 858: avg loss training: 2.8265,... Gradient Norm: 0.1575\n",
      "Epoch 859: avg loss training: 2.8265,... Gradient Norm: 0.1700\n",
      "Epoch 860: avg loss training: 2.8265,... Gradient Norm: 0.1619\n",
      "Epoch 861: avg loss training: 2.8265,... Gradient Norm: 0.1681\n",
      "Epoch 862: avg loss training: 2.8265,... Gradient Norm: 0.2902\n",
      "Epoch 863: avg loss training: 2.8265,... Gradient Norm: 0.2745\n",
      "Epoch 864: avg loss training: 2.8265,... Gradient Norm: 0.1974\n",
      "Epoch 865: avg loss training: 2.8265,... Gradient Norm: 0.1645\n",
      "Epoch 866: avg loss training: 2.8265,... Gradient Norm: 0.1390\n",
      "Epoch 867: avg loss training: 2.8265,... Gradient Norm: 0.1564\n",
      "Epoch 868: avg loss training: 2.8265,... Gradient Norm: 0.1172\n",
      "Epoch 869: avg loss training: 2.8265,... Gradient Norm: 0.1680\n",
      "Epoch 870: avg loss training: 2.8265,... Gradient Norm: 0.2633\n",
      "Epoch 871: avg loss training: 2.8265,... Gradient Norm: 0.1097\n",
      "Epoch 872: avg loss training: 2.8265,... Gradient Norm: 0.1337\n",
      "Epoch 873: avg loss training: 2.8265,... Gradient Norm: 0.1510\n",
      "Epoch 874: avg loss training: 2.8265,... Gradient Norm: 0.1038\n",
      "Epoch 875: avg loss training: 2.8265,... Gradient Norm: 0.0934\n",
      "Epoch 876: avg loss training: 2.8265,... Gradient Norm: 0.3147\n",
      "Epoch 877: avg loss training: 2.8265,... Gradient Norm: 0.1298\n",
      "Epoch 878: avg loss training: 2.8265,... Gradient Norm: 0.1171\n",
      "Epoch 879: avg loss training: 2.8265,... Gradient Norm: 0.1233\n",
      "Epoch 880: avg loss training: 2.8265,... Gradient Norm: 0.1324\n",
      "Epoch 881: avg loss training: 2.8265,... Gradient Norm: 0.1157\n",
      "Epoch 882: avg loss training: 2.8265,... Gradient Norm: 0.2660\n",
      "Epoch 883: avg loss training: 2.8265,... Gradient Norm: 0.2269\n",
      "Epoch 884: avg loss training: 2.8265,... Gradient Norm: 0.1599\n",
      "Epoch 885: avg loss training: 2.8265,... Gradient Norm: 0.2911\n",
      "Epoch 886: avg loss training: 2.8265,... Gradient Norm: 0.1497\n",
      "Epoch 887: avg loss training: 2.8265,... Gradient Norm: 0.3304\n",
      "Epoch 888: avg loss training: 2.8265,... Gradient Norm: 0.1992\n",
      "Epoch 889: avg loss training: 2.8265,... Gradient Norm: 0.2593\n",
      "Epoch 890: avg loss training: 2.8265,... Gradient Norm: 0.1396\n",
      "Epoch 891: avg loss training: 2.8265,... Gradient Norm: 0.3599\n",
      "Epoch 892: avg loss training: 2.8265,... Gradient Norm: 0.1314\n",
      "Epoch 893: avg loss training: 2.8265,... Gradient Norm: 0.1881\n",
      "Epoch 894: avg loss training: 2.8265,... Gradient Norm: 0.2427\n",
      "Epoch 895: avg loss training: 2.8265,... Gradient Norm: 0.0900\n",
      "Epoch 896: avg loss training: 2.8265,... Gradient Norm: 0.3487\n",
      "Epoch 897: avg loss training: 2.8265,... Gradient Norm: 0.1138\n",
      "Epoch 898: avg loss training: 2.8265,... Gradient Norm: 0.1432\n",
      "Epoch 899: avg loss training: 2.8265,... Gradient Norm: 0.1414\n",
      "Epoch 900: avg loss training: 2.8265,... Gradient Norm: 0.1063\n",
      "Epoch 901: avg loss training: 2.8265,... Gradient Norm: 0.2662\n",
      "Epoch 902: avg loss training: 2.8265,... Gradient Norm: 0.0923\n",
      "Epoch 903: avg loss training: 2.8265,... Gradient Norm: 0.1299\n",
      "Epoch 904: avg loss training: 2.8265,... Gradient Norm: 0.1177\n",
      "Epoch 905: avg loss training: 2.8265,... Gradient Norm: 0.1945\n",
      "Epoch 906: avg loss training: 2.8265,... Gradient Norm: 0.3391\n",
      "Epoch 907: avg loss training: 2.8265,... Gradient Norm: 0.1350\n",
      "Epoch 908: avg loss training: 2.8265,... Gradient Norm: 0.2012\n",
      "Epoch 909: avg loss training: 2.8265,... Gradient Norm: 0.1411\n",
      "Epoch 910: avg loss training: 2.8265,... Gradient Norm: 0.1346\n",
      "Epoch 911: avg loss training: 2.8265,... Gradient Norm: 0.2993\n",
      "Epoch 912: avg loss training: 2.8265,... Gradient Norm: 0.2554\n",
      "Epoch 913: avg loss training: 2.8265,... Gradient Norm: 0.0908\n",
      "Epoch 914: avg loss training: 2.8265,... Gradient Norm: 0.2632\n",
      "Epoch 915: avg loss training: 2.8265,... Gradient Norm: 0.1184\n",
      "Epoch 916: avg loss training: 2.8265,... Gradient Norm: 0.2608\n",
      "Epoch 917: avg loss training: 2.8265,... Gradient Norm: 0.1467\n",
      "Epoch 918: avg loss training: 2.8265,... Gradient Norm: 0.1194\n",
      "Epoch 919: avg loss training: 2.8265,... Gradient Norm: 0.1841\n",
      "Epoch 920: avg loss training: 2.8265,... Gradient Norm: 0.3201\n",
      "Epoch 921: avg loss training: 2.8265,... Gradient Norm: 0.2776\n",
      "Epoch 922: avg loss training: 2.8265,... Gradient Norm: 0.2876\n",
      "Epoch 923: avg loss training: 2.8265,... Gradient Norm: 0.1337\n",
      "Epoch 924: avg loss training: 2.8265,... Gradient Norm: 0.1585\n",
      "Epoch 925: avg loss training: 2.8265,... Gradient Norm: 0.1015\n",
      "Epoch 926: avg loss training: 2.8265,... Gradient Norm: 0.2640\n",
      "Epoch 927: avg loss training: 2.8265,... Gradient Norm: 0.1375\n",
      "Epoch 928: avg loss training: 2.8265,... Gradient Norm: 0.1169\n",
      "Epoch 929: avg loss training: 2.8265,... Gradient Norm: 0.2841\n",
      "Epoch 930: avg loss training: 2.8265,... Gradient Norm: 0.2686\n",
      "Epoch 931: avg loss training: 2.8265,... Gradient Norm: 0.1780\n",
      "Epoch 932: avg loss training: 2.8265,... Gradient Norm: 0.1574\n",
      "Epoch 933: avg loss training: 2.8265,... Gradient Norm: 0.1512\n",
      "Epoch 934: avg loss training: 2.8265,... Gradient Norm: 0.1084\n",
      "Epoch 935: avg loss training: 2.8265,... Gradient Norm: 0.1250\n",
      "Epoch 936: avg loss training: 2.8265,... Gradient Norm: 0.1661\n",
      "Epoch 937: avg loss training: 2.8265,... Gradient Norm: 0.3667\n",
      "Epoch 938: avg loss training: 2.8265,... Gradient Norm: 0.1280\n",
      "Epoch 939: avg loss training: 2.8265,... Gradient Norm: 0.2567\n",
      "Epoch 940: avg loss training: 2.8265,... Gradient Norm: 0.1070\n",
      "Epoch 941: avg loss training: 2.8265,... Gradient Norm: 0.2726\n",
      "Epoch 942: avg loss training: 2.8265,... Gradient Norm: 0.1216\n",
      "Epoch 943: avg loss training: 2.8265,... Gradient Norm: 0.2831\n",
      "Epoch 944: avg loss training: 2.8265,... Gradient Norm: 0.1380\n",
      "Epoch 945: avg loss training: 2.8265,... Gradient Norm: 0.1395\n",
      "Epoch 946: avg loss training: 2.8265,... Gradient Norm: 0.3673\n",
      "Epoch 947: avg loss training: 2.8265,... Gradient Norm: 0.1224\n",
      "Epoch 948: avg loss training: 2.8265,... Gradient Norm: 0.2544\n",
      "Epoch 949: avg loss training: 2.8265,... Gradient Norm: 0.1801\n",
      "Epoch 950: avg loss training: 2.8265,... Gradient Norm: 0.4085\n",
      "Epoch 951: avg loss training: 2.8265,... Gradient Norm: 0.1610\n",
      "Epoch 952: avg loss training: 2.8265,... Gradient Norm: 0.1774\n",
      "Epoch 953: avg loss training: 2.8265,... Gradient Norm: 0.2775\n",
      "Epoch 954: avg loss training: 2.8265,... Gradient Norm: 0.1127\n",
      "Epoch 955: avg loss training: 2.8265,... Gradient Norm: 0.1152\n",
      "Epoch 956: avg loss training: 2.8265,... Gradient Norm: 0.3004\n",
      "Epoch 957: avg loss training: 2.8265,... Gradient Norm: 0.2533\n",
      "Epoch 958: avg loss training: 2.8265,... Gradient Norm: 0.1326\n",
      "Epoch 959: avg loss training: 2.8265,... Gradient Norm: 0.1376\n",
      "Epoch 960: avg loss training: 2.8265,... Gradient Norm: 0.1298\n",
      "Epoch 961: avg loss training: 2.8265,... Gradient Norm: 0.2080\n",
      "Epoch 962: avg loss training: 2.8265,... Gradient Norm: 0.1662\n",
      "Epoch 963: avg loss training: 2.8265,... Gradient Norm: 0.1403\n",
      "Epoch 964: avg loss training: 2.8265,... Gradient Norm: 0.3850\n",
      "Epoch 965: avg loss training: 2.8265,... Gradient Norm: 0.2815\n",
      "Epoch 966: avg loss training: 2.8265,... Gradient Norm: 0.2187\n",
      "Epoch 967: avg loss training: 2.8265,... Gradient Norm: 0.1195\n",
      "Epoch 968: avg loss training: 2.8265,... Gradient Norm: 0.1260\n",
      "Epoch 969: avg loss training: 2.8265,... Gradient Norm: 0.1570\n",
      "Epoch 970: avg loss training: 2.8265,... Gradient Norm: 0.1309\n",
      "Epoch 971: avg loss training: 2.8265,... Gradient Norm: 0.3079\n",
      "Epoch 972: avg loss training: 2.8265,... Gradient Norm: 0.0930\n",
      "Epoch 973: avg loss training: 2.8265,... Gradient Norm: 0.3704\n",
      "Epoch 974: avg loss training: 2.8265,... Gradient Norm: 0.3596\n",
      "Epoch 975: avg loss training: 2.8265,... Gradient Norm: 0.2071\n",
      "Epoch 976: avg loss training: 2.8265,... Gradient Norm: 0.2175\n",
      "Epoch 977: avg loss training: 2.8265,... Gradient Norm: 0.1459\n",
      "Epoch 978: avg loss training: 2.8265,... Gradient Norm: 0.1247\n",
      "Epoch 979: avg loss training: 2.8265,... Gradient Norm: 0.1187\n",
      "Epoch 980: avg loss training: 2.8265,... Gradient Norm: 0.0638\n",
      "Epoch 981: avg loss training: 2.8265,... Gradient Norm: 0.2783\n",
      "Epoch 982: avg loss training: 2.8265,... Gradient Norm: 0.2742\n",
      "Epoch 983: avg loss training: 2.8265,... Gradient Norm: 0.1188\n",
      "Epoch 984: avg loss training: 2.8265,... Gradient Norm: 0.1241\n",
      "Epoch 985: avg loss training: 2.8265,... Gradient Norm: 0.1488\n",
      "Epoch 986: avg loss training: 2.8265,... Gradient Norm: 0.2504\n",
      "Epoch 987: avg loss training: 2.8265,... Gradient Norm: 0.1409\n",
      "Epoch 988: avg loss training: 2.8265,... Gradient Norm: 0.1313\n",
      "Epoch 989: avg loss training: 2.8265,... Gradient Norm: 0.3303\n",
      "Epoch 990: avg loss training: 2.8265,... Gradient Norm: 0.1352\n",
      "Epoch 991: avg loss training: 2.8265,... Gradient Norm: 0.0632\n",
      "Epoch 992: avg loss training: 2.8265,... Gradient Norm: 0.1023\n",
      "Epoch 993: avg loss training: 2.8265,... Gradient Norm: 0.1642\n",
      "Epoch 994: avg loss training: 2.8265,... Gradient Norm: 0.1661\n",
      "Epoch 995: avg loss training: 2.8265,... Gradient Norm: 0.1325\n",
      "Epoch 996: avg loss training: 2.8265,... Gradient Norm: 0.2774\n",
      "Epoch 997: avg loss training: 2.8265,... Gradient Norm: 0.1556\n",
      "Epoch 998: avg loss training: 2.8265,... Gradient Norm: 0.1357\n",
      "Epoch 999: avg loss training: 2.8265,... Gradient Norm: 0.1102\n",
      "Epoch 1000: avg loss training: 2.8265,... Gradient Norm: 0.2054\n",
      "final result: (0.022909228520449743, -0.0690273597669036) ********************\n"
     ]
    }
   ],
   "source": [
    "score_new, score_orig, model_x, model_y, new_x_f, new_x_r, f_forward, f_reverse= loci_w_marginal(\n",
    "    x, y, independence_test=False, neural_network=True, \n",
    "    return_function=True, n_steps=1000, marginal_loglik = True\n",
    ")\n",
    "\n",
    "print(f\"final result: {score_new, score_orig} ********************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4567bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: avg loss training: 3.5056,... Gradient Norm: 7.0747\n",
      "Epoch 2: avg loss training: 120.5641,... Gradient Norm: 1372.0607\n",
      "Epoch 3: avg loss training: 9.3453,... Gradient Norm: 79.3654\n",
      "Epoch 4: avg loss training: 32.6629,... Gradient Norm: 404.1977\n",
      "Epoch 5: avg loss training: 13.3740,... Gradient Norm: 118.2449\n",
      "Epoch 6: avg loss training: 10.6215,... Gradient Norm: 59.4674\n",
      "Epoch 7: avg loss training: 11.9939,... Gradient Norm: 100.9173\n",
      "Epoch 8: avg loss training: 7.2389,... Gradient Norm: 31.9358\n",
      "Epoch 9: avg loss training: 5.2544,... Gradient Norm: 24.8780\n",
      "Epoch 10: avg loss training: 4.0854,... Gradient Norm: 10.1568\n",
      "Epoch 11: avg loss training: 4.0477,... Gradient Norm: 10.2384\n",
      "Epoch 12: avg loss training: 4.4805,... Gradient Norm: 17.5710\n",
      "Epoch 13: avg loss training: 4.6511,... Gradient Norm: 19.1036\n",
      "Epoch 14: avg loss training: 4.4699,... Gradient Norm: 16.1959\n",
      "Epoch 15: avg loss training: 4.1469,... Gradient Norm: 11.4120\n",
      "Epoch 16: avg loss training: 3.9182,... Gradient Norm: 8.6553\n",
      "Epoch 17: avg loss training: 3.8537,... Gradient Norm: 8.1019\n",
      "Epoch 18: avg loss training: 3.6771,... Gradient Norm: 5.3468\n",
      "Epoch 19: avg loss training: 3.5645,... Gradient Norm: 6.2252\n",
      "Epoch 20: avg loss training: 3.4564,... Gradient Norm: 6.2304\n",
      "Epoch 21: avg loss training: 3.3507,... Gradient Norm: 3.7966\n",
      "Epoch 22: avg loss training: 3.3265,... Gradient Norm: 4.1842\n",
      "Epoch 23: avg loss training: 3.3367,... Gradient Norm: 6.5697\n",
      "Epoch 24: avg loss training: 3.2738,... Gradient Norm: 5.0784\n",
      "Epoch 25: avg loss training: 3.2087,... Gradient Norm: 2.3465\n",
      "Epoch 26: avg loss training: 3.1996,... Gradient Norm: 4.6886\n",
      "Epoch 27: avg loss training: 3.1679,... Gradient Norm: 4.7515\n",
      "Epoch 28: avg loss training: 3.1144,... Gradient Norm: 2.2922\n",
      "Epoch 29: avg loss training: 3.1048,... Gradient Norm: 4.6988\n",
      "Epoch 30: avg loss training: 3.0651,... Gradient Norm: 3.8921\n",
      "Epoch 31: avg loss training: 3.0181,... Gradient Norm: 1.3880\n",
      "Epoch 32: avg loss training: 3.0122,... Gradient Norm: 4.5835\n",
      "Epoch 33: avg loss training: 2.9877,... Gradient Norm: 3.6320\n",
      "Epoch 34: avg loss training: 2.9682,... Gradient Norm: 1.6226\n",
      "Epoch 35: avg loss training: 2.9696,... Gradient Norm: 3.3995\n",
      "Epoch 36: avg loss training: 2.9488,... Gradient Norm: 2.7017\n",
      "Epoch 37: avg loss training: 2.9300,... Gradient Norm: 1.2784\n",
      "Epoch 38: avg loss training: 2.9285,... Gradient Norm: 3.2920\n",
      "Epoch 39: avg loss training: 2.9086,... Gradient Norm: 1.9994\n",
      "Epoch 40: avg loss training: 2.8963,... Gradient Norm: 2.0613\n",
      "Epoch 41: avg loss training: 2.8948,... Gradient Norm: 3.3788\n",
      "Epoch 42: avg loss training: 2.8812,... Gradient Norm: 0.7712\n",
      "Epoch 43: avg loss training: 2.8839,... Gradient Norm: 3.5483\n",
      "Epoch 44: avg loss training: 2.8718,... Gradient Norm: 2.0637\n",
      "Epoch 45: avg loss training: 2.8657,... Gradient Norm: 1.9843\n",
      "Epoch 46: avg loss training: 2.8648,... Gradient Norm: 2.5941\n",
      "Epoch 47: avg loss training: 2.8591,... Gradient Norm: 0.7023\n",
      "Epoch 48: avg loss training: 2.8554,... Gradient Norm: 1.9514\n",
      "Epoch 49: avg loss training: 2.8494,... Gradient Norm: 1.0160\n",
      "Epoch 50: avg loss training: 2.8483,... Gradient Norm: 1.6001\n",
      "Epoch 51: avg loss training: 2.8433,... Gradient Norm: 1.2498\n",
      "Epoch 52: avg loss training: 2.8386,... Gradient Norm: 1.0948\n",
      "Epoch 53: avg loss training: 2.8373,... Gradient Norm: 1.5003\n",
      "Epoch 54: avg loss training: 2.8332,... Gradient Norm: 0.8668\n",
      "Epoch 55: avg loss training: 2.8302,... Gradient Norm: 1.4295\n",
      "Epoch 56: avg loss training: 2.8272,... Gradient Norm: 0.5788\n",
      "Epoch 57: avg loss training: 2.8268,... Gradient Norm: 1.6801\n",
      "Epoch 58: avg loss training: 2.8228,... Gradient Norm: 0.4239\n",
      "Epoch 59: avg loss training: 2.8220,... Gradient Norm: 1.3964\n",
      "Epoch 60: avg loss training: 2.8196,... Gradient Norm: 0.7053\n",
      "Epoch 61: avg loss training: 2.8175,... Gradient Norm: 0.9212\n",
      "Epoch 62: avg loss training: 2.8162,... Gradient Norm: 0.6967\n",
      "Epoch 63: avg loss training: 2.8150,... Gradient Norm: 0.8202\n",
      "Epoch 64: avg loss training: 2.8126,... Gradient Norm: 0.4905\n",
      "Epoch 65: avg loss training: 2.8117,... Gradient Norm: 0.7959\n",
      "Epoch 66: avg loss training: 2.8098,... Gradient Norm: 0.5206\n",
      "Epoch 67: avg loss training: 2.8086,... Gradient Norm: 0.8887\n",
      "Epoch 68: avg loss training: 2.8072,... Gradient Norm: 0.6729\n",
      "Epoch 69: avg loss training: 2.8054,... Gradient Norm: 0.8521\n",
      "Epoch 70: avg loss training: 2.8037,... Gradient Norm: 0.3527\n",
      "Epoch 71: avg loss training: 2.8024,... Gradient Norm: 0.9085\n",
      "Epoch 72: avg loss training: 2.8006,... Gradient Norm: 0.2456\n",
      "Epoch 73: avg loss training: 2.7995,... Gradient Norm: 0.7186\n",
      "Epoch 74: avg loss training: 2.7977,... Gradient Norm: 0.3221\n",
      "Epoch 75: avg loss training: 2.7965,... Gradient Norm: 0.6157\n",
      "Epoch 76: avg loss training: 2.7951,... Gradient Norm: 0.3686\n",
      "Epoch 77: avg loss training: 2.7939,... Gradient Norm: 0.3725\n",
      "Epoch 78: avg loss training: 2.7927,... Gradient Norm: 0.4890\n",
      "Epoch 79: avg loss training: 2.7914,... Gradient Norm: 0.2230\n",
      "Epoch 80: avg loss training: 2.7903,... Gradient Norm: 0.3254\n",
      "Epoch 81: avg loss training: 2.7891,... Gradient Norm: 0.2499\n",
      "Epoch 82: avg loss training: 2.7881,... Gradient Norm: 0.3613\n",
      "Epoch 83: avg loss training: 2.7869,... Gradient Norm: 0.1294\n",
      "Epoch 84: avg loss training: 2.7860,... Gradient Norm: 0.3033\n",
      "Epoch 85: avg loss training: 2.7849,... Gradient Norm: 0.1927\n",
      "Epoch 86: avg loss training: 2.7840,... Gradient Norm: 0.2912\n",
      "Epoch 87: avg loss training: 2.7831,... Gradient Norm: 0.2599\n",
      "Epoch 88: avg loss training: 2.7822,... Gradient Norm: 0.3253\n",
      "Epoch 89: avg loss training: 2.7814,... Gradient Norm: 0.2559\n",
      "Epoch 90: avg loss training: 2.7805,... Gradient Norm: 0.2835\n",
      "Epoch 91: avg loss training: 2.7798,... Gradient Norm: 0.4867\n",
      "Epoch 92: avg loss training: 2.7789,... Gradient Norm: 0.4557\n",
      "Epoch 93: avg loss training: 2.7781,... Gradient Norm: 0.3081\n",
      "Epoch 94: avg loss training: 2.7773,... Gradient Norm: 0.2346\n",
      "Epoch 95: avg loss training: 2.7767,... Gradient Norm: 0.4854\n",
      "Epoch 96: avg loss training: 2.7762,... Gradient Norm: 0.7547\n",
      "Epoch 97: avg loss training: 2.7756,... Gradient Norm: 1.1377\n",
      "Epoch 98: avg loss training: 2.7754,... Gradient Norm: 1.6426\n",
      "Epoch 99: avg loss training: 2.7756,... Gradient Norm: 2.3717\n",
      "Epoch 100: avg loss training: 2.7761,... Gradient Norm: 3.0778\n",
      "Epoch 101: avg loss training: 2.7760,... Gradient Norm: 3.1682\n",
      "Epoch 102: avg loss training: 2.7743,... Gradient Norm: 2.6888\n",
      "Epoch 103: avg loss training: 2.7726,... Gradient Norm: 1.6672\n",
      "Epoch 104: avg loss training: 2.7714,... Gradient Norm: 0.2956\n",
      "Epoch 105: avg loss training: 2.7712,... Gradient Norm: 0.9776\n",
      "Epoch 106: avg loss training: 2.7710,... Gradient Norm: 1.4923\n",
      "Epoch 107: avg loss training: 2.7703,... Gradient Norm: 1.3737\n",
      "Epoch 108: avg loss training: 2.7696,... Gradient Norm: 0.9121\n",
      "Epoch 109: avg loss training: 2.7689,... Gradient Norm: 0.4042\n",
      "Epoch 110: avg loss training: 2.7683,... Gradient Norm: 0.2825\n",
      "Epoch 111: avg loss training: 2.7681,... Gradient Norm: 0.9942\n",
      "Epoch 112: avg loss training: 2.7680,... Gradient Norm: 1.2823\n",
      "Epoch 113: avg loss training: 2.7673,... Gradient Norm: 0.8635\n",
      "Epoch 114: avg loss training: 2.7666,... Gradient Norm: 0.2111\n",
      "Epoch 115: avg loss training: 2.7662,... Gradient Norm: 0.7665\n",
      "Epoch 116: avg loss training: 2.7659,... Gradient Norm: 1.1595\n",
      "Epoch 117: avg loss training: 2.7653,... Gradient Norm: 1.2236\n",
      "Epoch 118: avg loss training: 2.7647,... Gradient Norm: 1.4325\n",
      "Epoch 119: avg loss training: 2.7646,... Gradient Norm: 1.9304\n",
      "Epoch 120: avg loss training: 2.7651,... Gradient Norm: 2.7317\n",
      "Epoch 121: avg loss training: 2.7676,... Gradient Norm: 4.3370\n",
      "Epoch 122: avg loss training: 2.7718,... Gradient Norm: 5.9231\n",
      "Epoch 123: avg loss training: 2.7724,... Gradient Norm: 6.5757\n",
      "Epoch 124: avg loss training: 2.7683,... Gradient Norm: 5.1594\n",
      "Epoch 125: avg loss training: 2.7620,... Gradient Norm: 1.9465\n",
      "Epoch 126: avg loss training: 2.7614,... Gradient Norm: 1.8043\n",
      "Epoch 127: avg loss training: 2.7655,... Gradient Norm: 4.5237\n",
      "Epoch 128: avg loss training: 2.7665,... Gradient Norm: 4.9602\n",
      "Epoch 129: avg loss training: 2.7614,... Gradient Norm: 2.8691\n",
      "Epoch 130: avg loss training: 2.7591,... Gradient Norm: 0.1712\n",
      "Epoch 131: avg loss training: 2.7602,... Gradient Norm: 2.1654\n",
      "Epoch 132: avg loss training: 2.7605,... Gradient Norm: 2.9672\n",
      "Epoch 133: avg loss training: 2.7595,... Gradient Norm: 2.3689\n",
      "Epoch 134: avg loss training: 2.7579,... Gradient Norm: 0.8024\n",
      "Epoch 135: avg loss training: 2.7578,... Gradient Norm: 1.2468\n",
      "Epoch 136: avg loss training: 2.7586,... Gradient Norm: 2.4808\n",
      "Epoch 137: avg loss training: 2.7582,... Gradient Norm: 2.4857\n",
      "Epoch 138: avg loss training: 2.7572,... Gradient Norm: 1.5025\n",
      "Epoch 139: avg loss training: 2.7562,... Gradient Norm: 0.3275\n",
      "Epoch 140: avg loss training: 2.7569,... Gradient Norm: 2.0227\n",
      "Epoch 141: avg loss training: 2.7576,... Gradient Norm: 3.0141\n",
      "Epoch 142: avg loss training: 2.7569,... Gradient Norm: 2.5209\n",
      "Epoch 143: avg loss training: 2.7553,... Gradient Norm: 0.9164\n",
      "Epoch 144: avg loss training: 2.7552,... Gradient Norm: 1.0519\n",
      "Epoch 145: avg loss training: 2.7559,... Gradient Norm: 2.3771\n",
      "Epoch 146: avg loss training: 2.7558,... Gradient Norm: 2.6084\n",
      "Epoch 147: avg loss training: 2.7548,... Gradient Norm: 1.6680\n",
      "Epoch 148: avg loss training: 2.7539,... Gradient Norm: 0.1046\n",
      "Epoch 149: avg loss training: 2.7542,... Gradient Norm: 1.5121\n",
      "Epoch 150: avg loss training: 2.7544,... Gradient Norm: 2.1264\n",
      "Epoch 151: avg loss training: 2.7539,... Gradient Norm: 1.6587\n",
      "Epoch 152: avg loss training: 2.7531,... Gradient Norm: 0.4793\n",
      "Epoch 153: avg loss training: 2.7530,... Gradient Norm: 0.9880\n",
      "Epoch 154: avg loss training: 2.7533,... Gradient Norm: 1.7127\n",
      "Epoch 155: avg loss training: 2.7529,... Gradient Norm: 1.4667\n",
      "Epoch 156: avg loss training: 2.7523,... Gradient Norm: 0.4904\n",
      "Epoch 157: avg loss training: 2.7522,... Gradient Norm: 0.6617\n",
      "Epoch 158: avg loss training: 2.7522,... Gradient Norm: 1.2081\n",
      "Epoch 159: avg loss training: 2.7518,... Gradient Norm: 0.8871\n",
      "Epoch 160: avg loss training: 2.7515,... Gradient Norm: 0.1596\n",
      "Epoch 161: avg loss training: 2.7513,... Gradient Norm: 0.4496\n",
      "Epoch 162: avg loss training: 2.7511,... Gradient Norm: 0.1078\n",
      "Epoch 163: avg loss training: 2.7510,... Gradient Norm: 0.7709\n",
      "Epoch 164: avg loss training: 2.7510,... Gradient Norm: 1.2025\n",
      "Epoch 165: avg loss training: 2.7506,... Gradient Norm: 0.8445\n",
      "Epoch 166: avg loss training: 2.7503,... Gradient Norm: 0.0940\n",
      "Epoch 167: avg loss training: 2.7503,... Gradient Norm: 0.5499\n",
      "Epoch 168: avg loss training: 2.7500,... Gradient Norm: 0.3380\n",
      "Epoch 169: avg loss training: 2.7498,... Gradient Norm: 0.3790\n",
      "Epoch 170: avg loss training: 2.7498,... Gradient Norm: 0.7784\n",
      "Epoch 171: avg loss training: 2.7496,... Gradient Norm: 0.8387\n",
      "Epoch 172: avg loss training: 2.7494,... Gradient Norm: 0.9404\n",
      "Epoch 173: avg loss training: 2.7493,... Gradient Norm: 0.9549\n",
      "Epoch 174: avg loss training: 2.7491,... Gradient Norm: 0.7361\n",
      "Epoch 175: avg loss training: 2.7489,... Gradient Norm: 0.2984\n",
      "Epoch 176: avg loss training: 2.7487,... Gradient Norm: 0.1743\n",
      "Epoch 177: avg loss training: 2.7486,... Gradient Norm: 0.2110\n",
      "Epoch 178: avg loss training: 2.7484,... Gradient Norm: 0.0793\n",
      "Epoch 179: avg loss training: 2.7482,... Gradient Norm: 0.2555\n",
      "Epoch 180: avg loss training: 2.7481,... Gradient Norm: 0.4096\n",
      "Epoch 181: avg loss training: 2.7480,... Gradient Norm: 0.5029\n",
      "Epoch 182: avg loss training: 2.7479,... Gradient Norm: 0.5109\n",
      "Epoch 183: avg loss training: 2.7477,... Gradient Norm: 0.3019\n",
      "Epoch 184: avg loss training: 2.7476,... Gradient Norm: 0.1127\n",
      "Epoch 185: avg loss training: 2.7475,... Gradient Norm: 0.3179\n",
      "Epoch 186: avg loss training: 2.7474,... Gradient Norm: 0.6306\n",
      "Epoch 187: avg loss training: 2.7473,... Gradient Norm: 0.8840\n",
      "Epoch 188: avg loss training: 2.7473,... Gradient Norm: 1.1753\n",
      "Epoch 189: avg loss training: 2.7473,... Gradient Norm: 1.3436\n",
      "Epoch 190: avg loss training: 2.7471,... Gradient Norm: 1.0493\n",
      "Epoch 191: avg loss training: 2.7468,... Gradient Norm: 0.3985\n",
      "Epoch 192: avg loss training: 2.7467,... Gradient Norm: 0.2685\n",
      "Epoch 193: avg loss training: 2.7467,... Gradient Norm: 0.8925\n",
      "Epoch 194: avg loss training: 2.7467,... Gradient Norm: 1.3523\n",
      "Epoch 195: avg loss training: 2.7467,... Gradient Norm: 1.6243\n",
      "Epoch 196: avg loss training: 2.7465,... Gradient Norm: 1.5593\n",
      "Epoch 197: avg loss training: 2.7464,... Gradient Norm: 1.2836\n",
      "Epoch 198: avg loss training: 2.7461,... Gradient Norm: 0.7187\n",
      "Epoch 199: avg loss training: 2.7459,... Gradient Norm: 0.3285\n",
      "Epoch 200: avg loss training: 2.7460,... Gradient Norm: 1.2414\n",
      "Epoch 201: avg loss training: 2.7461,... Gradient Norm: 1.7765\n",
      "Epoch 202: avg loss training: 2.7462,... Gradient Norm: 1.8625\n",
      "Epoch 203: avg loss training: 2.7457,... Gradient Norm: 1.2038\n",
      "Epoch 204: avg loss training: 2.7454,... Gradient Norm: 0.2560\n",
      "Epoch 205: avg loss training: 2.7454,... Gradient Norm: 0.7743\n",
      "Epoch 206: avg loss training: 2.7454,... Gradient Norm: 1.2279\n",
      "Epoch 207: avg loss training: 2.7453,... Gradient Norm: 1.2876\n",
      "Epoch 208: avg loss training: 2.7451,... Gradient Norm: 0.8851\n",
      "Epoch 209: avg loss training: 2.7449,... Gradient Norm: 0.2937\n",
      "Epoch 210: avg loss training: 2.7448,... Gradient Norm: 0.4389\n",
      "Epoch 211: avg loss training: 2.7447,... Gradient Norm: 0.9280\n",
      "Epoch 212: avg loss training: 2.7448,... Gradient Norm: 1.2560\n",
      "Epoch 213: avg loss training: 2.7446,... Gradient Norm: 1.4211\n",
      "Epoch 214: avg loss training: 2.7446,... Gradient Norm: 1.2927\n",
      "Epoch 215: avg loss training: 2.7443,... Gradient Norm: 0.7364\n",
      "Epoch 216: avg loss training: 2.7442,... Gradient Norm: 0.2499\n",
      "Epoch 217: avg loss training: 2.7442,... Gradient Norm: 0.7758\n",
      "Epoch 218: avg loss training: 2.7441,... Gradient Norm: 0.9170\n",
      "Epoch 219: avg loss training: 2.7439,... Gradient Norm: 0.7473\n",
      "Epoch 220: avg loss training: 2.7438,... Gradient Norm: 0.5729\n",
      "Epoch 221: avg loss training: 2.7437,... Gradient Norm: 0.6725\n",
      "Epoch 222: avg loss training: 2.7436,... Gradient Norm: 0.5808\n",
      "Epoch 223: avg loss training: 2.7435,... Gradient Norm: 0.2423\n",
      "Epoch 224: avg loss training: 2.7434,... Gradient Norm: 0.2434\n",
      "Epoch 225: avg loss training: 2.7433,... Gradient Norm: 0.8432\n",
      "Epoch 226: avg loss training: 2.7434,... Gradient Norm: 1.3623\n",
      "Epoch 227: avg loss training: 2.7433,... Gradient Norm: 1.4739\n",
      "Epoch 228: avg loss training: 2.7432,... Gradient Norm: 1.0566\n",
      "Epoch 229: avg loss training: 2.7430,... Gradient Norm: 0.2510\n",
      "Epoch 230: avg loss training: 2.7429,... Gradient Norm: 0.7329\n",
      "Epoch 231: avg loss training: 2.7429,... Gradient Norm: 1.1496\n",
      "Epoch 232: avg loss training: 2.7427,... Gradient Norm: 0.9748\n",
      "Epoch 233: avg loss training: 2.7426,... Gradient Norm: 0.6316\n",
      "Epoch 234: avg loss training: 2.7425,... Gradient Norm: 0.2093\n",
      "Epoch 235: avg loss training: 2.7424,... Gradient Norm: 0.3373\n",
      "Epoch 236: avg loss training: 2.7423,... Gradient Norm: 0.4514\n",
      "Epoch 237: avg loss training: 2.7422,... Gradient Norm: 0.6350\n",
      "Epoch 238: avg loss training: 2.7422,... Gradient Norm: 1.1243\n",
      "Epoch 239: avg loss training: 2.7423,... Gradient Norm: 1.5848\n",
      "Epoch 240: avg loss training: 2.7423,... Gradient Norm: 1.5288\n",
      "Epoch 241: avg loss training: 2.7420,... Gradient Norm: 0.7721\n",
      "Epoch 242: avg loss training: 2.7419,... Gradient Norm: 0.5064\n",
      "Epoch 243: avg loss training: 2.7420,... Gradient Norm: 1.4848\n",
      "Epoch 244: avg loss training: 2.7420,... Gradient Norm: 1.5348\n",
      "Epoch 245: avg loss training: 2.7416,... Gradient Norm: 0.6695\n",
      "Epoch 246: avg loss training: 2.7416,... Gradient Norm: 0.5668\n",
      "Epoch 247: avg loss training: 2.7417,... Gradient Norm: 1.3570\n",
      "Epoch 248: avg loss training: 2.7417,... Gradient Norm: 1.5182\n",
      "Epoch 249: avg loss training: 2.7414,... Gradient Norm: 0.9523\n",
      "Epoch 250: avg loss training: 2.7413,... Gradient Norm: 0.2757\n",
      "Epoch 251: avg loss training: 2.7413,... Gradient Norm: 0.8242\n",
      "Epoch 252: avg loss training: 2.7412,... Gradient Norm: 0.8865\n",
      "Epoch 253: avg loss training: 2.7410,... Gradient Norm: 0.3903\n",
      "Epoch 254: avg loss training: 2.7410,... Gradient Norm: 0.2757\n",
      "Epoch 255: avg loss training: 2.7409,... Gradient Norm: 0.6029\n",
      "Epoch 256: avg loss training: 2.7409,... Gradient Norm: 0.7792\n",
      "Epoch 257: avg loss training: 2.7408,... Gradient Norm: 0.7436\n",
      "Epoch 258: avg loss training: 2.7407,... Gradient Norm: 0.5152\n",
      "Epoch 259: avg loss training: 2.7406,... Gradient Norm: 0.2187\n",
      "Epoch 260: avg loss training: 2.7405,... Gradient Norm: 0.1875\n",
      "Epoch 261: avg loss training: 2.7405,... Gradient Norm: 0.2792\n",
      "Epoch 262: avg loss training: 2.7405,... Gradient Norm: 0.3434\n",
      "Epoch 263: avg loss training: 2.7404,... Gradient Norm: 0.1304\n",
      "Epoch 264: avg loss training: 2.7404,... Gradient Norm: 0.4866\n",
      "Epoch 265: avg loss training: 2.7403,... Gradient Norm: 0.9082\n",
      "Epoch 266: avg loss training: 2.7403,... Gradient Norm: 1.0347\n",
      "Epoch 267: avg loss training: 2.7403,... Gradient Norm: 0.6907\n",
      "Epoch 268: avg loss training: 2.7401,... Gradient Norm: 0.3575\n",
      "Epoch 269: avg loss training: 2.7402,... Gradient Norm: 0.8945\n",
      "Epoch 270: avg loss training: 2.7401,... Gradient Norm: 0.7444\n",
      "Epoch 271: avg loss training: 2.7399,... Gradient Norm: 0.3513\n",
      "Epoch 272: avg loss training: 2.7399,... Gradient Norm: 0.6120\n",
      "Epoch 273: avg loss training: 2.7400,... Gradient Norm: 1.0402\n",
      "Epoch 274: avg loss training: 2.7398,... Gradient Norm: 0.8075\n",
      "Epoch 275: avg loss training: 2.7397,... Gradient Norm: 0.2799\n",
      "Epoch 276: avg loss training: 2.7397,... Gradient Norm: 0.2308\n",
      "Epoch 277: avg loss training: 2.7396,... Gradient Norm: 0.3901\n",
      "Epoch 278: avg loss training: 2.7395,... Gradient Norm: 0.1668\n",
      "Epoch 279: avg loss training: 2.7395,... Gradient Norm: 0.2466\n",
      "Epoch 280: avg loss training: 2.7394,... Gradient Norm: 0.3396\n",
      "Epoch 281: avg loss training: 2.7394,... Gradient Norm: 0.2394\n",
      "Epoch 282: avg loss training: 2.7394,... Gradient Norm: 0.2734\n",
      "Epoch 283: avg loss training: 2.7393,... Gradient Norm: 0.1907\n",
      "Epoch 284: avg loss training: 2.7393,... Gradient Norm: 0.4567\n",
      "Epoch 285: avg loss training: 2.7392,... Gradient Norm: 0.3174\n",
      "Epoch 286: avg loss training: 2.7392,... Gradient Norm: 0.2734\n",
      "Epoch 287: avg loss training: 2.7392,... Gradient Norm: 0.6403\n",
      "Epoch 288: avg loss training: 2.7391,... Gradient Norm: 0.6211\n",
      "Epoch 289: avg loss training: 2.7390,... Gradient Norm: 0.1955\n",
      "Epoch 290: avg loss training: 2.7390,... Gradient Norm: 0.4798\n",
      "Epoch 291: avg loss training: 2.7389,... Gradient Norm: 0.5009\n",
      "Epoch 292: avg loss training: 2.7389,... Gradient Norm: 0.2230\n",
      "Epoch 293: avg loss training: 2.7388,... Gradient Norm: 0.2545\n",
      "Epoch 294: avg loss training: 2.7388,... Gradient Norm: 0.4019\n",
      "Epoch 295: avg loss training: 2.7388,... Gradient Norm: 0.4184\n",
      "Epoch 296: avg loss training: 2.7387,... Gradient Norm: 0.1850\n",
      "Epoch 297: avg loss training: 2.7387,... Gradient Norm: 0.4010\n",
      "Epoch 298: avg loss training: 2.7387,... Gradient Norm: 0.4428\n",
      "Epoch 299: avg loss training: 2.7386,... Gradient Norm: 0.3091\n",
      "Epoch 300: avg loss training: 2.7386,... Gradient Norm: 0.4024\n",
      "Epoch 301: avg loss training: 2.7385,... Gradient Norm: 0.4155\n",
      "Epoch 302: avg loss training: 2.7385,... Gradient Norm: 0.1699\n",
      "Epoch 303: avg loss training: 2.7384,... Gradient Norm: 0.2308\n",
      "Epoch 304: avg loss training: 2.7384,... Gradient Norm: 0.3434\n",
      "Epoch 305: avg loss training: 2.7383,... Gradient Norm: 0.4241\n",
      "Epoch 306: avg loss training: 2.7383,... Gradient Norm: 0.3529\n",
      "Epoch 307: avg loss training: 2.7382,... Gradient Norm: 0.1902\n",
      "Epoch 308: avg loss training: 2.7382,... Gradient Norm: 0.4979\n",
      "Epoch 309: avg loss training: 2.7382,... Gradient Norm: 0.8442\n",
      "Epoch 310: avg loss training: 2.7382,... Gradient Norm: 0.7420\n",
      "Epoch 311: avg loss training: 2.7381,... Gradient Norm: 0.3284\n",
      "Epoch 312: avg loss training: 2.7381,... Gradient Norm: 0.2659\n",
      "Epoch 313: avg loss training: 2.7381,... Gradient Norm: 0.4883\n",
      "Epoch 314: avg loss training: 2.7380,... Gradient Norm: 0.2882\n",
      "Epoch 315: avg loss training: 2.7380,... Gradient Norm: 0.3034\n",
      "Epoch 316: avg loss training: 2.7379,... Gradient Norm: 0.5125\n",
      "Epoch 317: avg loss training: 2.7379,... Gradient Norm: 0.3976\n",
      "Epoch 318: avg loss training: 2.7379,... Gradient Norm: 0.2174\n",
      "Epoch 319: avg loss training: 2.7378,... Gradient Norm: 0.4192\n",
      "Epoch 320: avg loss training: 2.7378,... Gradient Norm: 0.4515\n",
      "Epoch 321: avg loss training: 2.7378,... Gradient Norm: 0.1261\n",
      "Epoch 322: avg loss training: 2.7378,... Gradient Norm: 0.6474\n",
      "Epoch 323: avg loss training: 2.7377,... Gradient Norm: 0.5842\n",
      "Epoch 324: avg loss training: 2.7377,... Gradient Norm: 0.3568\n",
      "Epoch 325: avg loss training: 2.7377,... Gradient Norm: 0.9689\n",
      "Epoch 326: avg loss training: 2.7377,... Gradient Norm: 0.6049\n",
      "Epoch 327: avg loss training: 2.7376,... Gradient Norm: 0.3531\n",
      "Epoch 328: avg loss training: 2.7376,... Gradient Norm: 0.5118\n",
      "Epoch 329: avg loss training: 2.7376,... Gradient Norm: 0.3673\n",
      "Epoch 330: avg loss training: 2.7376,... Gradient Norm: 0.6128\n",
      "Epoch 331: avg loss training: 2.7375,... Gradient Norm: 0.3497\n",
      "Epoch 332: avg loss training: 2.7375,... Gradient Norm: 0.9271\n",
      "Epoch 333: avg loss training: 2.7374,... Gradient Norm: 0.3117\n",
      "Epoch 334: avg loss training: 2.7374,... Gradient Norm: 0.5657\n",
      "Epoch 335: avg loss training: 2.7374,... Gradient Norm: 0.3586\n",
      "Epoch 336: avg loss training: 2.7374,... Gradient Norm: 0.6589\n",
      "Epoch 337: avg loss training: 2.7374,... Gradient Norm: 0.8706\n",
      "Epoch 338: avg loss training: 2.7373,... Gradient Norm: 0.1931\n",
      "Epoch 339: avg loss training: 2.7373,... Gradient Norm: 0.9853\n",
      "Epoch 340: avg loss training: 2.7373,... Gradient Norm: 0.4683\n",
      "Epoch 341: avg loss training: 2.7373,... Gradient Norm: 0.6400\n",
      "Epoch 342: avg loss training: 2.7372,... Gradient Norm: 0.2781\n",
      "Epoch 343: avg loss training: 2.7372,... Gradient Norm: 0.4357\n",
      "Epoch 344: avg loss training: 2.7372,... Gradient Norm: 0.1505\n",
      "Epoch 345: avg loss training: 2.7371,... Gradient Norm: 0.4624\n",
      "Epoch 346: avg loss training: 2.7371,... Gradient Norm: 0.2625\n",
      "Epoch 347: avg loss training: 2.7371,... Gradient Norm: 0.5495\n",
      "Epoch 348: avg loss training: 2.7371,... Gradient Norm: 0.3120\n",
      "Epoch 349: avg loss training: 2.7370,... Gradient Norm: 0.3888\n",
      "Epoch 350: avg loss training: 2.7370,... Gradient Norm: 0.3190\n",
      "Epoch 351: avg loss training: 2.7370,... Gradient Norm: 0.2954\n",
      "Epoch 352: avg loss training: 2.7370,... Gradient Norm: 0.2786\n",
      "Epoch 353: avg loss training: 2.7370,... Gradient Norm: 0.2001\n",
      "Epoch 354: avg loss training: 2.7369,... Gradient Norm: 0.4721\n",
      "Epoch 355: avg loss training: 2.7369,... Gradient Norm: 0.1634\n",
      "Epoch 356: avg loss training: 2.7369,... Gradient Norm: 0.2619\n",
      "Epoch 357: avg loss training: 2.7369,... Gradient Norm: 0.3007\n",
      "Epoch 358: avg loss training: 2.7368,... Gradient Norm: 0.1175\n",
      "Epoch 359: avg loss training: 2.7368,... Gradient Norm: 0.1527\n",
      "Epoch 360: avg loss training: 2.7368,... Gradient Norm: 0.0962\n",
      "Epoch 361: avg loss training: 2.7368,... Gradient Norm: 0.3190\n",
      "Epoch 362: avg loss training: 2.7367,... Gradient Norm: 0.3370\n",
      "Epoch 363: avg loss training: 2.7367,... Gradient Norm: 0.1835\n",
      "Epoch 364: avg loss training: 2.7367,... Gradient Norm: 0.2637\n",
      "Epoch 365: avg loss training: 2.7367,... Gradient Norm: 0.1139\n",
      "Epoch 366: avg loss training: 2.7367,... Gradient Norm: 0.2394\n",
      "Epoch 367: avg loss training: 2.7367,... Gradient Norm: 0.1844\n",
      "Epoch 368: avg loss training: 2.7366,... Gradient Norm: 0.0848\n",
      "Epoch 369: avg loss training: 2.7366,... Gradient Norm: 0.2665\n",
      "Epoch 370: avg loss training: 2.7366,... Gradient Norm: 0.2097\n",
      "Epoch 371: avg loss training: 2.7366,... Gradient Norm: 0.2732\n",
      "Epoch 372: avg loss training: 2.7366,... Gradient Norm: 0.1521\n",
      "Epoch 373: avg loss training: 2.7365,... Gradient Norm: 0.4993\n",
      "Epoch 374: avg loss training: 2.7365,... Gradient Norm: 0.1952\n",
      "Epoch 375: avg loss training: 2.7365,... Gradient Norm: 0.2841\n",
      "Epoch 376: avg loss training: 2.7365,... Gradient Norm: 0.2197\n",
      "Epoch 377: avg loss training: 2.7365,... Gradient Norm: 0.3643\n",
      "Epoch 378: avg loss training: 2.7365,... Gradient Norm: 0.3266\n",
      "Epoch 379: avg loss training: 2.7365,... Gradient Norm: 0.3629\n",
      "Epoch 380: avg loss training: 2.7364,... Gradient Norm: 0.2768\n",
      "Epoch 381: avg loss training: 2.7364,... Gradient Norm: 0.2311\n",
      "Epoch 382: avg loss training: 2.7364,... Gradient Norm: 0.3592\n",
      "Epoch 383: avg loss training: 2.7364,... Gradient Norm: 0.2437\n",
      "Epoch 384: avg loss training: 2.7364,... Gradient Norm: 0.2973\n",
      "Epoch 385: avg loss training: 2.7364,... Gradient Norm: 0.2235\n",
      "Epoch 386: avg loss training: 2.7363,... Gradient Norm: 0.3798\n",
      "Epoch 387: avg loss training: 2.7363,... Gradient Norm: 0.2114\n",
      "Epoch 388: avg loss training: 2.7363,... Gradient Norm: 0.2289\n",
      "Epoch 389: avg loss training: 2.7363,... Gradient Norm: 0.2417\n",
      "Epoch 390: avg loss training: 2.7363,... Gradient Norm: 0.1433\n",
      "Epoch 391: avg loss training: 2.7363,... Gradient Norm: 0.0751\n",
      "Epoch 392: avg loss training: 2.7362,... Gradient Norm: 0.1325\n",
      "Epoch 393: avg loss training: 2.7362,... Gradient Norm: 0.1953\n",
      "Epoch 394: avg loss training: 2.7362,... Gradient Norm: 0.1469\n",
      "Epoch 395: avg loss training: 2.7362,... Gradient Norm: 0.1026\n",
      "Epoch 396: avg loss training: 2.7362,... Gradient Norm: 0.2482\n",
      "Epoch 397: avg loss training: 2.7362,... Gradient Norm: 0.2204\n",
      "Epoch 398: avg loss training: 2.7362,... Gradient Norm: 0.1122\n",
      "Epoch 399: avg loss training: 2.7361,... Gradient Norm: 0.1553\n",
      "Epoch 400: avg loss training: 2.7361,... Gradient Norm: 0.1998\n",
      "Epoch 401: avg loss training: 2.7361,... Gradient Norm: 0.2838\n",
      "Epoch 402: avg loss training: 2.7361,... Gradient Norm: 0.0770\n",
      "Epoch 403: avg loss training: 2.7361,... Gradient Norm: 0.4656\n",
      "Epoch 404: avg loss training: 2.7361,... Gradient Norm: 0.2346\n",
      "Epoch 405: avg loss training: 2.7361,... Gradient Norm: 0.2742\n",
      "Epoch 406: avg loss training: 2.7361,... Gradient Norm: 0.1734\n",
      "Epoch 407: avg loss training: 2.7360,... Gradient Norm: 0.2434\n",
      "Epoch 408: avg loss training: 2.7360,... Gradient Norm: 0.2872\n",
      "Epoch 409: avg loss training: 2.7360,... Gradient Norm: 0.2848\n",
      "Epoch 410: avg loss training: 2.7360,... Gradient Norm: 0.2661\n",
      "Epoch 411: avg loss training: 2.7360,... Gradient Norm: 0.3035\n",
      "Epoch 412: avg loss training: 2.7360,... Gradient Norm: 0.1590\n",
      "Epoch 413: avg loss training: 2.7360,... Gradient Norm: 0.1473\n",
      "Epoch 414: avg loss training: 2.7360,... Gradient Norm: 0.1965\n",
      "Epoch 415: avg loss training: 2.7359,... Gradient Norm: 0.1157\n",
      "Epoch 416: avg loss training: 2.7359,... Gradient Norm: 0.2205\n",
      "Epoch 417: avg loss training: 2.7359,... Gradient Norm: 0.1638\n",
      "Epoch 418: avg loss training: 2.7359,... Gradient Norm: 0.1771\n",
      "Epoch 419: avg loss training: 2.7359,... Gradient Norm: 0.1114\n",
      "Epoch 420: avg loss training: 2.7359,... Gradient Norm: 0.2160\n",
      "Epoch 421: avg loss training: 2.7359,... Gradient Norm: 0.1324\n",
      "Epoch 422: avg loss training: 2.7359,... Gradient Norm: 0.1499\n",
      "Epoch 423: avg loss training: 2.7358,... Gradient Norm: 0.2236\n",
      "Epoch 424: avg loss training: 2.7358,... Gradient Norm: 0.2584\n",
      "Epoch 425: avg loss training: 2.7358,... Gradient Norm: 0.2742\n",
      "Epoch 426: avg loss training: 2.7358,... Gradient Norm: 0.1068\n",
      "Epoch 427: avg loss training: 2.7358,... Gradient Norm: 0.3663\n",
      "Epoch 428: avg loss training: 2.7358,... Gradient Norm: 0.1793\n",
      "Epoch 429: avg loss training: 2.7358,... Gradient Norm: 0.1946\n",
      "Epoch 430: avg loss training: 2.7358,... Gradient Norm: 0.1057\n",
      "Epoch 431: avg loss training: 2.7358,... Gradient Norm: 0.3018\n",
      "Epoch 432: avg loss training: 2.7358,... Gradient Norm: 0.1063\n",
      "Epoch 433: avg loss training: 2.7357,... Gradient Norm: 0.2499\n",
      "Epoch 434: avg loss training: 2.7357,... Gradient Norm: 0.2205\n",
      "Epoch 435: avg loss training: 2.7357,... Gradient Norm: 0.2478\n",
      "Epoch 436: avg loss training: 2.7357,... Gradient Norm: 0.2672\n",
      "Epoch 437: avg loss training: 2.7357,... Gradient Norm: 0.1431\n",
      "Epoch 438: avg loss training: 2.7357,... Gradient Norm: 0.4079\n",
      "Epoch 439: avg loss training: 2.7357,... Gradient Norm: 0.1682\n",
      "Epoch 440: avg loss training: 2.7357,... Gradient Norm: 0.2730\n",
      "Epoch 441: avg loss training: 2.7357,... Gradient Norm: 0.2367\n",
      "Epoch 442: avg loss training: 2.7357,... Gradient Norm: 0.1504\n",
      "Epoch 443: avg loss training: 2.7356,... Gradient Norm: 0.1046\n",
      "Epoch 444: avg loss training: 2.7356,... Gradient Norm: 0.1016\n",
      "Epoch 445: avg loss training: 2.7356,... Gradient Norm: 0.2265\n",
      "Epoch 446: avg loss training: 2.7356,... Gradient Norm: 0.1511\n",
      "Epoch 447: avg loss training: 2.7356,... Gradient Norm: 0.0994\n",
      "Epoch 448: avg loss training: 2.7356,... Gradient Norm: 0.2760\n",
      "Epoch 449: avg loss training: 2.7356,... Gradient Norm: 0.1438\n",
      "Epoch 450: avg loss training: 2.7356,... Gradient Norm: 0.1684\n",
      "Epoch 451: avg loss training: 2.7356,... Gradient Norm: 0.2943\n",
      "Epoch 452: avg loss training: 2.7356,... Gradient Norm: 0.1713\n",
      "Epoch 453: avg loss training: 2.7356,... Gradient Norm: 0.2724\n",
      "Epoch 454: avg loss training: 2.7355,... Gradient Norm: 0.0871\n",
      "Epoch 455: avg loss training: 2.7355,... Gradient Norm: 0.3160\n",
      "Epoch 456: avg loss training: 2.7355,... Gradient Norm: 0.1321\n",
      "Epoch 457: avg loss training: 2.7355,... Gradient Norm: 0.3029\n",
      "Epoch 458: avg loss training: 2.7355,... Gradient Norm: 0.2889\n",
      "Epoch 459: avg loss training: 2.7355,... Gradient Norm: 0.2055\n",
      "Epoch 460: avg loss training: 2.7355,... Gradient Norm: 0.4121\n",
      "Epoch 461: avg loss training: 2.7355,... Gradient Norm: 0.1166\n",
      "Epoch 462: avg loss training: 2.7355,... Gradient Norm: 0.3355\n",
      "Epoch 463: avg loss training: 2.7355,... Gradient Norm: 0.3022\n",
      "Epoch 464: avg loss training: 2.7355,... Gradient Norm: 0.2624\n",
      "Epoch 465: avg loss training: 2.7355,... Gradient Norm: 0.2288\n",
      "Epoch 466: avg loss training: 2.7355,... Gradient Norm: 0.2309\n",
      "Epoch 467: avg loss training: 2.7354,... Gradient Norm: 0.2317\n",
      "Epoch 468: avg loss training: 2.7354,... Gradient Norm: 0.3145\n",
      "Epoch 469: avg loss training: 2.7354,... Gradient Norm: 0.1101\n",
      "Epoch 470: avg loss training: 2.7354,... Gradient Norm: 0.3257\n",
      "Epoch 471: avg loss training: 2.7354,... Gradient Norm: 0.1481\n",
      "Epoch 472: avg loss training: 2.7354,... Gradient Norm: 0.2054\n",
      "Epoch 473: avg loss training: 2.7354,... Gradient Norm: 0.1972\n",
      "Epoch 474: avg loss training: 2.7354,... Gradient Norm: 0.1927\n",
      "Epoch 475: avg loss training: 2.7354,... Gradient Norm: 0.1524\n",
      "Epoch 476: avg loss training: 2.7354,... Gradient Norm: 0.3977\n",
      "Epoch 477: avg loss training: 2.7354,... Gradient Norm: 0.1303\n",
      "Epoch 478: avg loss training: 2.7354,... Gradient Norm: 0.2821\n",
      "Epoch 479: avg loss training: 2.7354,... Gradient Norm: 0.1410\n",
      "Epoch 480: avg loss training: 2.7353,... Gradient Norm: 0.2348\n",
      "Epoch 481: avg loss training: 2.7353,... Gradient Norm: 0.2299\n",
      "Epoch 482: avg loss training: 2.7353,... Gradient Norm: 0.2698\n",
      "Epoch 483: avg loss training: 2.7353,... Gradient Norm: 0.2622\n",
      "Epoch 484: avg loss training: 2.7353,... Gradient Norm: 0.1051\n",
      "Epoch 485: avg loss training: 2.7353,... Gradient Norm: 0.3010\n",
      "Epoch 486: avg loss training: 2.7353,... Gradient Norm: 0.1380\n",
      "Epoch 487: avg loss training: 2.7353,... Gradient Norm: 0.1265\n",
      "Epoch 488: avg loss training: 2.7353,... Gradient Norm: 0.2055\n",
      "Epoch 489: avg loss training: 2.7353,... Gradient Norm: 0.1490\n",
      "Epoch 490: avg loss training: 2.7353,... Gradient Norm: 0.2956\n",
      "Epoch 491: avg loss training: 2.7353,... Gradient Norm: 0.2432\n",
      "Epoch 492: avg loss training: 2.7353,... Gradient Norm: 0.2301\n",
      "Epoch 493: avg loss training: 2.7353,... Gradient Norm: 0.2430\n",
      "Epoch 494: avg loss training: 2.7353,... Gradient Norm: 0.0889\n",
      "Epoch 495: avg loss training: 2.7353,... Gradient Norm: 0.3484\n",
      "Epoch 496: avg loss training: 2.7352,... Gradient Norm: 0.1009\n",
      "Epoch 497: avg loss training: 2.7352,... Gradient Norm: 0.2851\n",
      "Epoch 498: avg loss training: 2.7352,... Gradient Norm: 0.2542\n",
      "Epoch 499: avg loss training: 2.7352,... Gradient Norm: 0.1705\n",
      "Epoch 500: avg loss training: 2.7352,... Gradient Norm: 0.2118\n",
      "Epoch 501: avg loss training: 2.7352,... Gradient Norm: 0.1923\n",
      "Epoch 502: avg loss training: 2.7352,... Gradient Norm: 0.1061\n",
      "Epoch 503: avg loss training: 2.7352,... Gradient Norm: 0.2475\n",
      "Epoch 504: avg loss training: 2.7352,... Gradient Norm: 0.1578\n",
      "Epoch 505: avg loss training: 2.7352,... Gradient Norm: 0.1199\n",
      "Epoch 506: avg loss training: 2.7352,... Gradient Norm: 0.1101\n",
      "Epoch 507: avg loss training: 2.7352,... Gradient Norm: 0.2709\n",
      "Epoch 508: avg loss training: 2.7352,... Gradient Norm: 0.1002\n",
      "Epoch 509: avg loss training: 2.7352,... Gradient Norm: 0.2744\n",
      "Epoch 510: avg loss training: 2.7352,... Gradient Norm: 0.2238\n",
      "Epoch 511: avg loss training: 2.7352,... Gradient Norm: 0.1192\n",
      "Epoch 512: avg loss training: 2.7352,... Gradient Norm: 0.2031\n",
      "Epoch 513: avg loss training: 2.7351,... Gradient Norm: 0.1598\n",
      "Epoch 514: avg loss training: 2.7351,... Gradient Norm: 0.2144\n",
      "Epoch 515: avg loss training: 2.7351,... Gradient Norm: 0.1702\n",
      "Epoch 516: avg loss training: 2.7351,... Gradient Norm: 0.1883\n",
      "Epoch 517: avg loss training: 2.7351,... Gradient Norm: 0.1746\n",
      "Epoch 518: avg loss training: 2.7351,... Gradient Norm: 0.2013\n",
      "Epoch 519: avg loss training: 2.7351,... Gradient Norm: 0.2155\n",
      "Epoch 520: avg loss training: 2.7351,... Gradient Norm: 0.1803\n",
      "Epoch 521: avg loss training: 2.7351,... Gradient Norm: 0.0986\n",
      "Epoch 522: avg loss training: 2.7351,... Gradient Norm: 0.1430\n",
      "Epoch 523: avg loss training: 2.7351,... Gradient Norm: 0.1387\n",
      "Epoch 524: avg loss training: 2.7351,... Gradient Norm: 0.2356\n",
      "Epoch 525: avg loss training: 2.7351,... Gradient Norm: 0.2131\n",
      "Epoch 526: avg loss training: 2.7351,... Gradient Norm: 0.2380\n",
      "Epoch 527: avg loss training: 2.7351,... Gradient Norm: 0.2550\n",
      "Epoch 528: avg loss training: 2.7351,... Gradient Norm: 0.2903\n",
      "Epoch 529: avg loss training: 2.7351,... Gradient Norm: 0.2368\n",
      "Epoch 530: avg loss training: 2.7351,... Gradient Norm: 0.2247\n",
      "Epoch 531: avg loss training: 2.7351,... Gradient Norm: 0.1006\n",
      "Epoch 532: avg loss training: 2.7351,... Gradient Norm: 0.2710\n",
      "Epoch 533: avg loss training: 2.7351,... Gradient Norm: 0.2021\n",
      "Epoch 534: avg loss training: 2.7350,... Gradient Norm: 0.1358\n",
      "Epoch 535: avg loss training: 2.7350,... Gradient Norm: 0.2174\n",
      "Epoch 536: avg loss training: 2.7350,... Gradient Norm: 0.2062\n",
      "Epoch 537: avg loss training: 2.7350,... Gradient Norm: 0.1048\n",
      "Epoch 538: avg loss training: 2.7350,... Gradient Norm: 0.2123\n",
      "Epoch 539: avg loss training: 2.7350,... Gradient Norm: 0.2018\n",
      "Epoch 540: avg loss training: 2.7350,... Gradient Norm: 0.1356\n",
      "Epoch 541: avg loss training: 2.7350,... Gradient Norm: 0.1255\n",
      "Epoch 542: avg loss training: 2.7350,... Gradient Norm: 0.2511\n",
      "Epoch 543: avg loss training: 2.7350,... Gradient Norm: 0.1120\n",
      "Epoch 544: avg loss training: 2.7350,... Gradient Norm: 0.2707\n",
      "Epoch 545: avg loss training: 2.7350,... Gradient Norm: 0.1856\n",
      "Epoch 546: avg loss training: 2.7350,... Gradient Norm: 0.2623\n",
      "Epoch 547: avg loss training: 2.7350,... Gradient Norm: 0.2760\n",
      "Epoch 548: avg loss training: 2.7350,... Gradient Norm: 0.1902\n",
      "Epoch 549: avg loss training: 2.7350,... Gradient Norm: 0.2220\n",
      "Epoch 550: avg loss training: 2.7350,... Gradient Norm: 0.1529\n",
      "Epoch 551: avg loss training: 2.7350,... Gradient Norm: 0.1017\n",
      "Epoch 552: avg loss training: 2.7350,... Gradient Norm: 0.3185\n",
      "Epoch 553: avg loss training: 2.7350,... Gradient Norm: 0.2550\n",
      "Epoch 554: avg loss training: 2.7350,... Gradient Norm: 0.1954\n",
      "Epoch 555: avg loss training: 2.7350,... Gradient Norm: 0.2160\n",
      "Epoch 556: avg loss training: 2.7350,... Gradient Norm: 0.1635\n",
      "Epoch 557: avg loss training: 2.7350,... Gradient Norm: 0.2568\n",
      "Epoch 558: avg loss training: 2.7349,... Gradient Norm: 0.1440\n",
      "Epoch 559: avg loss training: 2.7349,... Gradient Norm: 0.1922\n",
      "Epoch 560: avg loss training: 2.7349,... Gradient Norm: 0.2892\n",
      "Epoch 561: avg loss training: 2.7349,... Gradient Norm: 0.1958\n",
      "Epoch 562: avg loss training: 2.7349,... Gradient Norm: 0.2169\n",
      "Epoch 563: avg loss training: 2.7349,... Gradient Norm: 0.2782\n",
      "Epoch 564: avg loss training: 2.7349,... Gradient Norm: 0.1143\n",
      "Epoch 565: avg loss training: 2.7349,... Gradient Norm: 0.1846\n",
      "Epoch 566: avg loss training: 2.7349,... Gradient Norm: 0.2273\n",
      "Epoch 567: avg loss training: 2.7349,... Gradient Norm: 0.1912\n",
      "Epoch 568: avg loss training: 2.7349,... Gradient Norm: 0.0927\n",
      "Epoch 569: avg loss training: 2.7349,... Gradient Norm: 0.1404\n",
      "Epoch 570: avg loss training: 2.7349,... Gradient Norm: 0.2226\n",
      "Epoch 571: avg loss training: 2.7349,... Gradient Norm: 0.1457\n",
      "Epoch 572: avg loss training: 2.7349,... Gradient Norm: 0.1371\n",
      "Epoch 573: avg loss training: 2.7349,... Gradient Norm: 0.1645\n",
      "Epoch 574: avg loss training: 2.7349,... Gradient Norm: 0.0535\n",
      "Epoch 575: avg loss training: 2.7349,... Gradient Norm: 0.1613\n",
      "Epoch 576: avg loss training: 2.7349,... Gradient Norm: 0.1274\n",
      "Epoch 577: avg loss training: 2.7349,... Gradient Norm: 0.2469\n",
      "Epoch 578: avg loss training: 2.7349,... Gradient Norm: 0.1572\n",
      "Epoch 579: avg loss training: 2.7349,... Gradient Norm: 0.2620\n",
      "Epoch 580: avg loss training: 2.7349,... Gradient Norm: 0.1762\n",
      "Epoch 581: avg loss training: 2.7349,... Gradient Norm: 0.3257\n",
      "Epoch 582: avg loss training: 2.7349,... Gradient Norm: 0.1541\n",
      "Epoch 583: avg loss training: 2.7349,... Gradient Norm: 0.2800\n",
      "Epoch 584: avg loss training: 2.7349,... Gradient Norm: 0.1705\n",
      "Epoch 585: avg loss training: 2.7349,... Gradient Norm: 0.1068\n",
      "Epoch 586: avg loss training: 2.7349,... Gradient Norm: 0.1242\n",
      "Epoch 587: avg loss training: 2.7349,... Gradient Norm: 0.1217\n",
      "Epoch 588: avg loss training: 2.7349,... Gradient Norm: 0.0756\n",
      "Epoch 589: avg loss training: 2.7348,... Gradient Norm: 0.1733\n",
      "Epoch 590: avg loss training: 2.7348,... Gradient Norm: 0.2310\n",
      "Epoch 591: avg loss training: 2.7348,... Gradient Norm: 0.2248\n",
      "Epoch 592: avg loss training: 2.7348,... Gradient Norm: 0.1093\n",
      "Epoch 593: avg loss training: 2.7348,... Gradient Norm: 0.0834\n",
      "Epoch 594: avg loss training: 2.7348,... Gradient Norm: 0.2374\n",
      "Epoch 595: avg loss training: 2.7348,... Gradient Norm: 0.2318\n",
      "Epoch 596: avg loss training: 2.7348,... Gradient Norm: 0.1538\n",
      "Epoch 597: avg loss training: 2.7348,... Gradient Norm: 0.3134\n",
      "Epoch 598: avg loss training: 2.7348,... Gradient Norm: 0.0792\n",
      "Epoch 599: avg loss training: 2.7348,... Gradient Norm: 0.1797\n",
      "Epoch 600: avg loss training: 2.7348,... Gradient Norm: 0.1351\n",
      "Epoch 601: avg loss training: 2.7348,... Gradient Norm: 0.2175\n",
      "Epoch 602: avg loss training: 2.7348,... Gradient Norm: 0.1871\n",
      "Epoch 603: avg loss training: 2.7348,... Gradient Norm: 0.2175\n",
      "Epoch 604: avg loss training: 2.7348,... Gradient Norm: 0.2464\n",
      "Epoch 605: avg loss training: 2.7348,... Gradient Norm: 0.2222\n",
      "Epoch 606: avg loss training: 2.7348,... Gradient Norm: 0.2738\n",
      "Epoch 607: avg loss training: 2.7348,... Gradient Norm: 0.1475\n",
      "Epoch 608: avg loss training: 2.7348,... Gradient Norm: 0.2365\n",
      "Epoch 609: avg loss training: 2.7348,... Gradient Norm: 0.3384\n",
      "Epoch 610: avg loss training: 2.7348,... Gradient Norm: 0.1375\n",
      "Epoch 611: avg loss training: 2.7348,... Gradient Norm: 0.2556\n",
      "Epoch 612: avg loss training: 2.7348,... Gradient Norm: 0.1845\n",
      "Epoch 613: avg loss training: 2.7348,... Gradient Norm: 0.1225\n",
      "Epoch 614: avg loss training: 2.7348,... Gradient Norm: 0.2432\n",
      "Epoch 615: avg loss training: 2.7348,... Gradient Norm: 0.1766\n",
      "Epoch 616: avg loss training: 2.7348,... Gradient Norm: 0.1974\n",
      "Epoch 617: avg loss training: 2.7348,... Gradient Norm: 0.1009\n",
      "Epoch 618: avg loss training: 2.7348,... Gradient Norm: 0.2481\n",
      "Epoch 619: avg loss training: 2.7348,... Gradient Norm: 0.2502\n",
      "Epoch 620: avg loss training: 2.7348,... Gradient Norm: 0.1521\n",
      "Epoch 621: avg loss training: 2.7348,... Gradient Norm: 0.2190\n",
      "Epoch 622: avg loss training: 2.7348,... Gradient Norm: 0.1701\n",
      "Epoch 623: avg loss training: 2.7348,... Gradient Norm: 0.2162\n",
      "Epoch 624: avg loss training: 2.7348,... Gradient Norm: 0.1610\n",
      "Epoch 625: avg loss training: 2.7348,... Gradient Norm: 0.2028\n",
      "Epoch 626: avg loss training: 2.7348,... Gradient Norm: 0.1123\n",
      "Epoch 627: avg loss training: 2.7348,... Gradient Norm: 0.2225\n",
      "Epoch 628: avg loss training: 2.7348,... Gradient Norm: 0.1151\n",
      "Epoch 629: avg loss training: 2.7348,... Gradient Norm: 0.1271\n",
      "Epoch 630: avg loss training: 2.7348,... Gradient Norm: 0.0781\n",
      "Epoch 631: avg loss training: 2.7348,... Gradient Norm: 0.1896\n",
      "Epoch 632: avg loss training: 2.7348,... Gradient Norm: 0.1401\n",
      "Epoch 633: avg loss training: 2.7348,... Gradient Norm: 0.2172\n",
      "Epoch 634: avg loss training: 2.7347,... Gradient Norm: 0.1532\n",
      "Epoch 635: avg loss training: 2.7347,... Gradient Norm: 0.3028\n",
      "Epoch 636: avg loss training: 2.7347,... Gradient Norm: 0.1017\n",
      "Epoch 637: avg loss training: 2.7347,... Gradient Norm: 0.2402\n",
      "Epoch 638: avg loss training: 2.7347,... Gradient Norm: 0.1248\n",
      "Epoch 639: avg loss training: 2.7347,... Gradient Norm: 0.2549\n",
      "Epoch 640: avg loss training: 2.7347,... Gradient Norm: 0.1337\n",
      "Epoch 641: avg loss training: 2.7347,... Gradient Norm: 0.1706\n",
      "Epoch 642: avg loss training: 2.7347,... Gradient Norm: 0.1457\n",
      "Epoch 643: avg loss training: 2.7347,... Gradient Norm: 0.0558\n",
      "Epoch 644: avg loss training: 2.7347,... Gradient Norm: 0.2244\n",
      "Epoch 645: avg loss training: 2.7347,... Gradient Norm: 0.1186\n",
      "Epoch 646: avg loss training: 2.7347,... Gradient Norm: 0.1479\n",
      "Epoch 647: avg loss training: 2.7347,... Gradient Norm: 0.1227\n",
      "Epoch 648: avg loss training: 2.7347,... Gradient Norm: 0.2521\n",
      "Epoch 649: avg loss training: 2.7347,... Gradient Norm: 0.1472\n",
      "Epoch 650: avg loss training: 2.7347,... Gradient Norm: 0.1180\n",
      "Epoch 651: avg loss training: 2.7347,... Gradient Norm: 0.2174\n",
      "Epoch 652: avg loss training: 2.7347,... Gradient Norm: 0.1402\n",
      "Epoch 653: avg loss training: 2.7347,... Gradient Norm: 0.1383\n",
      "Epoch 654: avg loss training: 2.7347,... Gradient Norm: 0.2285\n",
      "Epoch 655: avg loss training: 2.7347,... Gradient Norm: 0.1820\n",
      "Epoch 656: avg loss training: 2.7347,... Gradient Norm: 0.1151\n",
      "Epoch 657: avg loss training: 2.7347,... Gradient Norm: 0.1058\n",
      "Epoch 658: avg loss training: 2.7347,... Gradient Norm: 0.2305\n",
      "Epoch 659: avg loss training: 2.7347,... Gradient Norm: 0.1456\n",
      "Epoch 660: avg loss training: 2.7347,... Gradient Norm: 0.1499\n",
      "Epoch 661: avg loss training: 2.7347,... Gradient Norm: 0.2925\n",
      "Epoch 662: avg loss training: 2.7347,... Gradient Norm: 0.0730\n",
      "Epoch 663: avg loss training: 2.7347,... Gradient Norm: 0.1948\n",
      "Epoch 664: avg loss training: 2.7347,... Gradient Norm: 0.2099\n",
      "Epoch 665: avg loss training: 2.7347,... Gradient Norm: 0.0896\n",
      "Epoch 666: avg loss training: 2.7347,... Gradient Norm: 0.1041\n",
      "Epoch 667: avg loss training: 2.7347,... Gradient Norm: 0.2930\n",
      "Epoch 668: avg loss training: 2.7347,... Gradient Norm: 0.2351\n",
      "Epoch 669: avg loss training: 2.7347,... Gradient Norm: 0.1713\n",
      "Epoch 670: avg loss training: 2.7347,... Gradient Norm: 0.2620\n",
      "Epoch 671: avg loss training: 2.7347,... Gradient Norm: 0.1170\n",
      "Epoch 672: avg loss training: 2.7347,... Gradient Norm: 0.1648\n",
      "Epoch 673: avg loss training: 2.7347,... Gradient Norm: 0.2145\n",
      "Epoch 674: avg loss training: 2.7347,... Gradient Norm: 0.1116\n",
      "Epoch 675: avg loss training: 2.7347,... Gradient Norm: 0.2009\n",
      "Epoch 676: avg loss training: 2.7347,... Gradient Norm: 0.1685\n",
      "Epoch 677: avg loss training: 2.7347,... Gradient Norm: 0.1670\n",
      "Epoch 678: avg loss training: 2.7347,... Gradient Norm: 0.1580\n",
      "Epoch 679: avg loss training: 2.7347,... Gradient Norm: 0.2622\n",
      "Epoch 680: avg loss training: 2.7347,... Gradient Norm: 0.1563\n",
      "Epoch 681: avg loss training: 2.7347,... Gradient Norm: 0.1797\n",
      "Epoch 682: avg loss training: 2.7347,... Gradient Norm: 0.2617\n",
      "Epoch 683: avg loss training: 2.7347,... Gradient Norm: 0.0859\n",
      "Epoch 684: avg loss training: 2.7347,... Gradient Norm: 0.1872\n",
      "Epoch 685: avg loss training: 2.7347,... Gradient Norm: 0.1380\n",
      "Epoch 686: avg loss training: 2.7347,... Gradient Norm: 0.1384\n",
      "Epoch 687: avg loss training: 2.7347,... Gradient Norm: 0.1939\n",
      "Epoch 688: avg loss training: 2.7347,... Gradient Norm: 0.2384\n",
      "Epoch 689: avg loss training: 2.7347,... Gradient Norm: 0.1166\n",
      "Epoch 690: avg loss training: 2.7347,... Gradient Norm: 0.3431\n",
      "Epoch 691: avg loss training: 2.7347,... Gradient Norm: 0.0825\n",
      "Epoch 692: avg loss training: 2.7347,... Gradient Norm: 0.2280\n",
      "Epoch 693: avg loss training: 2.7347,... Gradient Norm: 0.2007\n",
      "Epoch 694: avg loss training: 2.7347,... Gradient Norm: 0.1065\n",
      "Epoch 695: avg loss training: 2.7347,... Gradient Norm: 0.0589\n",
      "Epoch 696: avg loss training: 2.7347,... Gradient Norm: 0.2211\n",
      "Epoch 697: avg loss training: 2.7347,... Gradient Norm: 0.2095\n",
      "Epoch 698: avg loss training: 2.7347,... Gradient Norm: 0.2005\n",
      "Epoch 699: avg loss training: 2.7347,... Gradient Norm: 0.1614\n",
      "Epoch 700: avg loss training: 2.7347,... Gradient Norm: 0.0796\n",
      "Epoch 701: avg loss training: 2.7347,... Gradient Norm: 0.3089\n",
      "Epoch 702: avg loss training: 2.7347,... Gradient Norm: 0.2723\n",
      "Epoch 703: avg loss training: 2.7347,... Gradient Norm: 0.1941\n",
      "Epoch 704: avg loss training: 2.7347,... Gradient Norm: 0.2272\n",
      "Epoch 705: avg loss training: 2.7346,... Gradient Norm: 0.2079\n",
      "Epoch 706: avg loss training: 2.7346,... Gradient Norm: 0.2019\n",
      "Epoch 707: avg loss training: 2.7346,... Gradient Norm: 0.1921\n",
      "Epoch 708: avg loss training: 2.7346,... Gradient Norm: 0.1434\n",
      "Epoch 709: avg loss training: 2.7346,... Gradient Norm: 0.0966\n",
      "Epoch 710: avg loss training: 2.7346,... Gradient Norm: 0.2641\n",
      "Epoch 711: avg loss training: 2.7346,... Gradient Norm: 0.1983\n",
      "Epoch 712: avg loss training: 2.7346,... Gradient Norm: 0.2157\n",
      "Epoch 713: avg loss training: 2.7346,... Gradient Norm: 0.2319\n",
      "Epoch 714: avg loss training: 2.7346,... Gradient Norm: 0.0937\n",
      "Epoch 715: avg loss training: 2.7346,... Gradient Norm: 0.2045\n",
      "Epoch 716: avg loss training: 2.7346,... Gradient Norm: 0.1989\n",
      "Epoch 717: avg loss training: 2.7346,... Gradient Norm: 0.1043\n",
      "Epoch 718: avg loss training: 2.7346,... Gradient Norm: 0.2580\n",
      "Epoch 719: avg loss training: 2.7346,... Gradient Norm: 0.0780\n",
      "Epoch 720: avg loss training: 2.7346,... Gradient Norm: 0.3243\n",
      "Epoch 721: avg loss training: 2.7346,... Gradient Norm: 0.1714\n",
      "Epoch 722: avg loss training: 2.7346,... Gradient Norm: 0.2775\n",
      "Epoch 723: avg loss training: 2.7346,... Gradient Norm: 0.2320\n",
      "Epoch 724: avg loss training: 2.7346,... Gradient Norm: 0.1117\n",
      "Epoch 725: avg loss training: 2.7346,... Gradient Norm: 0.3434\n",
      "Epoch 726: avg loss training: 2.7346,... Gradient Norm: 0.1926\n",
      "Epoch 727: avg loss training: 2.7346,... Gradient Norm: 0.1570\n",
      "Epoch 728: avg loss training: 2.7346,... Gradient Norm: 0.1765\n",
      "Epoch 729: avg loss training: 2.7346,... Gradient Norm: 0.2028\n",
      "Epoch 730: avg loss training: 2.7346,... Gradient Norm: 0.1451\n",
      "Epoch 731: avg loss training: 2.7346,... Gradient Norm: 0.0950\n",
      "Epoch 732: avg loss training: 2.7346,... Gradient Norm: 0.1277\n",
      "Epoch 733: avg loss training: 2.7346,... Gradient Norm: 0.2870\n",
      "Epoch 734: avg loss training: 2.7346,... Gradient Norm: 0.2085\n",
      "Epoch 735: avg loss training: 2.7346,... Gradient Norm: 0.2486\n",
      "Epoch 736: avg loss training: 2.7346,... Gradient Norm: 0.2347\n",
      "Epoch 737: avg loss training: 2.7346,... Gradient Norm: 0.2346\n",
      "Epoch 738: avg loss training: 2.7346,... Gradient Norm: 0.1965\n",
      "Epoch 739: avg loss training: 2.7346,... Gradient Norm: 0.1759\n",
      "Epoch 740: avg loss training: 2.7346,... Gradient Norm: 0.1612\n",
      "Epoch 741: avg loss training: 2.7346,... Gradient Norm: 0.1466\n",
      "Epoch 742: avg loss training: 2.7346,... Gradient Norm: 0.2753\n",
      "Epoch 743: avg loss training: 2.7346,... Gradient Norm: 0.1616\n",
      "Epoch 744: avg loss training: 2.7346,... Gradient Norm: 0.2498\n",
      "Epoch 745: avg loss training: 2.7346,... Gradient Norm: 0.1559\n",
      "Epoch 746: avg loss training: 2.7346,... Gradient Norm: 0.2499\n",
      "Epoch 747: avg loss training: 2.7346,... Gradient Norm: 0.2886\n",
      "Epoch 748: avg loss training: 2.7346,... Gradient Norm: 0.1716\n",
      "Epoch 749: avg loss training: 2.7346,... Gradient Norm: 0.1862\n",
      "Epoch 750: avg loss training: 2.7346,... Gradient Norm: 0.1110\n",
      "Epoch 751: avg loss training: 2.7346,... Gradient Norm: 0.1592\n",
      "Epoch 752: avg loss training: 2.7346,... Gradient Norm: 0.2440\n",
      "Epoch 753: avg loss training: 2.7346,... Gradient Norm: 0.1017\n",
      "Epoch 754: avg loss training: 2.7346,... Gradient Norm: 0.1561\n",
      "Epoch 755: avg loss training: 2.7346,... Gradient Norm: 0.1234\n",
      "Epoch 756: avg loss training: 2.7346,... Gradient Norm: 0.2871\n",
      "Epoch 757: avg loss training: 2.7346,... Gradient Norm: 0.0712\n",
      "Epoch 758: avg loss training: 2.7346,... Gradient Norm: 0.2102\n",
      "Epoch 759: avg loss training: 2.7346,... Gradient Norm: 0.1355\n",
      "Epoch 760: avg loss training: 2.7346,... Gradient Norm: 0.2220\n",
      "Epoch 761: avg loss training: 2.7346,... Gradient Norm: 0.1616\n",
      "Epoch 762: avg loss training: 2.7346,... Gradient Norm: 0.1512\n",
      "Epoch 763: avg loss training: 2.7346,... Gradient Norm: 0.1996\n",
      "Epoch 764: avg loss training: 2.7346,... Gradient Norm: 0.2534\n",
      "Epoch 765: avg loss training: 2.7346,... Gradient Norm: 0.1982\n",
      "Epoch 766: avg loss training: 2.7346,... Gradient Norm: 0.2095\n",
      "Epoch 767: avg loss training: 2.7346,... Gradient Norm: 0.0869\n",
      "Epoch 768: avg loss training: 2.7346,... Gradient Norm: 0.2078\n",
      "Epoch 769: avg loss training: 2.7346,... Gradient Norm: 0.2431\n",
      "Epoch 770: avg loss training: 2.7346,... Gradient Norm: 0.1457\n",
      "Epoch 771: avg loss training: 2.7346,... Gradient Norm: 0.2085\n",
      "Epoch 772: avg loss training: 2.7346,... Gradient Norm: 0.0993\n",
      "Epoch 773: avg loss training: 2.7346,... Gradient Norm: 0.1960\n",
      "Epoch 774: avg loss training: 2.7346,... Gradient Norm: 0.2159\n",
      "Epoch 775: avg loss training: 2.7346,... Gradient Norm: 0.1336\n",
      "Epoch 776: avg loss training: 2.7346,... Gradient Norm: 0.2260\n",
      "Epoch 777: avg loss training: 2.7346,... Gradient Norm: 0.1041\n",
      "Epoch 778: avg loss training: 2.7346,... Gradient Norm: 0.2901\n",
      "Epoch 779: avg loss training: 2.7346,... Gradient Norm: 0.2454\n",
      "Epoch 780: avg loss training: 2.7346,... Gradient Norm: 0.2223\n",
      "Epoch 781: avg loss training: 2.7346,... Gradient Norm: 0.2255\n",
      "Epoch 782: avg loss training: 2.7346,... Gradient Norm: 0.0879\n",
      "Epoch 783: avg loss training: 2.7346,... Gradient Norm: 0.1100\n",
      "Epoch 784: avg loss training: 2.7346,... Gradient Norm: 0.2539\n",
      "Epoch 785: avg loss training: 2.7346,... Gradient Norm: 0.2056\n",
      "Epoch 786: avg loss training: 2.7346,... Gradient Norm: 0.1261\n",
      "Epoch 787: avg loss training: 2.7346,... Gradient Norm: 0.2173\n",
      "Epoch 788: avg loss training: 2.7346,... Gradient Norm: 0.2058\n",
      "Epoch 789: avg loss training: 2.7346,... Gradient Norm: 0.0973\n",
      "Epoch 790: avg loss training: 2.7346,... Gradient Norm: 0.1456\n",
      "Epoch 791: avg loss training: 2.7346,... Gradient Norm: 0.2423\n",
      "Epoch 792: avg loss training: 2.7346,... Gradient Norm: 0.1276\n",
      "Epoch 793: avg loss training: 2.7346,... Gradient Norm: 0.2110\n",
      "Epoch 794: avg loss training: 2.7346,... Gradient Norm: 0.2043\n",
      "Epoch 795: avg loss training: 2.7346,... Gradient Norm: 0.2818\n",
      "Epoch 796: avg loss training: 2.7346,... Gradient Norm: 0.1349\n",
      "Epoch 797: avg loss training: 2.7346,... Gradient Norm: 0.2508\n",
      "Epoch 798: avg loss training: 2.7346,... Gradient Norm: 0.1868\n",
      "Epoch 799: avg loss training: 2.7346,... Gradient Norm: 0.1353\n",
      "Epoch 800: avg loss training: 2.7346,... Gradient Norm: 0.2600\n",
      "Epoch 801: avg loss training: 2.7346,... Gradient Norm: 0.1048\n",
      "Epoch 802: avg loss training: 2.7346,... Gradient Norm: 0.2001\n",
      "Epoch 803: avg loss training: 2.7346,... Gradient Norm: 0.1216\n",
      "Epoch 804: avg loss training: 2.7346,... Gradient Norm: 0.2186\n",
      "Epoch 805: avg loss training: 2.7346,... Gradient Norm: 0.2046\n",
      "Epoch 806: avg loss training: 2.7346,... Gradient Norm: 0.1761\n",
      "Epoch 807: avg loss training: 2.7346,... Gradient Norm: 0.1811\n",
      "Epoch 808: avg loss training: 2.7346,... Gradient Norm: 0.0936\n",
      "Epoch 809: avg loss training: 2.7346,... Gradient Norm: 0.1577\n",
      "Epoch 810: avg loss training: 2.7346,... Gradient Norm: 0.2446\n",
      "Epoch 811: avg loss training: 2.7346,... Gradient Norm: 0.2096\n",
      "Epoch 812: avg loss training: 2.7346,... Gradient Norm: 0.2521\n",
      "Epoch 813: avg loss training: 2.7346,... Gradient Norm: 0.2524\n",
      "Epoch 814: avg loss training: 2.7346,... Gradient Norm: 0.0672\n",
      "Epoch 815: avg loss training: 2.7346,... Gradient Norm: 0.2741\n",
      "Epoch 816: avg loss training: 2.7346,... Gradient Norm: 0.2637\n",
      "Epoch 817: avg loss training: 2.7346,... Gradient Norm: 0.1585\n",
      "Epoch 818: avg loss training: 2.7346,... Gradient Norm: 0.1935\n",
      "Epoch 819: avg loss training: 2.7346,... Gradient Norm: 0.1353\n",
      "Epoch 820: avg loss training: 2.7346,... Gradient Norm: 0.0699\n",
      "Epoch 821: avg loss training: 2.7346,... Gradient Norm: 0.2437\n",
      "Epoch 822: avg loss training: 2.7346,... Gradient Norm: 0.2067\n",
      "Epoch 823: avg loss training: 2.7346,... Gradient Norm: 0.1233\n",
      "Epoch 824: avg loss training: 2.7346,... Gradient Norm: 0.1416\n",
      "Epoch 825: avg loss training: 2.7346,... Gradient Norm: 0.1428\n",
      "Epoch 826: avg loss training: 2.7346,... Gradient Norm: 0.1310\n",
      "Epoch 827: avg loss training: 2.7346,... Gradient Norm: 0.1847\n",
      "Epoch 828: avg loss training: 2.7346,... Gradient Norm: 0.1558\n",
      "Epoch 829: avg loss training: 2.7346,... Gradient Norm: 0.0830\n",
      "Epoch 830: avg loss training: 2.7346,... Gradient Norm: 0.1158\n",
      "Epoch 831: avg loss training: 2.7346,... Gradient Norm: 0.1779\n",
      "Epoch 832: avg loss training: 2.7346,... Gradient Norm: 0.0738\n",
      "Epoch 833: avg loss training: 2.7346,... Gradient Norm: 0.3008\n",
      "Epoch 834: avg loss training: 2.7346,... Gradient Norm: 0.0912\n",
      "Epoch 835: avg loss training: 2.7346,... Gradient Norm: 0.1357\n",
      "Epoch 836: avg loss training: 2.7346,... Gradient Norm: 0.0777\n",
      "Epoch 837: avg loss training: 2.7346,... Gradient Norm: 0.2724\n",
      "Epoch 838: avg loss training: 2.7346,... Gradient Norm: 0.1809\n",
      "Epoch 839: avg loss training: 2.7346,... Gradient Norm: 0.1907\n",
      "Epoch 840: avg loss training: 2.7346,... Gradient Norm: 0.1951\n",
      "Epoch 841: avg loss training: 2.7346,... Gradient Norm: 0.1760\n",
      "Epoch 842: avg loss training: 2.7346,... Gradient Norm: 0.2173\n",
      "Epoch 843: avg loss training: 2.7346,... Gradient Norm: 0.1986\n",
      "Epoch 844: avg loss training: 2.7346,... Gradient Norm: 0.1723\n",
      "Epoch 845: avg loss training: 2.7346,... Gradient Norm: 0.2633\n",
      "Epoch 846: avg loss training: 2.7346,... Gradient Norm: 0.1538\n",
      "Epoch 847: avg loss training: 2.7346,... Gradient Norm: 0.1629\n",
      "Epoch 848: avg loss training: 2.7346,... Gradient Norm: 0.1619\n",
      "Epoch 849: avg loss training: 2.7346,... Gradient Norm: 0.1586\n",
      "Epoch 850: avg loss training: 2.7346,... Gradient Norm: 0.0905\n",
      "Epoch 851: avg loss training: 2.7346,... Gradient Norm: 0.1138\n",
      "Epoch 852: avg loss training: 2.7346,... Gradient Norm: 0.0839\n",
      "Epoch 853: avg loss training: 2.7346,... Gradient Norm: 0.1383\n",
      "Epoch 854: avg loss training: 2.7346,... Gradient Norm: 0.1279\n",
      "Epoch 855: avg loss training: 2.7346,... Gradient Norm: 0.1569\n",
      "Epoch 856: avg loss training: 2.7346,... Gradient Norm: 0.2293\n",
      "Epoch 857: avg loss training: 2.7346,... Gradient Norm: 0.1737\n",
      "Epoch 858: avg loss training: 2.7346,... Gradient Norm: 0.1909\n",
      "Epoch 859: avg loss training: 2.7346,... Gradient Norm: 0.1993\n",
      "Epoch 860: avg loss training: 2.7346,... Gradient Norm: 0.1579\n",
      "Epoch 861: avg loss training: 2.7346,... Gradient Norm: 0.2581\n",
      "Epoch 862: avg loss training: 2.7346,... Gradient Norm: 0.1465\n",
      "Epoch 863: avg loss training: 2.7346,... Gradient Norm: 0.1743\n",
      "Epoch 864: avg loss training: 2.7346,... Gradient Norm: 0.1615\n",
      "Epoch 865: avg loss training: 2.7346,... Gradient Norm: 0.2570\n",
      "Epoch 866: avg loss training: 2.7346,... Gradient Norm: 0.1109\n",
      "Epoch 867: avg loss training: 2.7346,... Gradient Norm: 0.1123\n",
      "Epoch 868: avg loss training: 2.7346,... Gradient Norm: 0.2008\n",
      "Epoch 869: avg loss training: 2.7346,... Gradient Norm: 0.1640\n",
      "Epoch 870: avg loss training: 2.7346,... Gradient Norm: 0.1031\n",
      "Epoch 871: avg loss training: 2.7346,... Gradient Norm: 0.1415\n",
      "Epoch 872: avg loss training: 2.7346,... Gradient Norm: 0.2031\n",
      "Epoch 873: avg loss training: 2.7346,... Gradient Norm: 0.0897\n",
      "Epoch 874: avg loss training: 2.7346,... Gradient Norm: 0.3191\n",
      "Epoch 875: avg loss training: 2.7346,... Gradient Norm: 0.2182\n",
      "Epoch 876: avg loss training: 2.7346,... Gradient Norm: 0.1838\n",
      "Epoch 877: avg loss training: 2.7346,... Gradient Norm: 0.1047\n",
      "Epoch 878: avg loss training: 2.7346,... Gradient Norm: 0.2897\n",
      "Epoch 879: avg loss training: 2.7346,... Gradient Norm: 0.1124\n",
      "Epoch 880: avg loss training: 2.7346,... Gradient Norm: 0.1755\n",
      "Epoch 881: avg loss training: 2.7346,... Gradient Norm: 0.1728\n",
      "Epoch 882: avg loss training: 2.7346,... Gradient Norm: 0.2098\n",
      "Epoch 883: avg loss training: 2.7346,... Gradient Norm: 0.2121\n",
      "Epoch 884: avg loss training: 2.7346,... Gradient Norm: 0.2078\n",
      "Epoch 885: avg loss training: 2.7346,... Gradient Norm: 0.2022\n",
      "Epoch 886: avg loss training: 2.7346,... Gradient Norm: 0.1141\n",
      "Epoch 887: avg loss training: 2.7346,... Gradient Norm: 0.2437\n",
      "Epoch 888: avg loss training: 2.7346,... Gradient Norm: 0.1329\n",
      "Epoch 889: avg loss training: 2.7346,... Gradient Norm: 0.1475\n",
      "Epoch 890: avg loss training: 2.7346,... Gradient Norm: 0.1614\n",
      "Epoch 891: avg loss training: 2.7346,... Gradient Norm: 0.1978\n",
      "Epoch 892: avg loss training: 2.7346,... Gradient Norm: 0.0861\n",
      "Epoch 893: avg loss training: 2.7346,... Gradient Norm: 0.1132\n",
      "Epoch 894: avg loss training: 2.7346,... Gradient Norm: 0.1414\n",
      "Epoch 895: avg loss training: 2.7346,... Gradient Norm: 0.1210\n",
      "Epoch 896: avg loss training: 2.7346,... Gradient Norm: 0.1405\n",
      "Epoch 897: avg loss training: 2.7346,... Gradient Norm: 0.1907\n",
      "Epoch 898: avg loss training: 2.7346,... Gradient Norm: 0.2521\n",
      "Epoch 899: avg loss training: 2.7346,... Gradient Norm: 0.1312\n",
      "Epoch 900: avg loss training: 2.7346,... Gradient Norm: 0.1760\n",
      "Epoch 901: avg loss training: 2.7346,... Gradient Norm: 0.2395\n",
      "Epoch 902: avg loss training: 2.7346,... Gradient Norm: 0.1141\n",
      "Epoch 903: avg loss training: 2.7346,... Gradient Norm: 0.2007\n",
      "Epoch 904: avg loss training: 2.7346,... Gradient Norm: 0.1861\n",
      "Epoch 905: avg loss training: 2.7346,... Gradient Norm: 0.1989\n",
      "Epoch 906: avg loss training: 2.7346,... Gradient Norm: 0.2412\n",
      "Epoch 907: avg loss training: 2.7346,... Gradient Norm: 0.1089\n",
      "Epoch 908: avg loss training: 2.7346,... Gradient Norm: 0.1822\n",
      "Epoch 909: avg loss training: 2.7346,... Gradient Norm: 0.1584\n",
      "Epoch 910: avg loss training: 2.7346,... Gradient Norm: 0.0778\n",
      "Epoch 911: avg loss training: 2.7346,... Gradient Norm: 0.1201\n",
      "Epoch 912: avg loss training: 2.7346,... Gradient Norm: 0.2214\n",
      "Epoch 913: avg loss training: 2.7346,... Gradient Norm: 0.2180\n",
      "Epoch 914: avg loss training: 2.7346,... Gradient Norm: 0.1053\n",
      "Epoch 915: avg loss training: 2.7346,... Gradient Norm: 0.1782\n",
      "Epoch 916: avg loss training: 2.7346,... Gradient Norm: 0.1512\n",
      "Epoch 917: avg loss training: 2.7346,... Gradient Norm: 0.1064\n",
      "Epoch 918: avg loss training: 2.7346,... Gradient Norm: 0.1974\n",
      "Epoch 919: avg loss training: 2.7346,... Gradient Norm: 0.1252\n",
      "Epoch 920: avg loss training: 2.7346,... Gradient Norm: 0.0956\n",
      "Epoch 921: avg loss training: 2.7346,... Gradient Norm: 0.2129\n",
      "Epoch 922: avg loss training: 2.7345,... Gradient Norm: 0.1116\n",
      "Epoch 923: avg loss training: 2.7345,... Gradient Norm: 0.0717\n",
      "Epoch 924: avg loss training: 2.7345,... Gradient Norm: 0.0658\n",
      "Epoch 925: avg loss training: 2.7345,... Gradient Norm: 0.2659\n",
      "Epoch 926: avg loss training: 2.7345,... Gradient Norm: 0.2474\n",
      "Epoch 927: avg loss training: 2.7345,... Gradient Norm: 0.2396\n",
      "Epoch 928: avg loss training: 2.7345,... Gradient Norm: 0.2185\n",
      "Epoch 929: avg loss training: 2.7345,... Gradient Norm: 0.1878\n",
      "Epoch 930: avg loss training: 2.7345,... Gradient Norm: 0.1514\n",
      "Epoch 931: avg loss training: 2.7345,... Gradient Norm: 0.1877\n",
      "Epoch 932: avg loss training: 2.7345,... Gradient Norm: 0.0725\n",
      "Epoch 933: avg loss training: 2.7345,... Gradient Norm: 0.1950\n",
      "Epoch 934: avg loss training: 2.7345,... Gradient Norm: 0.1208\n",
      "Epoch 935: avg loss training: 2.7345,... Gradient Norm: 0.1544\n",
      "Epoch 936: avg loss training: 2.7345,... Gradient Norm: 0.1275\n",
      "Epoch 937: avg loss training: 2.7345,... Gradient Norm: 0.2191\n",
      "Epoch 938: avg loss training: 2.7345,... Gradient Norm: 0.1998\n",
      "Epoch 939: avg loss training: 2.7345,... Gradient Norm: 0.1261\n",
      "Epoch 940: avg loss training: 2.7345,... Gradient Norm: 0.1819\n",
      "Epoch 941: avg loss training: 2.7345,... Gradient Norm: 0.2095\n",
      "Epoch 942: avg loss training: 2.7345,... Gradient Norm: 0.1239\n",
      "Epoch 943: avg loss training: 2.7345,... Gradient Norm: 0.1037\n",
      "Epoch 944: avg loss training: 2.7345,... Gradient Norm: 0.2986\n",
      "Epoch 945: avg loss training: 2.7345,... Gradient Norm: 0.1709\n",
      "Epoch 946: avg loss training: 2.7345,... Gradient Norm: 0.1405\n",
      "Epoch 947: avg loss training: 2.7345,... Gradient Norm: 0.2100\n",
      "Epoch 948: avg loss training: 2.7345,... Gradient Norm: 0.0772\n",
      "Epoch 949: avg loss training: 2.7345,... Gradient Norm: 0.1227\n",
      "Epoch 950: avg loss training: 2.7345,... Gradient Norm: 0.1623\n",
      "Epoch 951: avg loss training: 2.7345,... Gradient Norm: 0.2243\n",
      "Epoch 952: avg loss training: 2.7345,... Gradient Norm: 0.2459\n",
      "Epoch 953: avg loss training: 2.7345,... Gradient Norm: 0.1276\n",
      "Epoch 954: avg loss training: 2.7345,... Gradient Norm: 0.2057\n",
      "Epoch 955: avg loss training: 2.7345,... Gradient Norm: 0.0855\n",
      "Epoch 956: avg loss training: 2.7345,... Gradient Norm: 0.0981\n",
      "Epoch 957: avg loss training: 2.7345,... Gradient Norm: 0.1741\n",
      "Epoch 958: avg loss training: 2.7345,... Gradient Norm: 0.2214\n",
      "Epoch 959: avg loss training: 2.7345,... Gradient Norm: 0.2271\n",
      "Epoch 960: avg loss training: 2.7345,... Gradient Norm: 0.1471\n",
      "Epoch 961: avg loss training: 2.7345,... Gradient Norm: 0.1303\n",
      "Epoch 962: avg loss training: 2.7345,... Gradient Norm: 0.1522\n",
      "Epoch 963: avg loss training: 2.7345,... Gradient Norm: 0.0936\n",
      "Epoch 964: avg loss training: 2.7345,... Gradient Norm: 0.0939\n",
      "Epoch 965: avg loss training: 2.7345,... Gradient Norm: 0.1832\n",
      "Epoch 966: avg loss training: 2.7345,... Gradient Norm: 0.2185\n",
      "Epoch 967: avg loss training: 2.7345,... Gradient Norm: 0.0981\n",
      "Epoch 968: avg loss training: 2.7345,... Gradient Norm: 0.2423\n",
      "Epoch 969: avg loss training: 2.7345,... Gradient Norm: 0.1385\n",
      "Epoch 970: avg loss training: 2.7345,... Gradient Norm: 0.1151\n",
      "Epoch 971: avg loss training: 2.7345,... Gradient Norm: 0.0719\n",
      "Epoch 972: avg loss training: 2.7345,... Gradient Norm: 0.2055\n",
      "Epoch 973: avg loss training: 2.7345,... Gradient Norm: 0.0941\n",
      "Epoch 974: avg loss training: 2.7345,... Gradient Norm: 0.1507\n",
      "Epoch 975: avg loss training: 2.7345,... Gradient Norm: 0.1845\n",
      "Epoch 976: avg loss training: 2.7345,... Gradient Norm: 0.1703\n",
      "Epoch 977: avg loss training: 2.7345,... Gradient Norm: 0.1052\n",
      "Epoch 978: avg loss training: 2.7345,... Gradient Norm: 0.2698\n",
      "Epoch 979: avg loss training: 2.7345,... Gradient Norm: 0.1553\n",
      "Epoch 980: avg loss training: 2.7345,... Gradient Norm: 0.1942\n",
      "Epoch 981: avg loss training: 2.7345,... Gradient Norm: 0.1964\n",
      "Epoch 982: avg loss training: 2.7345,... Gradient Norm: 0.1897\n",
      "Epoch 983: avg loss training: 2.7345,... Gradient Norm: 0.2221\n",
      "Epoch 984: avg loss training: 2.7345,... Gradient Norm: 0.1049\n",
      "Epoch 985: avg loss training: 2.7345,... Gradient Norm: 0.1211\n",
      "Epoch 986: avg loss training: 2.7345,... Gradient Norm: 0.1698\n",
      "Epoch 987: avg loss training: 2.7345,... Gradient Norm: 0.2177\n",
      "Epoch 988: avg loss training: 2.7345,... Gradient Norm: 0.2178\n",
      "Epoch 989: avg loss training: 2.7345,... Gradient Norm: 0.2199\n",
      "Epoch 990: avg loss training: 2.7345,... Gradient Norm: 0.1943\n",
      "Epoch 991: avg loss training: 2.7345,... Gradient Norm: 0.2342\n",
      "Epoch 992: avg loss training: 2.7345,... Gradient Norm: 0.1624\n",
      "Epoch 993: avg loss training: 2.7345,... Gradient Norm: 0.1587\n",
      "Epoch 994: avg loss training: 2.7345,... Gradient Norm: 0.1127\n",
      "Epoch 995: avg loss training: 2.7345,... Gradient Norm: 0.1238\n",
      "Epoch 996: avg loss training: 2.7345,... Gradient Norm: 0.2180\n",
      "Epoch 997: avg loss training: 2.7345,... Gradient Norm: 0.1186\n",
      "Epoch 998: avg loss training: 2.7345,... Gradient Norm: 0.2304\n",
      "Epoch 999: avg loss training: 2.7345,... Gradient Norm: 0.0895\n",
      "Epoch 1000: avg loss training: 2.7345,... Gradient Norm: 0.2492\n",
      "Epoch 1: avg loss training: 3.5560,... Gradient Norm: 7.6769\n",
      "Epoch 2: avg loss training: 116.1941,... Gradient Norm: 1553.2389\n",
      "Epoch 3: avg loss training: 19.8902,... Gradient Norm: 178.2217\n",
      "Epoch 4: avg loss training: 9.2324,... Gradient Norm: 48.6483\n",
      "Epoch 5: avg loss training: 13.4934,... Gradient Norm: 99.9458\n",
      "Epoch 6: avg loss training: 7.7393,... Gradient Norm: 41.0065\n",
      "Epoch 7: avg loss training: 9.4162,... Gradient Norm: 85.3719\n",
      "Epoch 8: avg loss training: 6.2235,... Gradient Norm: 45.9193\n",
      "Epoch 9: avg loss training: 3.9792,... Gradient Norm: 5.7923\n",
      "Epoch 10: avg loss training: 4.6218,... Gradient Norm: 30.2393\n",
      "Epoch 11: avg loss training: 4.3769,... Gradient Norm: 26.5236\n",
      "Epoch 12: avg loss training: 4.2734,... Gradient Norm: 18.7816\n",
      "Epoch 13: avg loss training: 4.6997,... Gradient Norm: 24.3244\n",
      "Epoch 14: avg loss training: 4.3658,... Gradient Norm: 19.0068\n",
      "Epoch 15: avg loss training: 3.8803,... Gradient Norm: 7.6428\n",
      "Epoch 16: avg loss training: 3.7712,... Gradient Norm: 6.4225\n",
      "Epoch 17: avg loss training: 3.8754,... Gradient Norm: 12.1616\n",
      "Epoch 18: avg loss training: 3.7484,... Gradient Norm: 11.0372\n",
      "Epoch 19: avg loss training: 3.5227,... Gradient Norm: 6.7447\n",
      "Epoch 20: avg loss training: 3.4198,... Gradient Norm: 5.6765\n",
      "Epoch 21: avg loss training: 3.3887,... Gradient Norm: 5.9751\n",
      "Epoch 22: avg loss training: 3.3774,... Gradient Norm: 6.2791\n",
      "Epoch 23: avg loss training: 3.3934,... Gradient Norm: 8.7445\n",
      "Epoch 24: avg loss training: 3.3649,... Gradient Norm: 9.9045\n",
      "Epoch 25: avg loss training: 3.2508,... Gradient Norm: 7.4117\n",
      "Epoch 26: avg loss training: 3.1340,... Gradient Norm: 3.8911\n",
      "Epoch 27: avg loss training: 3.0791,... Gradient Norm: 3.2419\n",
      "Epoch 28: avg loss training: 3.0825,... Gradient Norm: 5.1832\n",
      "Epoch 29: avg loss training: 3.1056,... Gradient Norm: 6.2065\n",
      "Epoch 30: avg loss training: 3.0995,... Gradient Norm: 6.4018\n",
      "Epoch 31: avg loss training: 3.0558,... Gradient Norm: 4.2588\n",
      "Epoch 32: avg loss training: 3.0299,... Gradient Norm: 3.7147\n",
      "Epoch 33: avg loss training: 3.0145,... Gradient Norm: 2.6708\n",
      "Epoch 34: avg loss training: 3.0066,... Gradient Norm: 4.1881\n",
      "Epoch 35: avg loss training: 2.9887,... Gradient Norm: 2.8722\n",
      "Epoch 36: avg loss training: 2.9823,... Gradient Norm: 3.0083\n",
      "Epoch 37: avg loss training: 2.9818,... Gradient Norm: 2.1502\n",
      "Epoch 38: avg loss training: 2.9853,... Gradient Norm: 2.9278\n",
      "Epoch 39: avg loss training: 2.9776,... Gradient Norm: 2.1023\n",
      "Epoch 40: avg loss training: 2.9639,... Gradient Norm: 2.4933\n",
      "Epoch 41: avg loss training: 2.9453,... Gradient Norm: 1.9065\n",
      "Epoch 42: avg loss training: 2.9306,... Gradient Norm: 1.2399\n",
      "Epoch 43: avg loss training: 2.9271,... Gradient Norm: 2.0559\n",
      "Epoch 44: avg loss training: 2.9297,... Gradient Norm: 1.1423\n",
      "Epoch 45: avg loss training: 2.9356,... Gradient Norm: 2.5025\n",
      "Epoch 46: avg loss training: 2.9308,... Gradient Norm: 1.7264\n",
      "Epoch 47: avg loss training: 2.9216,... Gradient Norm: 1.9649\n",
      "Epoch 48: avg loss training: 2.9115,... Gradient Norm: 1.6965\n",
      "Epoch 49: avg loss training: 2.9053,... Gradient Norm: 0.9118\n",
      "Epoch 50: avg loss training: 2.9054,... Gradient Norm: 1.9151\n",
      "Epoch 51: avg loss training: 2.9053,... Gradient Norm: 0.8164\n",
      "Epoch 52: avg loss training: 2.9057,... Gradient Norm: 1.5865\n",
      "Epoch 53: avg loss training: 2.9027,... Gradient Norm: 1.3762\n",
      "Epoch 54: avg loss training: 2.8980,... Gradient Norm: 0.9531\n",
      "Epoch 55: avg loss training: 2.8946,... Gradient Norm: 1.4221\n",
      "Epoch 56: avg loss training: 2.8925,... Gradient Norm: 0.5900\n",
      "Epoch 57: avg loss training: 2.8927,... Gradient Norm: 1.2623\n",
      "Epoch 58: avg loss training: 2.8909,... Gradient Norm: 0.9071\n",
      "Epoch 59: avg loss training: 2.8879,... Gradient Norm: 0.7681\n",
      "Epoch 60: avg loss training: 2.8855,... Gradient Norm: 1.3379\n",
      "Epoch 61: avg loss training: 2.8835,... Gradient Norm: 0.6215\n",
      "Epoch 62: avg loss training: 2.8827,... Gradient Norm: 0.9623\n",
      "Epoch 63: avg loss training: 2.8811,... Gradient Norm: 1.1559\n",
      "Epoch 64: avg loss training: 2.8792,... Gradient Norm: 0.3917\n",
      "Epoch 65: avg loss training: 2.8780,... Gradient Norm: 0.7017\n",
      "Epoch 66: avg loss training: 2.8763,... Gradient Norm: 0.9839\n",
      "Epoch 67: avg loss training: 2.8746,... Gradient Norm: 0.3792\n",
      "Epoch 68: avg loss training: 2.8742,... Gradient Norm: 0.7461\n",
      "Epoch 69: avg loss training: 2.8734,... Gradient Norm: 0.8676\n",
      "Epoch 70: avg loss training: 2.8719,... Gradient Norm: 0.4505\n",
      "Epoch 71: avg loss training: 2.8709,... Gradient Norm: 0.4126\n",
      "Epoch 72: avg loss training: 2.8699,... Gradient Norm: 0.8659\n",
      "Epoch 73: avg loss training: 2.8684,... Gradient Norm: 0.7558\n",
      "Epoch 74: avg loss training: 2.8675,... Gradient Norm: 0.4064\n",
      "Epoch 75: avg loss training: 2.8669,... Gradient Norm: 0.2442\n",
      "Epoch 76: avg loss training: 2.8658,... Gradient Norm: 0.5274\n",
      "Epoch 77: avg loss training: 2.8652,... Gradient Norm: 0.7704\n",
      "Epoch 78: avg loss training: 2.8646,... Gradient Norm: 0.6911\n",
      "Epoch 79: avg loss training: 2.8637,... Gradient Norm: 0.5767\n",
      "Epoch 80: avg loss training: 2.8630,... Gradient Norm: 0.2197\n",
      "Epoch 81: avg loss training: 2.8626,... Gradient Norm: 0.3477\n",
      "Epoch 82: avg loss training: 2.8619,... Gradient Norm: 0.6670\n",
      "Epoch 83: avg loss training: 2.8612,... Gradient Norm: 0.7770\n",
      "Epoch 84: avg loss training: 2.8607,... Gradient Norm: 0.8919\n",
      "Epoch 85: avg loss training: 2.8601,... Gradient Norm: 0.7082\n",
      "Epoch 86: avg loss training: 2.8594,... Gradient Norm: 0.5266\n",
      "Epoch 87: avg loss training: 2.8588,... Gradient Norm: 0.2550\n",
      "Epoch 88: avg loss training: 2.8581,... Gradient Norm: 0.1372\n",
      "Epoch 89: avg loss training: 2.8576,... Gradient Norm: 0.4374\n",
      "Epoch 90: avg loss training: 2.8571,... Gradient Norm: 0.5973\n",
      "Epoch 91: avg loss training: 2.8566,... Gradient Norm: 0.7786\n",
      "Epoch 92: avg loss training: 2.8561,... Gradient Norm: 0.7917\n",
      "Epoch 93: avg loss training: 2.8556,... Gradient Norm: 0.7333\n",
      "Epoch 94: avg loss training: 2.8551,... Gradient Norm: 0.6515\n",
      "Epoch 95: avg loss training: 2.8546,... Gradient Norm: 0.5645\n",
      "Epoch 96: avg loss training: 2.8541,... Gradient Norm: 0.3607\n",
      "Epoch 97: avg loss training: 2.8537,... Gradient Norm: 0.2266\n",
      "Epoch 98: avg loss training: 2.8533,... Gradient Norm: 0.1546\n",
      "Epoch 99: avg loss training: 2.8528,... Gradient Norm: 0.1480\n",
      "Epoch 100: avg loss training: 2.8524,... Gradient Norm: 0.2527\n",
      "Epoch 101: avg loss training: 2.8521,... Gradient Norm: 0.4247\n",
      "Epoch 102: avg loss training: 2.8517,... Gradient Norm: 0.6530\n",
      "Epoch 103: avg loss training: 2.8515,... Gradient Norm: 0.8166\n",
      "Epoch 104: avg loss training: 2.8511,... Gradient Norm: 0.9279\n",
      "Epoch 105: avg loss training: 2.8508,... Gradient Norm: 0.9357\n",
      "Epoch 106: avg loss training: 2.8504,... Gradient Norm: 0.8740\n",
      "Epoch 107: avg loss training: 2.8500,... Gradient Norm: 0.7791\n",
      "Epoch 108: avg loss training: 2.8496,... Gradient Norm: 0.7780\n",
      "Epoch 109: avg loss training: 2.8493,... Gradient Norm: 0.8629\n",
      "Epoch 110: avg loss training: 2.8490,... Gradient Norm: 0.9706\n",
      "Epoch 111: avg loss training: 2.8487,... Gradient Norm: 1.1036\n",
      "Epoch 112: avg loss training: 2.8484,... Gradient Norm: 1.1890\n",
      "Epoch 113: avg loss training: 2.8481,... Gradient Norm: 1.2293\n",
      "Epoch 114: avg loss training: 2.8478,... Gradient Norm: 1.1889\n",
      "Epoch 115: avg loss training: 2.8475,... Gradient Norm: 1.0766\n",
      "Epoch 116: avg loss training: 2.8471,... Gradient Norm: 0.8388\n",
      "Epoch 117: avg loss training: 2.8467,... Gradient Norm: 0.5389\n",
      "Epoch 118: avg loss training: 2.8463,... Gradient Norm: 0.1402\n",
      "Epoch 119: avg loss training: 2.8461,... Gradient Norm: 0.3120\n",
      "Epoch 120: avg loss training: 2.8459,... Gradient Norm: 0.7120\n",
      "Epoch 121: avg loss training: 2.8457,... Gradient Norm: 0.9944\n",
      "Epoch 122: avg loss training: 2.8455,... Gradient Norm: 1.1009\n",
      "Epoch 123: avg loss training: 2.8453,... Gradient Norm: 1.0007\n",
      "Epoch 124: avg loss training: 2.8449,... Gradient Norm: 0.7924\n",
      "Epoch 125: avg loss training: 2.8446,... Gradient Norm: 0.3861\n",
      "Epoch 126: avg loss training: 2.8444,... Gradient Norm: 0.2295\n",
      "Epoch 127: avg loss training: 2.8442,... Gradient Norm: 0.6784\n",
      "Epoch 128: avg loss training: 2.8441,... Gradient Norm: 0.8653\n",
      "Epoch 129: avg loss training: 2.8438,... Gradient Norm: 0.6997\n",
      "Epoch 130: avg loss training: 2.8436,... Gradient Norm: 0.3400\n",
      "Epoch 131: avg loss training: 2.8433,... Gradient Norm: 0.1096\n",
      "Epoch 132: avg loss training: 2.8432,... Gradient Norm: 0.3946\n",
      "Epoch 133: avg loss training: 2.8430,... Gradient Norm: 0.6896\n",
      "Epoch 134: avg loss training: 2.8429,... Gradient Norm: 0.8488\n",
      "Epoch 135: avg loss training: 2.8427,... Gradient Norm: 0.8239\n",
      "Epoch 136: avg loss training: 2.8425,... Gradient Norm: 0.6061\n",
      "Epoch 137: avg loss training: 2.8423,... Gradient Norm: 0.2625\n",
      "Epoch 138: avg loss training: 2.8421,... Gradient Norm: 0.1449\n",
      "Epoch 139: avg loss training: 2.8420,... Gradient Norm: 0.3106\n",
      "Epoch 140: avg loss training: 2.8418,... Gradient Norm: 0.2598\n",
      "Epoch 141: avg loss training: 2.8416,... Gradient Norm: 0.1362\n",
      "Epoch 142: avg loss training: 2.8415,... Gradient Norm: 0.1497\n",
      "Epoch 143: avg loss training: 2.8413,... Gradient Norm: 0.1154\n",
      "Epoch 144: avg loss training: 2.8412,... Gradient Norm: 0.1750\n",
      "Epoch 145: avg loss training: 2.8411,... Gradient Norm: 0.2845\n",
      "Epoch 146: avg loss training: 2.8409,... Gradient Norm: 0.2609\n",
      "Epoch 147: avg loss training: 2.8408,... Gradient Norm: 0.1580\n",
      "Epoch 148: avg loss training: 2.8407,... Gradient Norm: 0.4162\n",
      "Epoch 149: avg loss training: 2.8406,... Gradient Norm: 0.7561\n",
      "Epoch 150: avg loss training: 2.8406,... Gradient Norm: 0.9437\n",
      "Epoch 151: avg loss training: 2.8405,... Gradient Norm: 0.9384\n",
      "Epoch 152: avg loss training: 2.8403,... Gradient Norm: 0.7405\n",
      "Epoch 153: avg loss training: 2.8401,... Gradient Norm: 0.2999\n",
      "Epoch 154: avg loss training: 2.8399,... Gradient Norm: 0.1362\n",
      "Epoch 155: avg loss training: 2.8399,... Gradient Norm: 0.3994\n",
      "Epoch 156: avg loss training: 2.8399,... Gradient Norm: 0.5795\n",
      "Epoch 157: avg loss training: 2.8397,... Gradient Norm: 0.6402\n",
      "Epoch 158: avg loss training: 2.8395,... Gradient Norm: 0.5011\n",
      "Epoch 159: avg loss training: 2.8395,... Gradient Norm: 0.2143\n",
      "Epoch 160: avg loss training: 2.8393,... Gradient Norm: 0.5185\n",
      "Epoch 161: avg loss training: 2.8393,... Gradient Norm: 0.7704\n",
      "Epoch 162: avg loss training: 2.8391,... Gradient Norm: 0.5044\n",
      "Epoch 163: avg loss training: 2.8389,... Gradient Norm: 0.1234\n",
      "Epoch 164: avg loss training: 2.8389,... Gradient Norm: 0.5341\n",
      "Epoch 165: avg loss training: 2.8388,... Gradient Norm: 0.5129\n",
      "Epoch 166: avg loss training: 2.8387,... Gradient Norm: 0.1880\n",
      "Epoch 167: avg loss training: 2.8387,... Gradient Norm: 0.6790\n",
      "Epoch 168: avg loss training: 2.8387,... Gradient Norm: 0.9256\n",
      "Epoch 169: avg loss training: 2.8384,... Gradient Norm: 0.5630\n",
      "Epoch 170: avg loss training: 2.8383,... Gradient Norm: 0.1153\n",
      "Epoch 171: avg loss training: 2.8383,... Gradient Norm: 0.5711\n",
      "Epoch 172: avg loss training: 2.8382,... Gradient Norm: 0.5819\n",
      "Epoch 173: avg loss training: 2.8382,... Gradient Norm: 0.3012\n",
      "Epoch 174: avg loss training: 2.8380,... Gradient Norm: 0.2103\n",
      "Epoch 175: avg loss training: 2.8381,... Gradient Norm: 0.4486\n",
      "Epoch 176: avg loss training: 2.8378,... Gradient Norm: 0.4352\n",
      "Epoch 177: avg loss training: 2.8379,... Gradient Norm: 0.2436\n",
      "Epoch 178: avg loss training: 2.8377,... Gradient Norm: 0.2929\n",
      "Epoch 179: avg loss training: 2.8376,... Gradient Norm: 0.3510\n",
      "Epoch 180: avg loss training: 2.8374,... Gradient Norm: 0.1923\n",
      "Epoch 181: avg loss training: 2.8374,... Gradient Norm: 0.3389\n",
      "Epoch 182: avg loss training: 2.8372,... Gradient Norm: 0.0903\n",
      "Epoch 183: avg loss training: 2.8373,... Gradient Norm: 0.3388\n",
      "Epoch 184: avg loss training: 2.8371,... Gradient Norm: 0.1406\n",
      "Epoch 185: avg loss training: 2.8370,... Gradient Norm: 0.2828\n",
      "Epoch 186: avg loss training: 2.8369,... Gradient Norm: 0.3208\n",
      "Epoch 187: avg loss training: 2.8368,... Gradient Norm: 0.0776\n",
      "Epoch 188: avg loss training: 2.8368,... Gradient Norm: 0.2324\n",
      "Epoch 189: avg loss training: 2.8366,... Gradient Norm: 0.1678\n",
      "Epoch 190: avg loss training: 2.8366,... Gradient Norm: 0.2750\n",
      "Epoch 191: avg loss training: 2.8365,... Gradient Norm: 0.1248\n",
      "Epoch 192: avg loss training: 2.8364,... Gradient Norm: 0.1573\n",
      "Epoch 193: avg loss training: 2.8363,... Gradient Norm: 0.0674\n",
      "Epoch 194: avg loss training: 2.8363,... Gradient Norm: 0.2108\n",
      "Epoch 195: avg loss training: 2.8361,... Gradient Norm: 0.1110\n",
      "Epoch 196: avg loss training: 2.8360,... Gradient Norm: 0.1683\n",
      "Epoch 197: avg loss training: 2.8359,... Gradient Norm: 0.1298\n",
      "Epoch 198: avg loss training: 2.8358,... Gradient Norm: 0.1033\n",
      "Epoch 199: avg loss training: 2.8358,... Gradient Norm: 0.1406\n",
      "Epoch 200: avg loss training: 2.8356,... Gradient Norm: 0.0885\n",
      "Epoch 201: avg loss training: 2.8356,... Gradient Norm: 0.2406\n",
      "Epoch 202: avg loss training: 2.8354,... Gradient Norm: 0.1394\n",
      "Epoch 203: avg loss training: 2.8354,... Gradient Norm: 0.1743\n",
      "Epoch 204: avg loss training: 2.8353,... Gradient Norm: 0.2970\n",
      "Epoch 205: avg loss training: 2.8352,... Gradient Norm: 0.3948\n",
      "Epoch 206: avg loss training: 2.8350,... Gradient Norm: 0.0897\n",
      "Epoch 207: avg loss training: 2.8350,... Gradient Norm: 0.4749\n",
      "Epoch 208: avg loss training: 2.8350,... Gradient Norm: 0.5013\n",
      "Epoch 209: avg loss training: 2.8348,... Gradient Norm: 0.2719\n",
      "Epoch 210: avg loss training: 2.8348,... Gradient Norm: 0.2496\n",
      "Epoch 211: avg loss training: 2.8347,... Gradient Norm: 0.5174\n",
      "Epoch 212: avg loss training: 2.8346,... Gradient Norm: 0.2034\n",
      "Epoch 213: avg loss training: 2.8345,... Gradient Norm: 0.6164\n",
      "Epoch 214: avg loss training: 2.8345,... Gradient Norm: 0.7590\n",
      "Epoch 215: avg loss training: 2.8343,... Gradient Norm: 0.1642\n",
      "Epoch 216: avg loss training: 2.8343,... Gradient Norm: 0.6215\n",
      "Epoch 217: avg loss training: 2.8342,... Gradient Norm: 0.4522\n",
      "Epoch 218: avg loss training: 2.8341,... Gradient Norm: 0.4374\n",
      "Epoch 219: avg loss training: 2.8341,... Gradient Norm: 0.6983\n",
      "Epoch 220: avg loss training: 2.8339,... Gradient Norm: 0.0684\n",
      "Epoch 221: avg loss training: 2.8340,... Gradient Norm: 0.6337\n",
      "Epoch 222: avg loss training: 2.8338,... Gradient Norm: 0.2807\n",
      "Epoch 223: avg loss training: 2.8338,... Gradient Norm: 0.3771\n",
      "Epoch 224: avg loss training: 2.8338,... Gradient Norm: 0.4808\n",
      "Epoch 225: avg loss training: 2.8336,... Gradient Norm: 0.0904\n",
      "Epoch 226: avg loss training: 2.8336,... Gradient Norm: 0.4599\n",
      "Epoch 227: avg loss training: 2.8335,... Gradient Norm: 0.2827\n",
      "Epoch 228: avg loss training: 2.8334,... Gradient Norm: 0.2955\n",
      "Epoch 229: avg loss training: 2.8334,... Gradient Norm: 0.4351\n",
      "Epoch 230: avg loss training: 2.8333,... Gradient Norm: 0.1009\n",
      "Epoch 231: avg loss training: 2.8333,... Gradient Norm: 0.4929\n",
      "Epoch 232: avg loss training: 2.8332,... Gradient Norm: 0.3564\n",
      "Epoch 233: avg loss training: 2.8331,... Gradient Norm: 0.1423\n",
      "Epoch 234: avg loss training: 2.8331,... Gradient Norm: 0.3300\n",
      "Epoch 235: avg loss training: 2.8330,... Gradient Norm: 0.1579\n",
      "Epoch 236: avg loss training: 2.8329,... Gradient Norm: 0.1816\n",
      "Epoch 237: avg loss training: 2.8329,... Gradient Norm: 0.3899\n",
      "Epoch 238: avg loss training: 2.8328,... Gradient Norm: 0.1511\n",
      "Epoch 239: avg loss training: 2.8328,... Gradient Norm: 0.2330\n",
      "Epoch 240: avg loss training: 2.8327,... Gradient Norm: 0.2450\n",
      "Epoch 241: avg loss training: 2.8326,... Gradient Norm: 0.1164\n",
      "Epoch 242: avg loss training: 2.8326,... Gradient Norm: 0.1648\n",
      "Epoch 243: avg loss training: 2.8325,... Gradient Norm: 0.0920\n",
      "Epoch 244: avg loss training: 2.8325,... Gradient Norm: 0.1225\n",
      "Epoch 245: avg loss training: 2.8324,... Gradient Norm: 0.1891\n",
      "Epoch 246: avg loss training: 2.8324,... Gradient Norm: 0.2276\n",
      "Epoch 247: avg loss training: 2.8323,... Gradient Norm: 0.1757\n",
      "Epoch 248: avg loss training: 2.8323,... Gradient Norm: 0.1842\n",
      "Epoch 249: avg loss training: 2.8322,... Gradient Norm: 0.1466\n",
      "Epoch 250: avg loss training: 2.8322,... Gradient Norm: 0.2084\n",
      "Epoch 251: avg loss training: 2.8321,... Gradient Norm: 0.2109\n",
      "Epoch 252: avg loss training: 2.8321,... Gradient Norm: 0.1236\n",
      "Epoch 253: avg loss training: 2.8320,... Gradient Norm: 0.2002\n",
      "Epoch 254: avg loss training: 2.8320,... Gradient Norm: 0.2509\n",
      "Epoch 255: avg loss training: 2.8319,... Gradient Norm: 0.0886\n",
      "Epoch 256: avg loss training: 2.8319,... Gradient Norm: 0.4498\n",
      "Epoch 257: avg loss training: 2.8318,... Gradient Norm: 0.3416\n",
      "Epoch 258: avg loss training: 2.8318,... Gradient Norm: 0.1604\n",
      "Epoch 259: avg loss training: 2.8317,... Gradient Norm: 0.2662\n",
      "Epoch 260: avg loss training: 2.8318,... Gradient Norm: 0.1696\n",
      "Epoch 261: avg loss training: 2.8317,... Gradient Norm: 0.1917\n",
      "Epoch 262: avg loss training: 2.8317,... Gradient Norm: 0.2741\n",
      "Epoch 263: avg loss training: 2.8316,... Gradient Norm: 0.1054\n",
      "Epoch 264: avg loss training: 2.8315,... Gradient Norm: 0.2714\n",
      "Epoch 265: avg loss training: 2.8315,... Gradient Norm: 0.1639\n",
      "Epoch 266: avg loss training: 2.8315,... Gradient Norm: 0.3661\n",
      "Epoch 267: avg loss training: 2.8314,... Gradient Norm: 0.3481\n",
      "Epoch 268: avg loss training: 2.8314,... Gradient Norm: 0.2323\n",
      "Epoch 269: avg loss training: 2.8313,... Gradient Norm: 0.3808\n",
      "Epoch 270: avg loss training: 2.8313,... Gradient Norm: 0.3555\n",
      "Epoch 271: avg loss training: 2.8313,... Gradient Norm: 0.6236\n",
      "Epoch 272: avg loss training: 2.8312,... Gradient Norm: 0.2390\n",
      "Epoch 273: avg loss training: 2.8312,... Gradient Norm: 0.7829\n",
      "Epoch 274: avg loss training: 2.8311,... Gradient Norm: 0.1926\n",
      "Epoch 275: avg loss training: 2.8311,... Gradient Norm: 0.5622\n",
      "Epoch 276: avg loss training: 2.8310,... Gradient Norm: 0.2017\n",
      "Epoch 277: avg loss training: 2.8310,... Gradient Norm: 0.7794\n",
      "Epoch 278: avg loss training: 2.8309,... Gradient Norm: 0.2217\n",
      "Epoch 279: avg loss training: 2.8309,... Gradient Norm: 0.4989\n",
      "Epoch 280: avg loss training: 2.8308,... Gradient Norm: 0.3596\n",
      "Epoch 281: avg loss training: 2.8308,... Gradient Norm: 0.1977\n",
      "Epoch 282: avg loss training: 2.8307,... Gradient Norm: 0.1579\n",
      "Epoch 283: avg loss training: 2.8307,... Gradient Norm: 0.2027\n",
      "Epoch 284: avg loss training: 2.8306,... Gradient Norm: 0.3813\n",
      "Epoch 285: avg loss training: 2.8306,... Gradient Norm: 0.2166\n",
      "Epoch 286: avg loss training: 2.8305,... Gradient Norm: 0.1496\n",
      "Epoch 287: avg loss training: 2.8305,... Gradient Norm: 0.1026\n",
      "Epoch 288: avg loss training: 2.8305,... Gradient Norm: 0.1402\n",
      "Epoch 289: avg loss training: 2.8304,... Gradient Norm: 0.3126\n",
      "Epoch 290: avg loss training: 2.8304,... Gradient Norm: 0.3882\n",
      "Epoch 291: avg loss training: 2.8304,... Gradient Norm: 0.1780\n",
      "Epoch 292: avg loss training: 2.8304,... Gradient Norm: 0.4937\n",
      "Epoch 293: avg loss training: 2.8303,... Gradient Norm: 0.2528\n",
      "Epoch 294: avg loss training: 2.8303,... Gradient Norm: 0.3005\n",
      "Epoch 295: avg loss training: 2.8302,... Gradient Norm: 0.3561\n",
      "Epoch 296: avg loss training: 2.8302,... Gradient Norm: 0.1582\n",
      "Epoch 297: avg loss training: 2.8301,... Gradient Norm: 0.2472\n",
      "Epoch 298: avg loss training: 2.8301,... Gradient Norm: 0.1782\n",
      "Epoch 299: avg loss training: 2.8301,... Gradient Norm: 0.3367\n",
      "Epoch 300: avg loss training: 2.8300,... Gradient Norm: 0.3211\n",
      "Epoch 301: avg loss training: 2.8300,... Gradient Norm: 0.2199\n",
      "Epoch 302: avg loss training: 2.8299,... Gradient Norm: 0.1011\n",
      "Epoch 303: avg loss training: 2.8299,... Gradient Norm: 0.1582\n",
      "Epoch 304: avg loss training: 2.8299,... Gradient Norm: 0.1068\n",
      "Epoch 305: avg loss training: 2.8298,... Gradient Norm: 0.3564\n",
      "Epoch 306: avg loss training: 2.8298,... Gradient Norm: 0.3820\n",
      "Epoch 307: avg loss training: 2.8298,... Gradient Norm: 0.2121\n",
      "Epoch 308: avg loss training: 2.8298,... Gradient Norm: 0.5947\n",
      "Epoch 309: avg loss training: 2.8297,... Gradient Norm: 0.2647\n",
      "Epoch 310: avg loss training: 2.8297,... Gradient Norm: 0.2869\n",
      "Epoch 311: avg loss training: 2.8296,... Gradient Norm: 0.5443\n",
      "Epoch 312: avg loss training: 2.8296,... Gradient Norm: 0.2791\n",
      "Epoch 313: avg loss training: 2.8296,... Gradient Norm: 0.2194\n",
      "Epoch 314: avg loss training: 2.8295,... Gradient Norm: 0.4348\n",
      "Epoch 315: avg loss training: 2.8295,... Gradient Norm: 0.3109\n",
      "Epoch 316: avg loss training: 2.8295,... Gradient Norm: 0.0741\n",
      "Epoch 317: avg loss training: 2.8295,... Gradient Norm: 0.2824\n",
      "Epoch 318: avg loss training: 2.8294,... Gradient Norm: 0.1538\n",
      "Epoch 319: avg loss training: 2.8294,... Gradient Norm: 0.1863\n",
      "Epoch 320: avg loss training: 2.8293,... Gradient Norm: 0.1764\n",
      "Epoch 321: avg loss training: 2.8293,... Gradient Norm: 0.3638\n",
      "Epoch 322: avg loss training: 2.8293,... Gradient Norm: 0.4482\n",
      "Epoch 323: avg loss training: 2.8293,... Gradient Norm: 0.2294\n",
      "Epoch 324: avg loss training: 2.8293,... Gradient Norm: 0.7541\n",
      "Epoch 325: avg loss training: 2.8292,... Gradient Norm: 0.2962\n",
      "Epoch 326: avg loss training: 2.8292,... Gradient Norm: 0.4419\n",
      "Epoch 327: avg loss training: 2.8292,... Gradient Norm: 0.1746\n",
      "Epoch 328: avg loss training: 2.8293,... Gradient Norm: 0.8575\n",
      "Epoch 329: avg loss training: 2.8291,... Gradient Norm: 0.4812\n",
      "Epoch 330: avg loss training: 2.8292,... Gradient Norm: 0.6291\n",
      "Epoch 331: avg loss training: 2.8290,... Gradient Norm: 0.3420\n",
      "Epoch 332: avg loss training: 2.8291,... Gradient Norm: 0.7061\n",
      "Epoch 333: avg loss training: 2.8290,... Gradient Norm: 0.4094\n",
      "Epoch 334: avg loss training: 2.8290,... Gradient Norm: 0.5339\n",
      "Epoch 335: avg loss training: 2.8290,... Gradient Norm: 0.1639\n",
      "Epoch 336: avg loss training: 2.8290,... Gradient Norm: 0.7127\n",
      "Epoch 337: avg loss training: 2.8289,... Gradient Norm: 0.2499\n",
      "Epoch 338: avg loss training: 2.8289,... Gradient Norm: 0.4041\n",
      "Epoch 339: avg loss training: 2.8289,... Gradient Norm: 0.2760\n",
      "Epoch 340: avg loss training: 2.8289,... Gradient Norm: 0.4748\n",
      "Epoch 341: avg loss training: 2.8289,... Gradient Norm: 0.3051\n",
      "Epoch 342: avg loss training: 2.8288,... Gradient Norm: 0.2563\n",
      "Epoch 343: avg loss training: 2.8288,... Gradient Norm: 0.4267\n",
      "Epoch 344: avg loss training: 2.8288,... Gradient Norm: 0.2362\n",
      "Epoch 345: avg loss training: 2.8288,... Gradient Norm: 0.2905\n",
      "Epoch 346: avg loss training: 2.8288,... Gradient Norm: 0.3339\n",
      "Epoch 347: avg loss training: 2.8287,... Gradient Norm: 0.5150\n",
      "Epoch 348: avg loss training: 2.8287,... Gradient Norm: 0.2596\n",
      "Epoch 349: avg loss training: 2.8287,... Gradient Norm: 0.4050\n",
      "Epoch 350: avg loss training: 2.8287,... Gradient Norm: 0.2149\n",
      "Epoch 351: avg loss training: 2.8286,... Gradient Norm: 0.5170\n",
      "Epoch 352: avg loss training: 2.8286,... Gradient Norm: 0.1052\n",
      "Epoch 353: avg loss training: 2.8286,... Gradient Norm: 0.2762\n",
      "Epoch 354: avg loss training: 2.8286,... Gradient Norm: 0.2728\n",
      "Epoch 355: avg loss training: 2.8286,... Gradient Norm: 0.3344\n",
      "Epoch 356: avg loss training: 2.8285,... Gradient Norm: 0.3137\n",
      "Epoch 357: avg loss training: 2.8285,... Gradient Norm: 0.2478\n",
      "Epoch 358: avg loss training: 2.8285,... Gradient Norm: 0.3249\n",
      "Epoch 359: avg loss training: 2.8285,... Gradient Norm: 0.0745\n",
      "Epoch 360: avg loss training: 2.8285,... Gradient Norm: 0.5269\n",
      "Epoch 361: avg loss training: 2.8284,... Gradient Norm: 0.1415\n",
      "Epoch 362: avg loss training: 2.8284,... Gradient Norm: 0.2479\n",
      "Epoch 363: avg loss training: 2.8284,... Gradient Norm: 0.2142\n",
      "Epoch 364: avg loss training: 2.8284,... Gradient Norm: 0.1097\n",
      "Epoch 365: avg loss training: 2.8284,... Gradient Norm: 0.4433\n",
      "Epoch 366: avg loss training: 2.8284,... Gradient Norm: 0.1885\n",
      "Epoch 367: avg loss training: 2.8284,... Gradient Norm: 0.3387\n",
      "Epoch 368: avg loss training: 2.8283,... Gradient Norm: 0.3098\n",
      "Epoch 369: avg loss training: 2.8283,... Gradient Norm: 0.1224\n",
      "Epoch 370: avg loss training: 2.8283,... Gradient Norm: 0.5027\n",
      "Epoch 371: avg loss training: 2.8283,... Gradient Norm: 0.1064\n",
      "Epoch 372: avg loss training: 2.8283,... Gradient Norm: 0.1877\n",
      "Epoch 373: avg loss training: 2.8282,... Gradient Norm: 0.2652\n",
      "Epoch 374: avg loss training: 2.8282,... Gradient Norm: 0.1363\n",
      "Epoch 375: avg loss training: 2.8282,... Gradient Norm: 0.3079\n",
      "Epoch 376: avg loss training: 2.8282,... Gradient Norm: 0.0695\n",
      "Epoch 377: avg loss training: 2.8282,... Gradient Norm: 0.1135\n",
      "Epoch 378: avg loss training: 2.8282,... Gradient Norm: 0.1302\n",
      "Epoch 379: avg loss training: 2.8282,... Gradient Norm: 0.3099\n",
      "Epoch 380: avg loss training: 2.8281,... Gradient Norm: 0.2351\n",
      "Epoch 381: avg loss training: 2.8281,... Gradient Norm: 0.1727\n",
      "Epoch 382: avg loss training: 2.8281,... Gradient Norm: 0.2037\n",
      "Epoch 383: avg loss training: 2.8281,... Gradient Norm: 0.1304\n",
      "Epoch 384: avg loss training: 2.8281,... Gradient Norm: 0.1280\n",
      "Epoch 385: avg loss training: 2.8281,... Gradient Norm: 0.2670\n",
      "Epoch 386: avg loss training: 2.8281,... Gradient Norm: 0.1729\n",
      "Epoch 387: avg loss training: 2.8280,... Gradient Norm: 0.1879\n",
      "Epoch 388: avg loss training: 2.8280,... Gradient Norm: 0.2169\n",
      "Epoch 389: avg loss training: 2.8280,... Gradient Norm: 0.4059\n",
      "Epoch 390: avg loss training: 2.8280,... Gradient Norm: 0.1304\n",
      "Epoch 391: avg loss training: 2.8280,... Gradient Norm: 0.2211\n",
      "Epoch 392: avg loss training: 2.8280,... Gradient Norm: 0.2052\n",
      "Epoch 393: avg loss training: 2.8280,... Gradient Norm: 0.1792\n",
      "Epoch 394: avg loss training: 2.8280,... Gradient Norm: 0.4379\n",
      "Epoch 395: avg loss training: 2.8280,... Gradient Norm: 0.2788\n",
      "Epoch 396: avg loss training: 2.8279,... Gradient Norm: 0.2031\n",
      "Epoch 397: avg loss training: 2.8279,... Gradient Norm: 0.1633\n",
      "Epoch 398: avg loss training: 2.8279,... Gradient Norm: 0.1023\n",
      "Epoch 399: avg loss training: 2.8279,... Gradient Norm: 0.3143\n",
      "Epoch 400: avg loss training: 2.8279,... Gradient Norm: 0.0900\n",
      "Epoch 401: avg loss training: 2.8279,... Gradient Norm: 0.1461\n",
      "Epoch 402: avg loss training: 2.8278,... Gradient Norm: 0.1504\n",
      "Epoch 403: avg loss training: 2.8278,... Gradient Norm: 0.2748\n",
      "Epoch 404: avg loss training: 2.8278,... Gradient Norm: 0.1693\n",
      "Epoch 405: avg loss training: 2.8278,... Gradient Norm: 0.1437\n",
      "Epoch 406: avg loss training: 2.8278,... Gradient Norm: 0.1936\n",
      "Epoch 407: avg loss training: 2.8278,... Gradient Norm: 0.3176\n",
      "Epoch 408: avg loss training: 2.8278,... Gradient Norm: 0.1199\n",
      "Epoch 409: avg loss training: 2.8278,... Gradient Norm: 0.2223\n",
      "Epoch 410: avg loss training: 2.8278,... Gradient Norm: 0.1603\n",
      "Epoch 411: avg loss training: 2.8277,... Gradient Norm: 0.2937\n",
      "Epoch 412: avg loss training: 2.8277,... Gradient Norm: 0.2277\n",
      "Epoch 413: avg loss training: 2.8277,... Gradient Norm: 0.1442\n",
      "Epoch 414: avg loss training: 2.8277,... Gradient Norm: 0.1202\n",
      "Epoch 415: avg loss training: 2.8277,... Gradient Norm: 0.2057\n",
      "Epoch 416: avg loss training: 2.8277,... Gradient Norm: 0.3143\n",
      "Epoch 417: avg loss training: 2.8277,... Gradient Norm: 0.2692\n",
      "Epoch 418: avg loss training: 2.8277,... Gradient Norm: 0.2891\n",
      "Epoch 419: avg loss training: 2.8277,... Gradient Norm: 0.2630\n",
      "Epoch 420: avg loss training: 2.8277,... Gradient Norm: 0.1674\n",
      "Epoch 421: avg loss training: 2.8276,... Gradient Norm: 0.3341\n",
      "Epoch 422: avg loss training: 2.8276,... Gradient Norm: 0.2995\n",
      "Epoch 423: avg loss training: 2.8276,... Gradient Norm: 0.1226\n",
      "Epoch 424: avg loss training: 2.8276,... Gradient Norm: 0.3056\n",
      "Epoch 425: avg loss training: 2.8276,... Gradient Norm: 0.2246\n",
      "Epoch 426: avg loss training: 2.8276,... Gradient Norm: 0.1034\n",
      "Epoch 427: avg loss training: 2.8276,... Gradient Norm: 0.1641\n",
      "Epoch 428: avg loss training: 2.8276,... Gradient Norm: 0.4917\n",
      "Epoch 429: avg loss training: 2.8276,... Gradient Norm: 0.1367\n",
      "Epoch 430: avg loss training: 2.8276,... Gradient Norm: 0.1462\n",
      "Epoch 431: avg loss training: 2.8276,... Gradient Norm: 0.2383\n",
      "Epoch 432: avg loss training: 2.8275,... Gradient Norm: 0.1718\n",
      "Epoch 433: avg loss training: 2.8275,... Gradient Norm: 0.3477\n",
      "Epoch 434: avg loss training: 2.8275,... Gradient Norm: 0.2542\n",
      "Epoch 435: avg loss training: 2.8275,... Gradient Norm: 0.1427\n",
      "Epoch 436: avg loss training: 2.8275,... Gradient Norm: 0.2096\n",
      "Epoch 437: avg loss training: 2.8275,... Gradient Norm: 0.1365\n",
      "Epoch 438: avg loss training: 2.8275,... Gradient Norm: 0.2263\n",
      "Epoch 439: avg loss training: 2.8275,... Gradient Norm: 0.3666\n",
      "Epoch 440: avg loss training: 2.8275,... Gradient Norm: 0.0948\n",
      "Epoch 441: avg loss training: 2.8275,... Gradient Norm: 0.2525\n",
      "Epoch 442: avg loss training: 2.8274,... Gradient Norm: 0.0803\n",
      "Epoch 443: avg loss training: 2.8274,... Gradient Norm: 0.3433\n",
      "Epoch 444: avg loss training: 2.8274,... Gradient Norm: 0.2426\n",
      "Epoch 445: avg loss training: 2.8274,... Gradient Norm: 0.1337\n",
      "Epoch 446: avg loss training: 2.8274,... Gradient Norm: 0.1449\n",
      "Epoch 447: avg loss training: 2.8274,... Gradient Norm: 0.2981\n",
      "Epoch 448: avg loss training: 2.8274,... Gradient Norm: 0.2844\n",
      "Epoch 449: avg loss training: 2.8274,... Gradient Norm: 0.1217\n",
      "Epoch 450: avg loss training: 2.8274,... Gradient Norm: 0.3026\n",
      "Epoch 451: avg loss training: 2.8274,... Gradient Norm: 0.2799\n",
      "Epoch 452: avg loss training: 2.8274,... Gradient Norm: 0.1596\n",
      "Epoch 453: avg loss training: 2.8274,... Gradient Norm: 0.4526\n",
      "Epoch 454: avg loss training: 2.8274,... Gradient Norm: 0.1184\n",
      "Epoch 455: avg loss training: 2.8274,... Gradient Norm: 0.3277\n",
      "Epoch 456: avg loss training: 2.8273,... Gradient Norm: 0.1619\n",
      "Epoch 457: avg loss training: 2.8273,... Gradient Norm: 0.3435\n",
      "Epoch 458: avg loss training: 2.8273,... Gradient Norm: 0.1313\n",
      "Epoch 459: avg loss training: 2.8273,... Gradient Norm: 0.1658\n",
      "Epoch 460: avg loss training: 2.8273,... Gradient Norm: 0.3087\n",
      "Epoch 461: avg loss training: 2.8273,... Gradient Norm: 0.1569\n",
      "Epoch 462: avg loss training: 2.8273,... Gradient Norm: 0.4355\n",
      "Epoch 463: avg loss training: 2.8273,... Gradient Norm: 0.1977\n",
      "Epoch 464: avg loss training: 2.8273,... Gradient Norm: 0.2690\n",
      "Epoch 465: avg loss training: 2.8273,... Gradient Norm: 0.1429\n",
      "Epoch 466: avg loss training: 2.8273,... Gradient Norm: 0.3502\n",
      "Epoch 467: avg loss training: 2.8273,... Gradient Norm: 0.1973\n",
      "Epoch 468: avg loss training: 2.8273,... Gradient Norm: 0.3132\n",
      "Epoch 469: avg loss training: 2.8272,... Gradient Norm: 0.1706\n",
      "Epoch 470: avg loss training: 2.8273,... Gradient Norm: 0.3399\n",
      "Epoch 471: avg loss training: 2.8272,... Gradient Norm: 0.2412\n",
      "Epoch 472: avg loss training: 2.8272,... Gradient Norm: 0.3579\n",
      "Epoch 473: avg loss training: 2.8272,... Gradient Norm: 0.1132\n",
      "Epoch 474: avg loss training: 2.8272,... Gradient Norm: 0.3704\n",
      "Epoch 475: avg loss training: 2.8272,... Gradient Norm: 0.1462\n",
      "Epoch 476: avg loss training: 2.8272,... Gradient Norm: 0.1769\n",
      "Epoch 477: avg loss training: 2.8272,... Gradient Norm: 0.0787\n",
      "Epoch 478: avg loss training: 2.8272,... Gradient Norm: 0.1022\n",
      "Epoch 479: avg loss training: 2.8272,... Gradient Norm: 0.1098\n",
      "Epoch 480: avg loss training: 2.8272,... Gradient Norm: 0.2162\n",
      "Epoch 481: avg loss training: 2.8272,... Gradient Norm: 0.2100\n",
      "Epoch 482: avg loss training: 2.8272,... Gradient Norm: 0.1298\n",
      "Epoch 483: avg loss training: 2.8272,... Gradient Norm: 0.2938\n",
      "Epoch 484: avg loss training: 2.8272,... Gradient Norm: 0.2436\n",
      "Epoch 485: avg loss training: 2.8271,... Gradient Norm: 0.1624\n",
      "Epoch 486: avg loss training: 2.8271,... Gradient Norm: 0.3070\n",
      "Epoch 487: avg loss training: 2.8271,... Gradient Norm: 0.2273\n",
      "Epoch 488: avg loss training: 2.8271,... Gradient Norm: 0.1425\n",
      "Epoch 489: avg loss training: 2.8271,... Gradient Norm: 0.1181\n",
      "Epoch 490: avg loss training: 2.8271,... Gradient Norm: 0.3118\n",
      "Epoch 491: avg loss training: 2.8271,... Gradient Norm: 0.2363\n",
      "Epoch 492: avg loss training: 2.8271,... Gradient Norm: 0.1664\n",
      "Epoch 493: avg loss training: 2.8271,... Gradient Norm: 0.1545\n",
      "Epoch 494: avg loss training: 2.8271,... Gradient Norm: 0.2650\n",
      "Epoch 495: avg loss training: 2.8271,... Gradient Norm: 0.2769\n",
      "Epoch 496: avg loss training: 2.8271,... Gradient Norm: 0.2044\n",
      "Epoch 497: avg loss training: 2.8271,... Gradient Norm: 0.2032\n",
      "Epoch 498: avg loss training: 2.8271,... Gradient Norm: 0.1641\n",
      "Epoch 499: avg loss training: 2.8271,... Gradient Norm: 0.1312\n",
      "Epoch 500: avg loss training: 2.8271,... Gradient Norm: 0.2177\n",
      "Epoch 501: avg loss training: 2.8271,... Gradient Norm: 0.2786\n",
      "Epoch 502: avg loss training: 2.8271,... Gradient Norm: 0.2295\n",
      "Epoch 503: avg loss training: 2.8270,... Gradient Norm: 0.1830\n",
      "Epoch 504: avg loss training: 2.8270,... Gradient Norm: 0.1133\n",
      "Epoch 505: avg loss training: 2.8270,... Gradient Norm: 0.1163\n",
      "Epoch 506: avg loss training: 2.8270,... Gradient Norm: 0.3682\n",
      "Epoch 507: avg loss training: 2.8270,... Gradient Norm: 0.1587\n",
      "Epoch 508: avg loss training: 2.8270,... Gradient Norm: 0.1145\n",
      "Epoch 509: avg loss training: 2.8270,... Gradient Norm: 0.1159\n",
      "Epoch 510: avg loss training: 2.8270,... Gradient Norm: 0.1278\n",
      "Epoch 511: avg loss training: 2.8270,... Gradient Norm: 0.2816\n",
      "Epoch 512: avg loss training: 2.8270,... Gradient Norm: 0.3184\n",
      "Epoch 513: avg loss training: 2.8270,... Gradient Norm: 0.1959\n",
      "Epoch 514: avg loss training: 2.8270,... Gradient Norm: 0.2433\n",
      "Epoch 515: avg loss training: 2.8270,... Gradient Norm: 0.1802\n",
      "Epoch 516: avg loss training: 2.8270,... Gradient Norm: 0.1352\n",
      "Epoch 517: avg loss training: 2.8270,... Gradient Norm: 0.3201\n",
      "Epoch 518: avg loss training: 2.8270,... Gradient Norm: 0.2818\n",
      "Epoch 519: avg loss training: 2.8270,... Gradient Norm: 0.1245\n",
      "Epoch 520: avg loss training: 2.8270,... Gradient Norm: 0.1656\n",
      "Epoch 521: avg loss training: 2.8270,... Gradient Norm: 0.1764\n",
      "Epoch 522: avg loss training: 2.8270,... Gradient Norm: 0.2264\n",
      "Epoch 523: avg loss training: 2.8270,... Gradient Norm: 0.0749\n",
      "Epoch 524: avg loss training: 2.8270,... Gradient Norm: 0.3933\n",
      "Epoch 525: avg loss training: 2.8270,... Gradient Norm: 0.2667\n",
      "Epoch 526: avg loss training: 2.8269,... Gradient Norm: 0.1336\n",
      "Epoch 527: avg loss training: 2.8269,... Gradient Norm: 0.1700\n",
      "Epoch 528: avg loss training: 2.8269,... Gradient Norm: 0.1742\n",
      "Epoch 529: avg loss training: 2.8269,... Gradient Norm: 0.1556\n",
      "Epoch 530: avg loss training: 2.8269,... Gradient Norm: 0.1334\n",
      "Epoch 531: avg loss training: 2.8269,... Gradient Norm: 0.4194\n",
      "Epoch 532: avg loss training: 2.8269,... Gradient Norm: 0.3758\n",
      "Epoch 533: avg loss training: 2.8269,... Gradient Norm: 0.2686\n",
      "Epoch 534: avg loss training: 2.8269,... Gradient Norm: 0.1843\n",
      "Epoch 535: avg loss training: 2.8269,... Gradient Norm: 0.1910\n",
      "Epoch 536: avg loss training: 2.8269,... Gradient Norm: 0.1958\n",
      "Epoch 537: avg loss training: 2.8269,... Gradient Norm: 0.1394\n",
      "Epoch 538: avg loss training: 2.8269,... Gradient Norm: 0.4039\n",
      "Epoch 539: avg loss training: 2.8269,... Gradient Norm: 0.3200\n",
      "Epoch 540: avg loss training: 2.8269,... Gradient Norm: 0.2397\n",
      "Epoch 541: avg loss training: 2.8269,... Gradient Norm: 0.1595\n",
      "Epoch 542: avg loss training: 2.8269,... Gradient Norm: 0.1618\n",
      "Epoch 543: avg loss training: 2.8269,... Gradient Norm: 0.1323\n",
      "Epoch 544: avg loss training: 2.8269,... Gradient Norm: 0.1834\n",
      "Epoch 545: avg loss training: 2.8269,... Gradient Norm: 0.3308\n",
      "Epoch 546: avg loss training: 2.8269,... Gradient Norm: 0.3865\n",
      "Epoch 547: avg loss training: 2.8269,... Gradient Norm: 0.1645\n",
      "Epoch 548: avg loss training: 2.8269,... Gradient Norm: 0.2046\n",
      "Epoch 549: avg loss training: 2.8269,... Gradient Norm: 0.2659\n",
      "Epoch 550: avg loss training: 2.8269,... Gradient Norm: 0.1666\n",
      "Epoch 551: avg loss training: 2.8269,... Gradient Norm: 0.3075\n",
      "Epoch 552: avg loss training: 2.8268,... Gradient Norm: 0.3137\n",
      "Epoch 553: avg loss training: 2.8268,... Gradient Norm: 0.2249\n",
      "Epoch 554: avg loss training: 2.8268,... Gradient Norm: 0.2162\n",
      "Epoch 555: avg loss training: 2.8268,... Gradient Norm: 0.1547\n",
      "Epoch 556: avg loss training: 2.8268,... Gradient Norm: 0.1445\n",
      "Epoch 557: avg loss training: 2.8268,... Gradient Norm: 0.1564\n",
      "Epoch 558: avg loss training: 2.8268,... Gradient Norm: 0.3276\n",
      "Epoch 559: avg loss training: 2.8268,... Gradient Norm: 0.0755\n",
      "Epoch 560: avg loss training: 2.8268,... Gradient Norm: 0.1865\n",
      "Epoch 561: avg loss training: 2.8268,... Gradient Norm: 0.1587\n",
      "Epoch 562: avg loss training: 2.8268,... Gradient Norm: 0.2359\n",
      "Epoch 563: avg loss training: 2.8268,... Gradient Norm: 0.1831\n",
      "Epoch 564: avg loss training: 2.8268,... Gradient Norm: 0.1320\n",
      "Epoch 565: avg loss training: 2.8268,... Gradient Norm: 0.2663\n",
      "Epoch 566: avg loss training: 2.8268,... Gradient Norm: 0.1302\n",
      "Epoch 567: avg loss training: 2.8268,... Gradient Norm: 0.1092\n",
      "Epoch 568: avg loss training: 2.8268,... Gradient Norm: 0.2177\n",
      "Epoch 569: avg loss training: 2.8268,... Gradient Norm: 0.1187\n",
      "Epoch 570: avg loss training: 2.8268,... Gradient Norm: 0.1330\n",
      "Epoch 571: avg loss training: 2.8268,... Gradient Norm: 0.2136\n",
      "Epoch 572: avg loss training: 2.8268,... Gradient Norm: 0.3384\n",
      "Epoch 573: avg loss training: 2.8268,... Gradient Norm: 0.1003\n",
      "Epoch 574: avg loss training: 2.8268,... Gradient Norm: 0.2518\n",
      "Epoch 575: avg loss training: 2.8268,... Gradient Norm: 0.1373\n",
      "Epoch 576: avg loss training: 2.8268,... Gradient Norm: 0.3368\n",
      "Epoch 577: avg loss training: 2.8268,... Gradient Norm: 0.3093\n",
      "Epoch 578: avg loss training: 2.8268,... Gradient Norm: 0.1087\n",
      "Epoch 579: avg loss training: 2.8268,... Gradient Norm: 0.1863\n",
      "Epoch 580: avg loss training: 2.8268,... Gradient Norm: 0.3250\n",
      "Epoch 581: avg loss training: 2.8268,... Gradient Norm: 0.3307\n",
      "Epoch 582: avg loss training: 2.8268,... Gradient Norm: 0.2775\n",
      "Epoch 583: avg loss training: 2.8268,... Gradient Norm: 0.1628\n",
      "Epoch 584: avg loss training: 2.8268,... Gradient Norm: 0.4192\n",
      "Epoch 585: avg loss training: 2.8268,... Gradient Norm: 0.1577\n",
      "Epoch 586: avg loss training: 2.8268,... Gradient Norm: 0.3783\n",
      "Epoch 587: avg loss training: 2.8267,... Gradient Norm: 0.2926\n",
      "Epoch 588: avg loss training: 2.8267,... Gradient Norm: 0.3912\n",
      "Epoch 589: avg loss training: 2.8267,... Gradient Norm: 0.3953\n",
      "Epoch 590: avg loss training: 2.8267,... Gradient Norm: 0.1454\n",
      "Epoch 591: avg loss training: 2.8267,... Gradient Norm: 0.3812\n",
      "Epoch 592: avg loss training: 2.8267,... Gradient Norm: 0.2607\n",
      "Epoch 593: avg loss training: 2.8267,... Gradient Norm: 0.1568\n",
      "Epoch 594: avg loss training: 2.8267,... Gradient Norm: 0.1589\n",
      "Epoch 595: avg loss training: 2.8267,... Gradient Norm: 0.3959\n",
      "Epoch 596: avg loss training: 2.8267,... Gradient Norm: 0.2831\n",
      "Epoch 597: avg loss training: 2.8267,... Gradient Norm: 0.3400\n",
      "Epoch 598: avg loss training: 2.8267,... Gradient Norm: 0.1293\n",
      "Epoch 599: avg loss training: 2.8267,... Gradient Norm: 0.1571\n",
      "Epoch 600: avg loss training: 2.8267,... Gradient Norm: 0.1498\n",
      "Epoch 601: avg loss training: 2.8267,... Gradient Norm: 0.0819\n",
      "Epoch 602: avg loss training: 2.8267,... Gradient Norm: 0.1975\n",
      "Epoch 603: avg loss training: 2.8267,... Gradient Norm: 0.2967\n",
      "Epoch 604: avg loss training: 2.8267,... Gradient Norm: 0.3176\n",
      "Epoch 605: avg loss training: 2.8267,... Gradient Norm: 0.1379\n",
      "Epoch 606: avg loss training: 2.8267,... Gradient Norm: 0.1518\n",
      "Epoch 607: avg loss training: 2.8267,... Gradient Norm: 0.2931\n",
      "Epoch 608: avg loss training: 2.8267,... Gradient Norm: 0.1263\n",
      "Epoch 609: avg loss training: 2.8267,... Gradient Norm: 0.1321\n",
      "Epoch 610: avg loss training: 2.8267,... Gradient Norm: 0.0851\n",
      "Epoch 611: avg loss training: 2.8267,... Gradient Norm: 0.3047\n",
      "Epoch 612: avg loss training: 2.8267,... Gradient Norm: 0.3089\n",
      "Epoch 613: avg loss training: 2.8267,... Gradient Norm: 0.1796\n",
      "Epoch 614: avg loss training: 2.8267,... Gradient Norm: 0.2018\n",
      "Epoch 615: avg loss training: 2.8267,... Gradient Norm: 0.1431\n",
      "Epoch 616: avg loss training: 2.8267,... Gradient Norm: 0.1568\n",
      "Epoch 617: avg loss training: 2.8267,... Gradient Norm: 0.1502\n",
      "Epoch 618: avg loss training: 2.8267,... Gradient Norm: 0.1443\n",
      "Epoch 619: avg loss training: 2.8267,... Gradient Norm: 0.3653\n",
      "Epoch 620: avg loss training: 2.8267,... Gradient Norm: 0.2625\n",
      "Epoch 621: avg loss training: 2.8267,... Gradient Norm: 0.2205\n",
      "Epoch 622: avg loss training: 2.8267,... Gradient Norm: 0.1505\n",
      "Epoch 623: avg loss training: 2.8267,... Gradient Norm: 0.1679\n",
      "Epoch 624: avg loss training: 2.8267,... Gradient Norm: 0.1274\n",
      "Epoch 625: avg loss training: 2.8267,... Gradient Norm: 0.1021\n",
      "Epoch 626: avg loss training: 2.8267,... Gradient Norm: 0.3167\n",
      "Epoch 627: avg loss training: 2.8267,... Gradient Norm: 0.3103\n",
      "Epoch 628: avg loss training: 2.8267,... Gradient Norm: 0.3818\n",
      "Epoch 629: avg loss training: 2.8267,... Gradient Norm: 0.1508\n",
      "Epoch 630: avg loss training: 2.8267,... Gradient Norm: 0.1549\n",
      "Epoch 631: avg loss training: 2.8267,... Gradient Norm: 0.1823\n",
      "Epoch 632: avg loss training: 2.8267,... Gradient Norm: 0.3479\n",
      "Epoch 633: avg loss training: 2.8267,... Gradient Norm: 0.1893\n",
      "Epoch 634: avg loss training: 2.8267,... Gradient Norm: 0.3875\n",
      "Epoch 635: avg loss training: 2.8267,... Gradient Norm: 0.3889\n",
      "Epoch 636: avg loss training: 2.8267,... Gradient Norm: 0.1212\n",
      "Epoch 637: avg loss training: 2.8267,... Gradient Norm: 0.2587\n",
      "Epoch 638: avg loss training: 2.8266,... Gradient Norm: 0.2122\n",
      "Epoch 639: avg loss training: 2.8266,... Gradient Norm: 0.1325\n",
      "Epoch 640: avg loss training: 2.8266,... Gradient Norm: 0.1688\n",
      "Epoch 641: avg loss training: 2.8266,... Gradient Norm: 0.3587\n",
      "Epoch 642: avg loss training: 2.8266,... Gradient Norm: 0.2625\n",
      "Epoch 643: avg loss training: 2.8266,... Gradient Norm: 0.2722\n",
      "Epoch 644: avg loss training: 2.8266,... Gradient Norm: 0.1426\n",
      "Epoch 645: avg loss training: 2.8266,... Gradient Norm: 0.3957\n",
      "Epoch 646: avg loss training: 2.8266,... Gradient Norm: 0.1002\n",
      "Epoch 647: avg loss training: 2.8266,... Gradient Norm: 0.2879\n",
      "Epoch 648: avg loss training: 2.8266,... Gradient Norm: 0.1689\n",
      "Epoch 649: avg loss training: 2.8266,... Gradient Norm: 0.1412\n",
      "Epoch 650: avg loss training: 2.8266,... Gradient Norm: 0.1472\n",
      "Epoch 651: avg loss training: 2.8266,... Gradient Norm: 0.3051\n",
      "Epoch 652: avg loss training: 2.8266,... Gradient Norm: 0.2696\n",
      "Epoch 653: avg loss training: 2.8266,... Gradient Norm: 0.1289\n",
      "Epoch 654: avg loss training: 2.8266,... Gradient Norm: 0.1416\n",
      "Epoch 655: avg loss training: 2.8266,... Gradient Norm: 0.2150\n",
      "Epoch 656: avg loss training: 2.8266,... Gradient Norm: 0.1134\n",
      "Epoch 657: avg loss training: 2.8266,... Gradient Norm: 0.3544\n",
      "Epoch 658: avg loss training: 2.8266,... Gradient Norm: 0.1037\n",
      "Epoch 659: avg loss training: 2.8266,... Gradient Norm: 0.3102\n",
      "Epoch 660: avg loss training: 2.8266,... Gradient Norm: 0.1103\n",
      "Epoch 661: avg loss training: 2.8266,... Gradient Norm: 0.3088\n",
      "Epoch 662: avg loss training: 2.8266,... Gradient Norm: 0.2190\n",
      "Epoch 663: avg loss training: 2.8266,... Gradient Norm: 0.0949\n",
      "Epoch 664: avg loss training: 2.8266,... Gradient Norm: 0.3095\n",
      "Epoch 665: avg loss training: 2.8266,... Gradient Norm: 0.2808\n",
      "Epoch 666: avg loss training: 2.8266,... Gradient Norm: 0.1428\n",
      "Epoch 667: avg loss training: 2.8266,... Gradient Norm: 0.1592\n",
      "Epoch 668: avg loss training: 2.8266,... Gradient Norm: 0.1061\n",
      "Epoch 669: avg loss training: 2.8266,... Gradient Norm: 0.2965\n",
      "Epoch 670: avg loss training: 2.8266,... Gradient Norm: 0.1687\n",
      "Epoch 671: avg loss training: 2.8266,... Gradient Norm: 0.3336\n",
      "Epoch 672: avg loss training: 2.8266,... Gradient Norm: 0.1619\n",
      "Epoch 673: avg loss training: 2.8266,... Gradient Norm: 0.1605\n",
      "Epoch 674: avg loss training: 2.8266,... Gradient Norm: 0.3667\n",
      "Epoch 675: avg loss training: 2.8266,... Gradient Norm: 0.1297\n",
      "Epoch 676: avg loss training: 2.8266,... Gradient Norm: 0.3859\n",
      "Epoch 677: avg loss training: 2.8266,... Gradient Norm: 0.0639\n",
      "Epoch 678: avg loss training: 2.8266,... Gradient Norm: 0.2582\n",
      "Epoch 679: avg loss training: 2.8266,... Gradient Norm: 0.1203\n",
      "Epoch 680: avg loss training: 2.8266,... Gradient Norm: 0.1656\n",
      "Epoch 681: avg loss training: 2.8266,... Gradient Norm: 0.1579\n",
      "Epoch 682: avg loss training: 2.8266,... Gradient Norm: 0.2503\n",
      "Epoch 683: avg loss training: 2.8266,... Gradient Norm: 0.2661\n",
      "Epoch 684: avg loss training: 2.8266,... Gradient Norm: 0.1448\n",
      "Epoch 685: avg loss training: 2.8266,... Gradient Norm: 0.1864\n",
      "Epoch 686: avg loss training: 2.8266,... Gradient Norm: 0.1320\n",
      "Epoch 687: avg loss training: 2.8266,... Gradient Norm: 0.2529\n",
      "Epoch 688: avg loss training: 2.8266,... Gradient Norm: 0.2268\n",
      "Epoch 689: avg loss training: 2.8266,... Gradient Norm: 0.1418\n",
      "Epoch 690: avg loss training: 2.8266,... Gradient Norm: 0.2935\n",
      "Epoch 691: avg loss training: 2.8266,... Gradient Norm: 0.1951\n",
      "Epoch 692: avg loss training: 2.8266,... Gradient Norm: 0.2428\n",
      "Epoch 693: avg loss training: 2.8266,... Gradient Norm: 0.1423\n",
      "Epoch 694: avg loss training: 2.8266,... Gradient Norm: 0.3482\n",
      "Epoch 695: avg loss training: 2.8266,... Gradient Norm: 0.1967\n",
      "Epoch 696: avg loss training: 2.8266,... Gradient Norm: 0.1280\n",
      "Epoch 697: avg loss training: 2.8266,... Gradient Norm: 0.1051\n",
      "Epoch 698: avg loss training: 2.8266,... Gradient Norm: 0.2484\n",
      "Epoch 699: avg loss training: 2.8266,... Gradient Norm: 0.2404\n",
      "Epoch 700: avg loss training: 2.8266,... Gradient Norm: 0.1319\n",
      "Epoch 701: avg loss training: 2.8266,... Gradient Norm: 0.1644\n",
      "Epoch 702: avg loss training: 2.8266,... Gradient Norm: 0.2267\n",
      "Epoch 703: avg loss training: 2.8266,... Gradient Norm: 0.2895\n",
      "Epoch 704: avg loss training: 2.8266,... Gradient Norm: 0.1493\n",
      "Epoch 705: avg loss training: 2.8266,... Gradient Norm: 0.1593\n",
      "Epoch 706: avg loss training: 2.8266,... Gradient Norm: 0.1574\n",
      "Epoch 707: avg loss training: 2.8266,... Gradient Norm: 0.2784\n",
      "Epoch 708: avg loss training: 2.8266,... Gradient Norm: 0.1209\n",
      "Epoch 709: avg loss training: 2.8266,... Gradient Norm: 0.1359\n",
      "Epoch 710: avg loss training: 2.8266,... Gradient Norm: 0.0955\n",
      "Epoch 711: avg loss training: 2.8266,... Gradient Norm: 0.2424\n",
      "Epoch 712: avg loss training: 2.8266,... Gradient Norm: 0.1665\n",
      "Epoch 713: avg loss training: 2.8266,... Gradient Norm: 0.0935\n",
      "Epoch 714: avg loss training: 2.8266,... Gradient Norm: 0.1216\n",
      "Epoch 715: avg loss training: 2.8266,... Gradient Norm: 0.3873\n",
      "Epoch 716: avg loss training: 2.8266,... Gradient Norm: 0.3226\n",
      "Epoch 717: avg loss training: 2.8266,... Gradient Norm: 0.1557\n",
      "Epoch 718: avg loss training: 2.8266,... Gradient Norm: 0.1207\n",
      "Epoch 719: avg loss training: 2.8266,... Gradient Norm: 0.1174\n",
      "Epoch 720: avg loss training: 2.8266,... Gradient Norm: 0.2579\n",
      "Epoch 721: avg loss training: 2.8266,... Gradient Norm: 0.2898\n",
      "Epoch 722: avg loss training: 2.8266,... Gradient Norm: 0.1701\n",
      "Epoch 723: avg loss training: 2.8266,... Gradient Norm: 0.1471\n",
      "Epoch 724: avg loss training: 2.8266,... Gradient Norm: 0.1904\n",
      "Epoch 725: avg loss training: 2.8266,... Gradient Norm: 0.2922\n",
      "Epoch 726: avg loss training: 2.8266,... Gradient Norm: 0.3001\n",
      "Epoch 727: avg loss training: 2.8266,... Gradient Norm: 0.3803\n",
      "Epoch 728: avg loss training: 2.8266,... Gradient Norm: 0.1377\n",
      "Epoch 729: avg loss training: 2.8266,... Gradient Norm: 0.2873\n",
      "Epoch 730: avg loss training: 2.8266,... Gradient Norm: 0.1859\n",
      "Epoch 731: avg loss training: 2.8266,... Gradient Norm: 0.1575\n",
      "Epoch 732: avg loss training: 2.8265,... Gradient Norm: 0.1188\n",
      "Epoch 733: avg loss training: 2.8265,... Gradient Norm: 0.2145\n",
      "Epoch 734: avg loss training: 2.8265,... Gradient Norm: 0.3204\n",
      "Epoch 735: avg loss training: 2.8265,... Gradient Norm: 0.3373\n",
      "Epoch 736: avg loss training: 2.8265,... Gradient Norm: 0.1735\n",
      "Epoch 737: avg loss training: 2.8265,... Gradient Norm: 0.2226\n",
      "Epoch 738: avg loss training: 2.8265,... Gradient Norm: 0.1397\n",
      "Epoch 739: avg loss training: 2.8265,... Gradient Norm: 0.1773\n",
      "Epoch 740: avg loss training: 2.8265,... Gradient Norm: 0.1416\n",
      "Epoch 741: avg loss training: 2.8265,... Gradient Norm: 0.2977\n",
      "Epoch 742: avg loss training: 2.8265,... Gradient Norm: 0.2922\n",
      "Epoch 743: avg loss training: 2.8265,... Gradient Norm: 0.3517\n",
      "Epoch 744: avg loss training: 2.8265,... Gradient Norm: 0.1469\n",
      "Epoch 745: avg loss training: 2.8265,... Gradient Norm: 0.1598\n",
      "Epoch 746: avg loss training: 2.8265,... Gradient Norm: 0.3000\n",
      "Epoch 747: avg loss training: 2.8265,... Gradient Norm: 0.1363\n",
      "Epoch 748: avg loss training: 2.8265,... Gradient Norm: 0.1640\n",
      "Epoch 749: avg loss training: 2.8265,... Gradient Norm: 0.1092\n",
      "Epoch 750: avg loss training: 2.8265,... Gradient Norm: 0.2866\n",
      "Epoch 751: avg loss training: 2.8265,... Gradient Norm: 0.2829\n",
      "Epoch 752: avg loss training: 2.8265,... Gradient Norm: 0.1409\n",
      "Epoch 753: avg loss training: 2.8265,... Gradient Norm: 0.1826\n",
      "Epoch 754: avg loss training: 2.8265,... Gradient Norm: 0.1358\n",
      "Epoch 755: avg loss training: 2.8265,... Gradient Norm: 0.1166\n",
      "Epoch 756: avg loss training: 2.8265,... Gradient Norm: 0.1835\n",
      "Epoch 757: avg loss training: 2.8265,... Gradient Norm: 0.1046\n",
      "Epoch 758: avg loss training: 2.8265,... Gradient Norm: 0.3242\n",
      "Epoch 759: avg loss training: 2.8265,... Gradient Norm: 0.0857\n",
      "Epoch 760: avg loss training: 2.8265,... Gradient Norm: 0.1874\n",
      "Epoch 761: avg loss training: 2.8265,... Gradient Norm: 0.1241\n",
      "Epoch 762: avg loss training: 2.8265,... Gradient Norm: 0.1319\n",
      "Epoch 763: avg loss training: 2.8265,... Gradient Norm: 0.1069\n",
      "Epoch 764: avg loss training: 2.8265,... Gradient Norm: 0.1784\n",
      "Epoch 765: avg loss training: 2.8265,... Gradient Norm: 0.2959\n",
      "Epoch 766: avg loss training: 2.8265,... Gradient Norm: 0.1510\n",
      "Epoch 767: avg loss training: 2.8265,... Gradient Norm: 0.1368\n",
      "Epoch 768: avg loss training: 2.8265,... Gradient Norm: 0.2063\n",
      "Epoch 769: avg loss training: 2.8265,... Gradient Norm: 0.1853\n",
      "Epoch 770: avg loss training: 2.8265,... Gradient Norm: 0.3209\n",
      "Epoch 771: avg loss training: 2.8265,... Gradient Norm: 0.3628\n",
      "Epoch 772: avg loss training: 2.8265,... Gradient Norm: 0.2690\n",
      "Epoch 773: avg loss training: 2.8265,... Gradient Norm: 0.1865\n",
      "Epoch 774: avg loss training: 2.8265,... Gradient Norm: 0.1501\n",
      "Epoch 775: avg loss training: 2.8265,... Gradient Norm: 0.1572\n",
      "Epoch 776: avg loss training: 2.8265,... Gradient Norm: 0.1140\n",
      "Epoch 777: avg loss training: 2.8265,... Gradient Norm: 0.2816\n",
      "Epoch 778: avg loss training: 2.8265,... Gradient Norm: 0.3750\n",
      "Epoch 779: avg loss training: 2.8265,... Gradient Norm: 0.3218\n",
      "Epoch 780: avg loss training: 2.8265,... Gradient Norm: 0.2701\n",
      "Epoch 781: avg loss training: 2.8265,... Gradient Norm: 0.1392\n",
      "Epoch 782: avg loss training: 2.8265,... Gradient Norm: 0.1406\n",
      "Epoch 783: avg loss training: 2.8265,... Gradient Norm: 0.1141\n",
      "Epoch 784: avg loss training: 2.8265,... Gradient Norm: 0.3179\n",
      "Epoch 785: avg loss training: 2.8265,... Gradient Norm: 0.0704\n",
      "Epoch 786: avg loss training: 2.8265,... Gradient Norm: 0.4011\n",
      "Epoch 787: avg loss training: 2.8265,... Gradient Norm: 0.3322\n",
      "Epoch 788: avg loss training: 2.8265,... Gradient Norm: 0.2359\n",
      "Epoch 789: avg loss training: 2.8265,... Gradient Norm: 0.2703\n",
      "Epoch 790: avg loss training: 2.8265,... Gradient Norm: 0.1374\n",
      "Epoch 791: avg loss training: 2.8265,... Gradient Norm: 0.1379\n",
      "Epoch 792: avg loss training: 2.8265,... Gradient Norm: 0.1351\n",
      "Epoch 793: avg loss training: 2.8265,... Gradient Norm: 0.1250\n",
      "Epoch 794: avg loss training: 2.8265,... Gradient Norm: 0.2183\n",
      "Epoch 795: avg loss training: 2.8265,... Gradient Norm: 0.3001\n",
      "Epoch 796: avg loss training: 2.8265,... Gradient Norm: 0.3954\n",
      "Epoch 797: avg loss training: 2.8265,... Gradient Norm: 0.1393\n",
      "Epoch 798: avg loss training: 2.8265,... Gradient Norm: 0.1929\n",
      "Epoch 799: avg loss training: 2.8265,... Gradient Norm: 0.2422\n",
      "Epoch 800: avg loss training: 2.8265,... Gradient Norm: 0.1267\n",
      "Epoch 801: avg loss training: 2.8265,... Gradient Norm: 0.1281\n",
      "Epoch 802: avg loss training: 2.8265,... Gradient Norm: 0.1279\n",
      "Epoch 803: avg loss training: 2.8265,... Gradient Norm: 0.1735\n",
      "Epoch 804: avg loss training: 2.8265,... Gradient Norm: 0.3537\n",
      "Epoch 805: avg loss training: 2.8265,... Gradient Norm: 0.3012\n",
      "Epoch 806: avg loss training: 2.8265,... Gradient Norm: 0.2806\n",
      "Epoch 807: avg loss training: 2.8265,... Gradient Norm: 0.1350\n",
      "Epoch 808: avg loss training: 2.8265,... Gradient Norm: 0.1334\n",
      "Epoch 809: avg loss training: 2.8265,... Gradient Norm: 0.1224\n",
      "Epoch 810: avg loss training: 2.8265,... Gradient Norm: 0.2796\n",
      "Epoch 811: avg loss training: 2.8265,... Gradient Norm: 0.3067\n",
      "Epoch 812: avg loss training: 2.8265,... Gradient Norm: 0.3793\n",
      "Epoch 813: avg loss training: 2.8265,... Gradient Norm: 0.1163\n",
      "Epoch 814: avg loss training: 2.8265,... Gradient Norm: 0.2874\n",
      "Epoch 815: avg loss training: 2.8265,... Gradient Norm: 0.1443\n",
      "Epoch 816: avg loss training: 2.8265,... Gradient Norm: 0.1367\n",
      "Epoch 817: avg loss training: 2.8265,... Gradient Norm: 0.1217\n",
      "Epoch 818: avg loss training: 2.8265,... Gradient Norm: 0.2570\n",
      "Epoch 819: avg loss training: 2.8265,... Gradient Norm: 0.1038\n",
      "Epoch 820: avg loss training: 2.8265,... Gradient Norm: 0.4028\n",
      "Epoch 821: avg loss training: 2.8265,... Gradient Norm: 0.3061\n",
      "Epoch 822: avg loss training: 2.8265,... Gradient Norm: 0.2673\n",
      "Epoch 823: avg loss training: 2.8265,... Gradient Norm: 0.1687\n",
      "Epoch 824: avg loss training: 2.8265,... Gradient Norm: 0.1464\n",
      "Epoch 825: avg loss training: 2.8265,... Gradient Norm: 0.1161\n",
      "Epoch 826: avg loss training: 2.8265,... Gradient Norm: 0.1583\n",
      "Epoch 827: avg loss training: 2.8265,... Gradient Norm: 0.0822\n",
      "Epoch 828: avg loss training: 2.8265,... Gradient Norm: 0.3052\n",
      "Epoch 829: avg loss training: 2.8265,... Gradient Norm: 0.1085\n",
      "Epoch 830: avg loss training: 2.8265,... Gradient Norm: 0.2041\n",
      "Epoch 831: avg loss training: 2.8265,... Gradient Norm: 0.1400\n",
      "Epoch 832: avg loss training: 2.8265,... Gradient Norm: 0.1415\n",
      "Epoch 833: avg loss training: 2.8265,... Gradient Norm: 0.1526\n",
      "Epoch 834: avg loss training: 2.8265,... Gradient Norm: 0.2704\n",
      "Epoch 835: avg loss training: 2.8265,... Gradient Norm: 0.3679\n",
      "Epoch 836: avg loss training: 2.8265,... Gradient Norm: 0.1462\n",
      "Epoch 837: avg loss training: 2.8265,... Gradient Norm: 0.2268\n",
      "Epoch 838: avg loss training: 2.8265,... Gradient Norm: 0.1429\n",
      "Epoch 839: avg loss training: 2.8265,... Gradient Norm: 0.1022\n",
      "Epoch 840: avg loss training: 2.8265,... Gradient Norm: 0.1656\n",
      "Epoch 841: avg loss training: 2.8265,... Gradient Norm: 0.1617\n",
      "Epoch 842: avg loss training: 2.8265,... Gradient Norm: 0.1332\n",
      "Epoch 843: avg loss training: 2.8265,... Gradient Norm: 0.0911\n",
      "Epoch 844: avg loss training: 2.8265,... Gradient Norm: 0.2837\n",
      "Epoch 845: avg loss training: 2.8265,... Gradient Norm: 0.2553\n",
      "Epoch 846: avg loss training: 2.8265,... Gradient Norm: 0.1795\n",
      "Epoch 847: avg loss training: 2.8265,... Gradient Norm: 0.1464\n",
      "Epoch 848: avg loss training: 2.8265,... Gradient Norm: 0.1159\n",
      "Epoch 849: avg loss training: 2.8265,... Gradient Norm: 0.1142\n",
      "Epoch 850: avg loss training: 2.8265,... Gradient Norm: 0.1221\n",
      "Epoch 851: avg loss training: 2.8265,... Gradient Norm: 0.3503\n",
      "Epoch 852: avg loss training: 2.8265,... Gradient Norm: 0.1843\n",
      "Epoch 853: avg loss training: 2.8265,... Gradient Norm: 0.1527\n",
      "Epoch 854: avg loss training: 2.8265,... Gradient Norm: 0.1094\n",
      "Epoch 855: avg loss training: 2.8265,... Gradient Norm: 0.1043\n",
      "Epoch 856: avg loss training: 2.8265,... Gradient Norm: 0.2653\n",
      "Epoch 857: avg loss training: 2.8265,... Gradient Norm: 0.1848\n",
      "Epoch 858: avg loss training: 2.8265,... Gradient Norm: 0.1575\n",
      "Epoch 859: avg loss training: 2.8265,... Gradient Norm: 0.1700\n",
      "Epoch 860: avg loss training: 2.8265,... Gradient Norm: 0.1619\n",
      "Epoch 861: avg loss training: 2.8265,... Gradient Norm: 0.1681\n",
      "Epoch 862: avg loss training: 2.8265,... Gradient Norm: 0.2902\n",
      "Epoch 863: avg loss training: 2.8265,... Gradient Norm: 0.2745\n",
      "Epoch 864: avg loss training: 2.8265,... Gradient Norm: 0.1974\n",
      "Epoch 865: avg loss training: 2.8265,... Gradient Norm: 0.1645\n",
      "Epoch 866: avg loss training: 2.8265,... Gradient Norm: 0.1390\n",
      "Epoch 867: avg loss training: 2.8265,... Gradient Norm: 0.1564\n",
      "Epoch 868: avg loss training: 2.8265,... Gradient Norm: 0.1172\n",
      "Epoch 869: avg loss training: 2.8265,... Gradient Norm: 0.1680\n",
      "Epoch 870: avg loss training: 2.8265,... Gradient Norm: 0.2633\n",
      "Epoch 871: avg loss training: 2.8265,... Gradient Norm: 0.1097\n",
      "Epoch 872: avg loss training: 2.8265,... Gradient Norm: 0.1337\n",
      "Epoch 873: avg loss training: 2.8265,... Gradient Norm: 0.1510\n",
      "Epoch 874: avg loss training: 2.8265,... Gradient Norm: 0.1038\n",
      "Epoch 875: avg loss training: 2.8265,... Gradient Norm: 0.0934\n",
      "Epoch 876: avg loss training: 2.8265,... Gradient Norm: 0.3147\n",
      "Epoch 877: avg loss training: 2.8265,... Gradient Norm: 0.1298\n",
      "Epoch 878: avg loss training: 2.8265,... Gradient Norm: 0.1171\n",
      "Epoch 879: avg loss training: 2.8265,... Gradient Norm: 0.1233\n",
      "Epoch 880: avg loss training: 2.8265,... Gradient Norm: 0.1324\n",
      "Epoch 881: avg loss training: 2.8265,... Gradient Norm: 0.1157\n",
      "Epoch 882: avg loss training: 2.8265,... Gradient Norm: 0.2660\n",
      "Epoch 883: avg loss training: 2.8265,... Gradient Norm: 0.2269\n",
      "Epoch 884: avg loss training: 2.8265,... Gradient Norm: 0.1599\n",
      "Epoch 885: avg loss training: 2.8265,... Gradient Norm: 0.2911\n",
      "Epoch 886: avg loss training: 2.8265,... Gradient Norm: 0.1497\n",
      "Epoch 887: avg loss training: 2.8265,... Gradient Norm: 0.3304\n",
      "Epoch 888: avg loss training: 2.8265,... Gradient Norm: 0.1992\n",
      "Epoch 889: avg loss training: 2.8265,... Gradient Norm: 0.2593\n",
      "Epoch 890: avg loss training: 2.8265,... Gradient Norm: 0.1396\n",
      "Epoch 891: avg loss training: 2.8265,... Gradient Norm: 0.3599\n",
      "Epoch 892: avg loss training: 2.8265,... Gradient Norm: 0.1314\n",
      "Epoch 893: avg loss training: 2.8265,... Gradient Norm: 0.1881\n",
      "Epoch 894: avg loss training: 2.8265,... Gradient Norm: 0.2427\n",
      "Epoch 895: avg loss training: 2.8265,... Gradient Norm: 0.0900\n",
      "Epoch 896: avg loss training: 2.8265,... Gradient Norm: 0.3487\n",
      "Epoch 897: avg loss training: 2.8265,... Gradient Norm: 0.1138\n",
      "Epoch 898: avg loss training: 2.8265,... Gradient Norm: 0.1432\n",
      "Epoch 899: avg loss training: 2.8265,... Gradient Norm: 0.1414\n",
      "Epoch 900: avg loss training: 2.8265,... Gradient Norm: 0.1063\n",
      "Epoch 901: avg loss training: 2.8265,... Gradient Norm: 0.2662\n",
      "Epoch 902: avg loss training: 2.8265,... Gradient Norm: 0.0923\n",
      "Epoch 903: avg loss training: 2.8265,... Gradient Norm: 0.1299\n",
      "Epoch 904: avg loss training: 2.8265,... Gradient Norm: 0.1177\n",
      "Epoch 905: avg loss training: 2.8265,... Gradient Norm: 0.1945\n",
      "Epoch 906: avg loss training: 2.8265,... Gradient Norm: 0.3391\n",
      "Epoch 907: avg loss training: 2.8265,... Gradient Norm: 0.1350\n",
      "Epoch 908: avg loss training: 2.8265,... Gradient Norm: 0.2012\n",
      "Epoch 909: avg loss training: 2.8265,... Gradient Norm: 0.1411\n",
      "Epoch 910: avg loss training: 2.8265,... Gradient Norm: 0.1346\n",
      "Epoch 911: avg loss training: 2.8265,... Gradient Norm: 0.2993\n",
      "Epoch 912: avg loss training: 2.8265,... Gradient Norm: 0.2554\n",
      "Epoch 913: avg loss training: 2.8265,... Gradient Norm: 0.0908\n",
      "Epoch 914: avg loss training: 2.8265,... Gradient Norm: 0.2632\n",
      "Epoch 915: avg loss training: 2.8265,... Gradient Norm: 0.1184\n",
      "Epoch 916: avg loss training: 2.8265,... Gradient Norm: 0.2608\n",
      "Epoch 917: avg loss training: 2.8265,... Gradient Norm: 0.1467\n",
      "Epoch 918: avg loss training: 2.8265,... Gradient Norm: 0.1194\n",
      "Epoch 919: avg loss training: 2.8265,... Gradient Norm: 0.1841\n",
      "Epoch 920: avg loss training: 2.8265,... Gradient Norm: 0.3201\n",
      "Epoch 921: avg loss training: 2.8265,... Gradient Norm: 0.2776\n",
      "Epoch 922: avg loss training: 2.8265,... Gradient Norm: 0.2876\n",
      "Epoch 923: avg loss training: 2.8265,... Gradient Norm: 0.1337\n",
      "Epoch 924: avg loss training: 2.8265,... Gradient Norm: 0.1585\n",
      "Epoch 925: avg loss training: 2.8265,... Gradient Norm: 0.1015\n",
      "Epoch 926: avg loss training: 2.8265,... Gradient Norm: 0.2640\n",
      "Epoch 927: avg loss training: 2.8265,... Gradient Norm: 0.1375\n",
      "Epoch 928: avg loss training: 2.8265,... Gradient Norm: 0.1169\n",
      "Epoch 929: avg loss training: 2.8265,... Gradient Norm: 0.2841\n",
      "Epoch 930: avg loss training: 2.8265,... Gradient Norm: 0.2686\n",
      "Epoch 931: avg loss training: 2.8265,... Gradient Norm: 0.1780\n",
      "Epoch 932: avg loss training: 2.8265,... Gradient Norm: 0.1574\n",
      "Epoch 933: avg loss training: 2.8265,... Gradient Norm: 0.1512\n",
      "Epoch 934: avg loss training: 2.8265,... Gradient Norm: 0.1084\n",
      "Epoch 935: avg loss training: 2.8265,... Gradient Norm: 0.1250\n",
      "Epoch 936: avg loss training: 2.8265,... Gradient Norm: 0.1661\n",
      "Epoch 937: avg loss training: 2.8265,... Gradient Norm: 0.3667\n",
      "Epoch 938: avg loss training: 2.8265,... Gradient Norm: 0.1280\n",
      "Epoch 939: avg loss training: 2.8265,... Gradient Norm: 0.2567\n",
      "Epoch 940: avg loss training: 2.8265,... Gradient Norm: 0.1070\n",
      "Epoch 941: avg loss training: 2.8265,... Gradient Norm: 0.2726\n",
      "Epoch 942: avg loss training: 2.8265,... Gradient Norm: 0.1216\n",
      "Epoch 943: avg loss training: 2.8265,... Gradient Norm: 0.2831\n",
      "Epoch 944: avg loss training: 2.8265,... Gradient Norm: 0.1380\n",
      "Epoch 945: avg loss training: 2.8265,... Gradient Norm: 0.1395\n",
      "Epoch 946: avg loss training: 2.8265,... Gradient Norm: 0.3673\n",
      "Epoch 947: avg loss training: 2.8265,... Gradient Norm: 0.1224\n",
      "Epoch 948: avg loss training: 2.8265,... Gradient Norm: 0.2544\n",
      "Epoch 949: avg loss training: 2.8265,... Gradient Norm: 0.1801\n",
      "Epoch 950: avg loss training: 2.8265,... Gradient Norm: 0.4085\n",
      "Epoch 951: avg loss training: 2.8265,... Gradient Norm: 0.1610\n",
      "Epoch 952: avg loss training: 2.8265,... Gradient Norm: 0.1774\n",
      "Epoch 953: avg loss training: 2.8265,... Gradient Norm: 0.2775\n",
      "Epoch 954: avg loss training: 2.8265,... Gradient Norm: 0.1127\n",
      "Epoch 955: avg loss training: 2.8265,... Gradient Norm: 0.1152\n",
      "Epoch 956: avg loss training: 2.8265,... Gradient Norm: 0.3004\n",
      "Epoch 957: avg loss training: 2.8265,... Gradient Norm: 0.2533\n",
      "Epoch 958: avg loss training: 2.8265,... Gradient Norm: 0.1326\n",
      "Epoch 959: avg loss training: 2.8265,... Gradient Norm: 0.1376\n",
      "Epoch 960: avg loss training: 2.8265,... Gradient Norm: 0.1298\n",
      "Epoch 961: avg loss training: 2.8265,... Gradient Norm: 0.2080\n",
      "Epoch 962: avg loss training: 2.8265,... Gradient Norm: 0.1662\n",
      "Epoch 963: avg loss training: 2.8265,... Gradient Norm: 0.1403\n",
      "Epoch 964: avg loss training: 2.8265,... Gradient Norm: 0.3850\n",
      "Epoch 965: avg loss training: 2.8265,... Gradient Norm: 0.2815\n",
      "Epoch 966: avg loss training: 2.8265,... Gradient Norm: 0.2187\n",
      "Epoch 967: avg loss training: 2.8265,... Gradient Norm: 0.1195\n",
      "Epoch 968: avg loss training: 2.8265,... Gradient Norm: 0.1260\n",
      "Epoch 969: avg loss training: 2.8265,... Gradient Norm: 0.1570\n",
      "Epoch 970: avg loss training: 2.8265,... Gradient Norm: 0.1309\n",
      "Epoch 971: avg loss training: 2.8265,... Gradient Norm: 0.3079\n",
      "Epoch 972: avg loss training: 2.8265,... Gradient Norm: 0.0930\n",
      "Epoch 973: avg loss training: 2.8265,... Gradient Norm: 0.3704\n",
      "Epoch 974: avg loss training: 2.8265,... Gradient Norm: 0.3596\n",
      "Epoch 975: avg loss training: 2.8265,... Gradient Norm: 0.2071\n",
      "Epoch 976: avg loss training: 2.8265,... Gradient Norm: 0.2175\n",
      "Epoch 977: avg loss training: 2.8265,... Gradient Norm: 0.1459\n",
      "Epoch 978: avg loss training: 2.8265,... Gradient Norm: 0.1247\n",
      "Epoch 979: avg loss training: 2.8265,... Gradient Norm: 0.1187\n",
      "Epoch 980: avg loss training: 2.8265,... Gradient Norm: 0.0638\n",
      "Epoch 981: avg loss training: 2.8265,... Gradient Norm: 0.2783\n",
      "Epoch 982: avg loss training: 2.8265,... Gradient Norm: 0.2742\n",
      "Epoch 983: avg loss training: 2.8265,... Gradient Norm: 0.1188\n",
      "Epoch 984: avg loss training: 2.8265,... Gradient Norm: 0.1241\n",
      "Epoch 985: avg loss training: 2.8265,... Gradient Norm: 0.1488\n",
      "Epoch 986: avg loss training: 2.8265,... Gradient Norm: 0.2504\n",
      "Epoch 987: avg loss training: 2.8265,... Gradient Norm: 0.1409\n",
      "Epoch 988: avg loss training: 2.8265,... Gradient Norm: 0.1313\n",
      "Epoch 989: avg loss training: 2.8265,... Gradient Norm: 0.3303\n",
      "Epoch 990: avg loss training: 2.8265,... Gradient Norm: 0.1352\n",
      "Epoch 991: avg loss training: 2.8265,... Gradient Norm: 0.0632\n",
      "Epoch 992: avg loss training: 2.8265,... Gradient Norm: 0.1023\n",
      "Epoch 993: avg loss training: 2.8265,... Gradient Norm: 0.1642\n",
      "Epoch 994: avg loss training: 2.8265,... Gradient Norm: 0.1661\n",
      "Epoch 995: avg loss training: 2.8265,... Gradient Norm: 0.1325\n",
      "Epoch 996: avg loss training: 2.8265,... Gradient Norm: 0.2774\n",
      "Epoch 997: avg loss training: 2.8265,... Gradient Norm: 0.1556\n",
      "Epoch 998: avg loss training: 2.8265,... Gradient Norm: 0.1357\n",
      "Epoch 999: avg loss training: 2.8265,... Gradient Norm: 0.1102\n",
      "Epoch 1000: avg loss training: 2.8265,... Gradient Norm: 0.2054\n",
      "final result: (0.007345430421993315, -0.08459115786536026) ********************\n"
     ]
    }
   ],
   "source": [
    "# here I use splines, not nn\n",
    "score_new, score_orig, model_x, model_y, new_x_f, new_x_r, f_forward, f_reverse= loci_w_marginal(\n",
    "    x, y, independence_test=False, neural_network=False, \n",
    "    return_function=True, n_steps=1000, marginal_loglik = True\n",
    ")\n",
    "\n",
    "print(f\"final result: {score_new, score_orig} ********************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2efb40",
   "metadata": {},
   "source": [
    "### Example of data transformation, x or y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "118d6482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAtcUlEQVR4nO3df1TUdb7H8dcMyi8RzEVBCUTTMNwEA2Hx3i29TeG1ulq3la3OgtR6uipdu2wnpb2J5haVZpR5w9rsx+56cnfTcsulK2y2m9FaGF3zB7v+xFQQrjdITFBm7h8dJif5NTDwYcbn45w5h/nO5/v9vmdE5+Xn+/l8PxaHw+EQAACAIVbTBQAAgEsbYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUQNMF9AVdrtdx48f1+DBg2WxWEyXAwAAusDhcOirr77SyJEjZbW23//hFWHk+PHjio6ONl0GAADohqNHj+ryyy9v93WvCCODBw+W9M2bCQ0NNVwNAADoioaGBkVHRzu/x9vjFWGk9dJMaGgoYQQAAC/T2RALBrACAACjCCMAAMAowggAADDKK8aMAACklpYWnTt3znQZgJOfn58GDBjQ49tuEEYAwAucPn1aX3zxhRwOh+lSABfBwcEaMWKE/P39u30MwggA9HMtLS364osvFBwcrGHDhnHzR/QLDodDzc3Nqq2t1aFDhzRu3LgOb2zWEcIIAPRz586dk8Ph0LBhwxQUFGS6HMApKChIAwcO1JEjR9Tc3KzAwMBuHYcBrADgJegRQX/U3d4Ql2N4oA4AAIBuI4wAAACjGDMCAF7KXrm7T89njZvQp+frDdXV1frJT36iDz/8UAMHDtSXX35puqQusVgs2rRpk2bNmmW6lF5BzwgAwOMsFkuHj6VLlxqp6+mnn9aJEydUUVGhv/3tb0Zq6A1z5sxxfrb+/v4aO3asHnnkEZ0/f16StG3bNufrVqtVYWFhmjRpkh588EGdOHHC5VhLly5t88+spKSk1+qnZwQA4HEXfsFt2LBBS5YsUWVlpXNbSEiI82eHw6GWlhYNGND7X0kHDhxQUlKSxo0b1+1jNDc39+ieGr1l+vTpevnll9XU1KQtW7ZowYIFGjhwoPLy8pxtKisrFRoaqoaGBu3cuVNPPvmkXnrpJW3btk1XX321s92ECRMuCh9Dhw7ttdrpGQEAeFxkZKTzERYWJovF4ny+b98+DR48WH/84x+VlJSkgIAAffDBBzpw4IBmzpypiIgIhYSEaPLkyRd9IcbGxuqxxx7T3XffrcGDBysmJkYvvPCC8/Xm5mbl5ORoxIgRCgwM1KhRo1RQUODc94033tBrr70mi8WiOXPmSJKqqqo0c+ZMhYSEKDQ0VLNnz1ZNTY3zmEuXLlViYqJ++ctfavTo0c7pqxaLRWvXrtXNN9+s4OBgXXXVVSorK9P+/fs1depUDRo0SFOmTNGBAwdc3sNbb72la665RoGBgRozZoyWLVvm7MGQpL///e+69tprFRgYqPj4eG3durVLn3lAQIAiIyM1atQozZs3TzabTZs3b3ZpM3z4cEVGRurKK6/Uj3/8Y23fvl3Dhg3TvHnzXNoNGDDA5c8wMjKyVwNYt8LImjVrFBsbq8DAQKWmpmrHjh0dtv/yyy+1YMECjRgxQgEBAbryyiu1ZcuWbhUMAJ5Wsqemwwd6x+LFi/X4449r7969mjhxok6fPq0ZM2aotLRUn376qaZPn65bbrlFVVVVLvs99dRTSk5O1qeffqr58+dr3rx5zl6XZ599Vps3b9Zvf/tbVVZW6je/+Y1iY2MlSR9//LGmT5+u2bNn68SJE3rmmWdkt9s1c+ZMnTp1Su+//762bt2qgwcPKiMjw+Wc+/fv1xtvvKGNGzeqoqLCuX358uXKzMxURUWFxo8frzvvvFP33nuv8vLy9Mknn8jhcCgnJ8fZ/i9/+YsyMzO1cOFC7dmzR2vXrtUrr7yiRx99VJJkt9t12223yd/fX3/9619VVFSkRYsWdevzDQoKUnNzc6dt/u3f/k3bt2/XyZMnu3UeT3C7T2zDhg3Kzc1VUVGRUlNTVVhYqPT0dFVWVmr48OEXtW9ubtYNN9yg4cOH6/e//72ioqJ05MgRDRkyxBP1AwC81COPPKIbbrjB+Xzo0KFKSEhwPl++fLk2bdqkzZs3u3yhz5gxQ/Pnz5ckLVq0SE8//bTee+89xcXFqaqqSuPGjdM//uM/ymKxaNSoUc79hg0bpoCAAAUFBSkyMlKStHXrVu3atUuHDh1SdHS0JOm1117ThAkT9PHHH2vy5MmSvvkue+211zRs2DCX95Cdna3Zs2c7a0lLS9PDDz+s9PR0SdLChQuVnZ3tbL9s2TItXrxYWVlZkqQxY8Zo+fLlevDBB5Wfn6+SkhLt27dP7777rkaOHClJeuyxx/TP//zPXf5cHQ6HSktL9e677+q+++7rtP348eMlSYcPH3Z+j+/atcvlUlp8fHynHQ894XYYWbVqlebOnev8cIuKivTOO+9o3bp1Wrx48UXt161bp1OnTjlHLktyplQAwKUrOTnZ5fnp06e1dOlSvfPOOzpx4oTOnz+vr7/++qKekYkTJzp/br380/q/+jlz5uiGG25QXFycpk+frptvvlk33nhjuzXs3btX0dHRziAiffPFO2TIEO3du9cZRkaNGnVREPluLREREZLkMvYiIiJCZ8+eVUNDg0JDQ/XZZ59p+/btzp4Q6Zvb/Z89e1Znzpxx1tMaRCQpLS2t3fov9PbbbyskJETnzp2T3W7XnXfe2aWBwq3rHV14U724uDiXSzwBAQFdqqG73Aojzc3NKi8vdxkMY7VaZbPZVFZW1uY+mzdvVlpamhYsWKC33npLw4YN05133qlFixbJz8+vzX2amprU1NTkfN7Q0OBOmQAALzBo0CCX5w888IC2bt2qlStXauzYsQoKCtLtt99+0aWG1v/YtrJYLLLb7ZKka665RocOHdIf//hHlZSUaPbs2bLZbPr973/v0VrbqqX1y7ytba31nT59WsuWLdNtt9120bG6eyv1VtOmTdPzzz8vf39/jRw5sssDgvfu3SvJtaOgdUZOX3ErjNTV1amlpcWZ/lpFRERo3759be5z8OBB/elPf9Jdd92lLVu2aP/+/Zo/f77OnTun/Pz8NvcpKCjQsmXL3CkNAODltm/frjlz5ujWW2+V9M0X9+HDh90+TmhoqDIyMpSRkaHbb79d06dP16lTp9qcDXLVVVfp6NGjOnr0qLN3ZM+ePfryyy8VHx/fo/fTlmuuuUaVlZXtftG31nPixAmNGDFCkvTRRx916diDBg1yO0B8/fXXeuGFF3Tttde22fPTV3p9HpXdbtfw4cP1wgsvyM/PT0lJSTp27JhWrFjRbhjJy8tTbm6u83lDQ4NLFxoAwPeMGzdOGzdu1C233CKLxaKHH37Y2aPQVatWrdKIESM0adIkWa1W/e53v1NkZGS74xRtNpuuvvpq3XXXXSosLNT58+c1f/58XXfddRddRvKEJUuW6Oabb1ZMTIxuv/12Wa1WffbZZ/r888/1i1/8QjabTVdeeaWysrK0YsUKNTQ06Oc//7nHzn/y5EmdPXtWX331lcrLy/Xkk0+qrq5OGzdu9Ng5usOtMBIeHi4/Pz+XKU+SVFNT4xwM9F0jRozQwIEDXS7JXHXVVaqurm53rnZAQECvX58CAG/nC3dEvdCqVat09913a8qUKQoPD9eiRYvcvkw/ePBgPfnkk/r73/8uPz8/TZ48WVu2bGl3MTeLxaK33npL9913n6699lpZrVZNnz5dq1ev9sRbukh6errefvttPfLII3riiSc0cOBAjR8/Xj/96U8lfTP0YdOmTbrnnnuUkpKi2NhYPfvss5o+fbpHzh8XFyeLxaKQkBCNGTNGN954o3Jzc9v9Du8rFkfryJUuSk1NVUpKivMPym63KyYmRjk5OW0OYH3ooYe0fv16HTx40PnL8Mwzz+iJJ57Q8ePHu3TOhoYGhYWFqb6+XqGhoe6UCwCd6mz6ri0+osPXe9vZs2d16NAhl3tcAP1FR7+fXf3+dvs+I7m5uXrxxRf16quvau/evZo3b54aGxuds2syMzNdBrjOmzdPp06d0sKFC/W3v/1N77zzjh577DEtWLDA3VMDAAAf5PaYkYyMDNXW1mrJkiWqrq5WYmKiiouLnYNaq6qqXLrDoqOj9e677+o//uM/NHHiREVFRWnhwoXdvokLAADwLW5fpjGByzQAehOXaYDuM3KZBgAAwJMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAACfM2fOHM2aNavTdhaLRW+++abHzhsbG6vCwkKPHe9S0etr0wAAekdnU5I9zd0pznPmzNGrr7560fb09HQVFxd7qqw2PfPMM+rKnStOnDihyy67rFdrudDSpUudC8H6+fnp8ssv16233qrly5crJCREhw8f1ujRo53tQ0JCFBMTo6lTp+r+++/XuHHjnK+98sorzhuOXujFF1903l7eWxBGAAC9Zvr06Xr55ZddtvXF2mNhYWEdvt66NpqJNVkmTJigkpISnT9/Xtu3b9fdd9+tM2fOaO3atc42JSUlmjBhgs6cOaNdu3bpmWeeUUJCgv7whz/o+uuvd7YLDQ1VZWWly/E7e+/9EZdpAAC9JiAgQJGRkS6PC3siLBaL1q5dq5tvvlnBwcG66qqrVFZWpv3792vq1KkaNGiQpkyZogMHDjj3Wbp0qRITE7V27VpFR0crODhYs2fPVn19vbPNdy/TTJ06VTk5Obr//vsVHh6u9PR05/kvvEzzxRdf6I477tDQoUM1aNAgJScn669//ask6cCBA5o5c6YiIiIUEhKiyZMnq6SkxO3PZMCAAYqMjNTll1+ujIwM3XXXXdq8ebNLm+9973uKjIzUmDFjNHPmTJWUlCg1NVX33HOPWlpaXD6/736+QUFBbtdkGmEEAGDU8uXLlZmZqYqKCo0fP1533nmn7r33XuXl5emTTz6Rw+FQTk6Oyz779+/Xb3/7W/3hD39QcXGxPv30U82fP7/D87z66qvy9/fX9u3bVVRUdNHrp0+f1nXXXadjx45p8+bN+uyzz/Tggw/Kbrc7X58xY4ZKS0v16aefavr06brllltUVVXVo/cfFBSk5ubmDttYrVYtXLhQR44cUXl5eY/O1x9xmQYA0GvefvtthYSEuGx76KGH9NBDDzmfZ2dna/bs2ZKkRYsWKS0tTQ8//LCz92LhwoUXjY04e/asXnvtNUVFRUmSVq9erZtuuklPPfVUu5dexo0bpyeffLLdWtevX6/a2lp9/PHHGjp0qCRp7NixztcTEhKUkJDgfL58+XJt2rRJmzdvvigsdVV5ebnWr1+vf/qnf+q07fjx4yVJhw8fVkpKiiSpvr7e5fMNCQlRdXV1t2oxiTACAOg106ZN0/PPP++yrfWLvtXEiROdP7cuunr11Ve7bDt79qwaGhqc65vExMQ4g4gkpaWlyW63q7Kyst0wkpSU1GGtFRUVmjRp0kX1tTp9+rSWLl2qd955RydOnND58+f19ddfu90zsmvXLoWEhKilpUXNzc266aab9Nxzz3W6X+uAXIvF4tw2ePBg7dy50/n8woVqvQlhBADQawYNGuTSu9CWgQMHOn9u/aJta1vr5ZKe1NKRzsZaPPDAA9q6datWrlypsWPHKigoSLfffnunl1i+Ky4uTps3b9aAAQM0cuRI+fv7d2m/vXv3SpLLbBur1drp5+sNvDNCAQAuaVVVVTp+/Ljz+UcffSSr1aq4uLhuH3PixImqqKjQqVOn2nx9+/btmjNnjm699VZdffXVioyM1OHDh90+j7+/v8aOHavY2NguBxG73a5nn31Wo0eP1qRJk9w+Z39HGAEA9JqmpiZVV1e7POrq6np83MDAQGVlZemzzz7TX/7yF/37v/+7Zs+e3aOpunfccYciIyM1a9Ysbd++XQcPHtQbb7yhsrIySd+MOdm4caMqKir02Wef6c477+xxb017/vd//1fV1dU6ePCgNm/eLJvNph07duill16Sn59fr5zTJC7TAAB6TXFxsUaMGOGyLS4uTvv27evRcceOHavbbrtNM2bM0KlTp3TzzTfrv/7rv3p0TH9/f/33f/+3fvazn2nGjBk6f/684uPjtWbNGknSqlWrdPfdd2vKlCkKDw/XokWL1NDQ0KNztsdms0mSgoODNWrUKE2bNk0vvPCCT1ySaYvF0ZVb1BnW0NCgsLAw1dfXOwcvAYCndHYnU3fvPOppZ8+e1aFDhzR69GgFBgYaraU/WLp0qd58801VVFSYLgXq+Pezq9/fXKYBAABGEUYAAIBRhBEAgFdZunQpl2h8DGEEAAAYRRgBAABGEUYAwEt4weRHXII88XtJGAGAfq71Jlfu3nYc6AtnzpyR5HoLf3dx0zMA6OcGDBig4OBg1dbWauDAgV67GBp8i8Ph0JkzZ3Ty5EkNGTKkR3eGJYwAQD9nsVg0YsQIHTp0SEeOHDFdDuBiyJAhPboNv0QYAQCv4O/vr3HjxnGpBv3KwIEDPbJWDmEEALyE1WrldvDwSVx4BAAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABg1ADTBQDwLHvl7k7bWOMm9EElANA19IwAAACjCCMAAMAowggAADCqW2FkzZo1io2NVWBgoFJTU7Vjx452277yyiuyWCwuj8DAwG4XDAAAfIvbYWTDhg3Kzc1Vfn6+du7cqYSEBKWnp+vkyZPt7hMaGqoTJ044H0eOHOlR0QAAwHe4PZtm1apVmjt3rrKzsyVJRUVFeuedd7Ru3TotXry4zX0sFosiIyN7VimMK9lT0+HrtviIPqoEAOBL3OoZaW5uVnl5uWw227cHsFpls9lUVlbW7n6nT5/WqFGjFB0drZkzZ2r37o6nHjY1NamhocHlAQAAfJNbYaSurk4tLS2KiHD9H3BERISqq6vb3CcuLk7r1q3TW2+9pV//+tey2+2aMmWKvvjii3bPU1BQoLCwMOcjOjranTIBAIAX6fXZNGlpacrMzFRiYqKuu+46bdy4UcOGDdPatWvb3ScvL0/19fXOx9GjR3u7TAAAYIhbY0bCw8Pl5+enmhrXsQM1NTVdHhMycOBATZo0Sfv372+3TUBAgAICAtwpDQAAeCm3woi/v7+SkpJUWlqqWbNmSZLsdrtKS0uVk5PTpWO0tLRo165dmjFjhtvFAvBN3MIeuLS5PZsmNzdXWVlZSk5OVkpKigoLC9XY2OicXZOZmamoqCgVFBRIkh555BH94Ac/0NixY/Xll19qxYoVOnLkiH7605969p0AAACv5HYYycjIUG1trZYsWaLq6molJiaquLjYOai1qqpKVuu3Q1H+7//+T3PnzlV1dbUuu+wyJSUl6cMPP1R8fLzn3gUAdKCzaekAzLI4HA6H6SI609DQoLCwMNXX1ys0NNR0OZcs7jPiHbzxkkdv19zTMMLvNtA9Xf3+Zm0aAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFFu34EVgPfjxmgA+hN6RgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABjF2jQAuo31YgB4Aj0jAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIrbwcOpZE+N6RKANpVWnenw9etjgvuoEgC9gZ4RAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUs2kA9Cp75W7TJQDo5wgjAIzrbOouAN/GZRoAAGAUYQQAABhFGAEAAEYRRgAAgFEMYAVwSXEcq+q0jSUqpg8qAdCKMALA67XOxrG0mFnssbNFJm3xEX1UCeCduEwDAACMIowAAACjuhVG1qxZo9jYWAUGBio1NVU7duzo0n6vv/66LBaLZs2a1Z3TAgAAH+R2GNmwYYNyc3OVn5+vnTt3KiEhQenp6Tp58mSH+x0+fFgPPPCAfvjDH3a7WAAA4HvcDiOrVq3S3LlzlZ2drfj4eBUVFSk4OFjr1q1rd5+WlhbdddddWrZsmcaMGdOjggEAgG9xK4w0NzervLxcNpvt2wNYrbLZbCorK2t3v0ceeUTDhw/XPffc06XzNDU1qaGhweUBAAB8k1tTe+vq6tTS0qKICNdpahEREdq3b1+b+3zwwQd66aWXVFFR0eXzFBQUaNmyZe6UBqAf8/aF8DqbugugZ3p1Ns1XX32ln/zkJ3rxxRcVHh7e5f3y8vJUX1/vfBw9erQXqwQAACa51TMSHh4uPz8/1dS4/i+hpqZGkZGRF7U/cOCADh8+rFtuucW5zW63f3PiAQNUWVmpK6644qL9AgICFBAQ4E5pAADAS7nVM+Lv76+kpCSVlpY6t9ntdpWWliotLe2i9uPHj9euXbtUUVHhfPzLv/yLpk2bpoqKCkVHR/f8HQAAAK/m9u3gc3NzlZWVpeTkZKWkpKiwsFCNjY3Kzs6WJGVmZioqKkoFBQUKDAzU97//fZf9hwwZIkkXbQcAAJcmt8NIRkaGamtrtWTJElVXVysxMVHFxcXOQa1VVVWyWrmxKwAA6JpuLZSXk5OjnJycNl/btm1bh/u+8sor3TklAPgsFtrDpY4uDAAAYBRhBAAAGEUYAQAARnVrzAgAz7NX7u60jTVuQh9U8o2u1AMAnkDPCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIzidvCAF+EW7QB8ET0jAADAKMIIAAAwijACAACMIowAAACjGMCKS0bJnpoOX7fFR/RRJQCACxFGgB7qygwXa9yEPqgEnuI4VtVpG0tUTB9UAlwauEwDAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKNamAeAzurKmDID+hzCCPsOquQCAtnCZBgAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYxWwaAD1WWnXGdAkAvBg9IwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACM6lYYWbNmjWJjYxUYGKjU1FTt2LGj3bYbN25UcnKyhgwZokGDBikxMVG/+tWvul0wAADwLW6HkQ0bNig3N1f5+fnauXOnEhISlJ6erpMnT7bZfujQofr5z3+usrIy/c///I+ys7OVnZ2td999t8fFAwAA7+d2GFm1apXmzp2r7OxsxcfHq6ioSMHBwVq3bl2b7adOnapbb71VV111la644gotXLhQEydO1AcffNDj4gEAgPdz63bwzc3NKi8vV15ennOb1WqVzWZTWVlZp/s7HA796U9/UmVlpZ544ol22zU1Nampqcn5vKGhwZ0ygX7HXrnbdAkA0G+51TNSV1enlpYWRUREuGyPiIhQdXV1u/vV19crJCRE/v7+uummm7R69WrdcMMN7bYvKChQWFiY8xEdHe1OmQAAwIv0yWyawYMHq6KiQh9//LEeffRR5ebmatu2be22z8vLU319vfNx9OjRvigTAAAY4NZlmvDwcPn5+ammpsZle01NjSIjI9vdz2q1auzYsZKkxMRE7d27VwUFBZo6dWqb7QMCAhQQEOBOaegHSvbUdN4IgNt6+nfLFh/ReSPAILd6Rvz9/ZWUlKTS0lLnNrvdrtLSUqWlpXX5OHa73WVMCAAAuHS51TMiSbm5ucrKylJycrJSUlJUWFioxsZGZWdnS5IyMzMVFRWlgoICSd+M/0hOTtYVV1yhpqYmbdmyRb/61a/0/PPPe/adAAAAr+R2GMnIyFBtba2WLFmi6upqJSYmqri42DmotaqqSlbrtx0ujY2Nmj9/vr744gsFBQVp/Pjx+vWvf62MjAzPvQsAAOC1LA6Hw2G6iM40NDQoLCxM9fX1Cg0NNV2OzzI95qO3r2t39v66e36m7UqlVWdMl9DnLFExXW7b2e9Wb//dY8wITOnq9zdr0wAAAKPcvkwDwPd01rNxfUxwH1UC4FJEzwgAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjGJqL/qN3ropGWCa6RsKAv0dYQQAusFxrKrTNu7cpRW4lHGZBgAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYxWwaAOglzLgBuoaeEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEaxNg0AGMT6NQA9IwAAwDDCCAAAMIowAgAAjCKMAAAAowgjAADAKGbTAB2wV+42XQIA+DzCCHxGyZ4a0yX4rNKqM6ZLAODDuEwDAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKOY2nsJYeqrWT2dHnt9TLCHKgGA/oWeEQAAYBRhBAAAGMVlGgDwAY5jVe2+ZverkyRZ4yb0VTmAW+gZAQAARhFGAACAUd0KI2vWrFFsbKwCAwOVmpqqHTt2tNv2xRdf1A9/+ENddtlluuyyy2Sz2TpsDwAALi1ujxnZsGGDcnNzVVRUpNTUVBUWFio9PV2VlZUaPnz4Re23bdumO+64Q1OmTFFgYKCeeOIJ3Xjjjdq9e7eioqI88iaAS0FnU4OZ+gtTOrttgC0+oo8qgbdyu2dk1apVmjt3rrKzsxUfH6+ioiIFBwdr3bp1bbb/zW9+o/nz5ysxMVHjx4/XL3/5S9ntdpWWlva4eAAA4P3c6hlpbm5WeXm58vLynNusVqtsNpvKysq6dIwzZ87o3LlzGjp0aLttmpqa1NTU5Hze0NDgTpkA4FM6minjDnvl7k7bMOMGJrjVM1JXV6eWlhZFRLh2uUVERKi6urpLx1i0aJFGjhwpm83WbpuCggKFhYU5H9HR0e6UCQAAvEifzqZ5/PHH9frrr2vTpk0KDAxst11eXp7q6+udj6NHj/ZhlQAAoC+5dZkmPDxcfn5+qqlxHaxUU1OjyMjIDvdduXKlHn/8cZWUlGjixIkdtg0ICFBAQIA7pQEAAC/lVs+Iv7+/kpKSXAaftg5GTUtLa3e/J598UsuXL1dxcbGSk5O7Xy0AAPA5bk/tzc3NVVZWlpKTk5WSkqLCwkI1NjYqOztbkpSZmamoqCgVFBRIkp544gktWbJE69evV2xsrHNsSUhIiEJCQjz4VgAAgDdyO4xkZGSotrZWS5YsUXV1tRITE1VcXOwc1FpVVSWr9dsOl+eff17Nzc26/fbbXY6Tn5+vpUuX9qx6oAe+O0Ohdf0OAEDf6tZCeTk5OcrJyWnztW3btrk8P3z4cHdOAQAALhGsTQMAAIwijAAAAKMIIwAAwKhujRkBfFFnC9F1xvRCdT2tH76LRRbR3xFG0G90tv6G3a+uy+tmeGotDwBA7+MyDQAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwitk0AHCJu3Dqr6Wl5qLXbfERfVkOLkH0jAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAo5hNgx7ryjowlqiYPqgEQH9x4b8Ldr+6Ntt0da0p+D56RgAAgFGEEQAAYBRhBAAAGEUYAQAARjGAFX2iK4Ncu8JeubuDc5xp9zUAXdPW39XvDkBl4Ck8jZ4RAABgFD0j8BoXLubVH/X3+gCgv6JnBAAAGEUYAQAARhFGAACAUYwZAQD4vI5m4rVilpA59IwAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKOYTXMJ68p6MZaomD6oBIA3+e7MFNaFQk/RMwIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAo1qZBh7qyfg0AAD1BzwgAADCqW2FkzZo1io2NVWBgoFJTU7Vjx4522+7evVv/+q//qtjYWFksFhUWFna3VgAA4IPcDiMbNmxQbm6u8vPztXPnTiUkJCg9PV0nT55ss/2ZM2c0ZswYPf7444qMjOxxwQAAwLe4HUZWrVqluXPnKjs7W/Hx8SoqKlJwcLDWrVvXZvvJkydrxYoV+vGPf6yAgIAeFwwAAHyLWwNYm5ubVV5erry8POc2q9Uqm82msrIyjxXV1NSkpqYm5/OGhgaPHRsA4J7SqjO9sr+lpUaSZIuP6NHx4f3cCiN1dXVqaWlRRITrL05ERIT27dvnsaIKCgq0bNkyjx3P19grd3faxho3oQ8qAQCg5/rlbJq8vDzV19c7H0ePHjVdEgAA6CVu9YyEh4fLz89PNTU1Lttramo8Ojg1ICCA8SUAAFwi3OoZ8ff3V1JSkkpLS53b7Ha7SktLlZaW5vHiAACA73P7Dqy5ubnKyspScnKyUlJSVFhYqMbGRmVnZ0uSMjMzFRUVpYKCAknfDHrds2eP8+djx46poqJCISEhGjt2rAffCgAA8EZuh5GMjAzV1tZqyZIlqq6uVmJiooqLi52DWquqqmS1ftvhcvz4cU2aNMn5fOXKlVq5cqWuu+46bdu2refvAACAbrpwpk/r7J4L9XSmT8mei4/pyeP7im6tTZOTk6OcnJw2X/tuwIiNjZXD4ejOaS5JXZkpAwC+oHXtK7tfXY+Ow+xB79cvZ9MAAIBLB2EEAAAYRRgBAABGEUYAAIBR3RrAiu5hcCoAABcjjAAAfFbr1FrHsZ4t9ofexWUaAABgFGEEAAAYRRgBAABGEUYAAIBRDGAFAHi1jmYqMnDVOxBGfEhnCz4BANAfcZkGAAAYRRgBAABGEUYAAIBRhBEAAGAUA1i7oCtryljjJvRBJV3nOFZlugQAALqEnhEAAGAUPSMAAKMuvC1BW66PCe72vvAO9IwAAACjCCMAAMAowggAADCKMSMAAPSx1hmPdr+6dtv0t1mavYmeEQAAYBRhBAAAGMVlGgBAv2Zy+m7Jno5XQLfFR/j0+fsKPSMAAMAowggAADCKyzQAAHTRd9f9ams2zKU0C8ZT6BkBAABGEUYAAIBRXKYBAKCb2prpY2npeAZMZ/t35zjejp4RAABgFGEEAAAYxWUaD7FX7jZdAgCgB747U8YbdOW7xxtm99AzAgAAjCKMAAAAowgjAADAKMaMAADgpTpbRPD6mOA+qqRn6BkBAABGXfI9I8yCAQB4Un+bleMNM27oGQEAAEYRRgAAgFGEEQAAYBRhBAAAGHXJD2AFAKA/8sRAWG+Z+kvPCAAAMKpbYWTNmjWKjY1VYGCgUlNTtWPHjg7b/+53v9P48eMVGBioq6++Wlu2bOlWsQAAwPe4HUY2bNig3Nxc5efna+fOnUpISFB6erpOnjzZZvsPP/xQd9xxh+655x59+umnmjVrlmbNmqXPP/+8x8UDAADvZ3E4HA53dkhNTdXkyZP13HPPSZLsdruio6N13333afHixRe1z8jIUGNjo95++23nth/84AdKTExUUVFRl87Z0NCgsLAw1dfXKzQ01J1yO+VLNz3r7NogAAAXah0z0ls3Pevq97dbA1ibm5tVXl6uvLw85zar1SqbzaaysrI29ykrK1Nubq7LtvT0dL355pvtnqepqUlNTU3O5/X19ZK+eVOeZj992uPHNKWxkTACAOi6htN2SZK1F75fpW+/tzvr93ArjNTV1amlpUUREREu2yMiIrRv374296murm6zfXV1dbvnKSgo0LJlyy7aHh0d7U65AACgH/jqq68UFhbW7uv9cmpvXl6eS2+K3W7XqVOn9L3vfU8Wi8VgZZ7V0NCg6OhoHT161OOXn+CKz7pv8Xn3HT7rvsNn7T6Hw6GvvvpKI0eO7LCdW2EkPDxcfn5+qqmpcdleU1OjyMjINveJjIx0q70kBQQEKCAgwGXbkCFD3CnVq4SGhvKL3Uf4rPsWn3ff4bPuO3zW7umoR6SVW7Np/P39lZSUpNLSUuc2u92u0tJSpaWltblPWlqaS3tJ2rp1a7vtAQDApcXtyzS5ubnKyspScnKyUlJSVFhYqMbGRmVnZ0uSMjMzFRUVpYKCAknSwoULdd111+mpp57STTfdpNdff12ffPKJXnjhBc++EwAA4JXcDiMZGRmqra3VkiVLVF1drcTERBUXFzsHqVZVVclq/bbDZcqUKVq/fr3+8z//Uw899JDGjRunN998U9///vc99y68VEBAgPLz8y+6JAXP47PuW3zefYfPuu/wWfcet+8zAgAA4EmsTQMAAIwijAAAAKMIIwAAwCjCCAAAMIow0k8cPnxY99xzj0aPHq2goCBdccUVys/PV3Nzs+nSfNKjjz6qKVOmKDg42KdvqGfCmjVrFBsbq8DAQKWmpmrHjh2mS/JJf/7zn3XLLbdo5MiRslgsHa73he4rKCjQ5MmTNXjwYA0fPlyzZs1SZWWl6bJ8DmGkn9i3b5/sdrvWrl2r3bt36+mnn1ZRUZEeeugh06X5pObmZv3oRz/SvHnzTJfiUzZs2KDc3Fzl5+dr586dSkhIUHp6uk6ePGm6NJ/T2NiohIQErVmzxnQpPu3999/XggUL9NFHH2nr1q06d+6cbrzxRjU2NpouzacwtbcfW7FihZ5//nkdPHjQdCk+65VXXtH999+vL7/80nQpPiE1NVWTJ0/Wc889J+mbOzRHR0frvvvu0+LFiw1X57ssFos2bdqkWbNmmS7F59XW1mr48OF6//33de2115oux2fQM9KP1dfXa+jQoabLALqkublZ5eXlstlszm1Wq1U2m01lZWUGKwM8p76+XpL4t9nDCCP91P79+7V69Wrde++9pksBuqSurk4tLS3OuzG3ioiIUHV1taGqAM+x2+26//779Q//8A/cRdzDCCO9bPHixbJYLB0+9u3b57LPsWPHNH36dP3oRz/S3LlzDVXufbrzWQNAVy1YsECff/65Xn/9ddOl+By316aBe372s59pzpw5HbYZM2aM8+fjx49r2rRpmjJlCosJusndzxqeFR4eLj8/P9XU1Lhsr6mpUWRkpKGqAM/IycnR22+/rT//+c+6/PLLTZfjcwgjvWzYsGEaNmxYl9oeO3ZM06ZNU1JSkl5++WWXBQfROXc+a3iev7+/kpKSVFpa6hxIabfbVVpaqpycHLPFAd3kcDh03333adOmTdq2bZtGjx5tuiSfRBjpJ44dO6apU6dq1KhRWrlypWpra52v8b9Kz6uqqtKpU6dUVVWllpYWVVRUSJLGjh2rkJAQs8V5sdzcXGVlZSk5OVkpKSkqLCxUY2OjsrOzTZfmc06fPq39+/c7nx86dEgVFRUaOnSoYmJiDFbmWxYsWKD169frrbfe0uDBg53jn8LCwhQUFGS4Oh/iQL/w8ssvOyS1+YDnZWVltflZv/fee6ZL83qrV692xMTEOPz9/R0pKSmOjz76yHRJPum9995r83c4KyvLdGk+pb1/l19++WXTpfkU7jMCAACMYlACAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAqP8H7dFLwoSg8cIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    transf_mat, logabsdet_mat = model_x._transform(new_x_f)\n",
    "\n",
    "# plot transform original variable\n",
    "plt.hist(transf_mat[:,0], bins=50, density=True, alpha=0.3, linewidth=2, color='salmon', label = \"Transformed PDF\")\n",
    "plt.hist(x, bins=50, density=True, alpha=0.3, label=\"Empirical PDF\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fc50d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    transf_mat, logabsdet_mat = model_y._transform(new_x_r)\n",
    "\n",
    "# plot transform original variable\n",
    "plt.hist(transf_mat[:,0], bins=50, density=True, alpha=0.3, linewidth=2, color='salmon', label = \"Transformed PDF\")\n",
    "plt.hist(y, bins=50, density=True, alpha=0.3, label=\"Empirical PDF\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c76a5f",
   "metadata": {},
   "source": [
    "### Results of original paper and marginal likelihood\n",
    "\n",
    "These processes were executed with 400 steps (epochs)\n",
    "\n",
    "| Dataset     | Estimation Time (minutes) | # Workers         | # samples |\n",
    "|-------------|---------------------------|-------------------|-----------|\n",
    "| MNU         | 56.02                     | 5                 |   100     |\n",
    "| Cha         | 200.35                    | 6                 |   300     |\n",
    "| SIM         | 41.69                     | 6                 |   100     |\n",
    "| SIMc        | 50.00                     | 6                 |   100     |\n",
    "| SIMG        | 48.22                     | 5                 |   100     |\n",
    "| SIMln       | 51.12                     | 5                 |   100     |\n",
    "| Tuebingen   | 83.69                     | 5                 |   108     |\n",
    "| Multi       | 194.82                    | 5                 |   300     |\n",
    "| Net         | 197.37                    | 6                 |   300     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad527adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial results\n",
    "import pandas as pd\n",
    "import os\n",
    "initial_ds = pd.read_csv('../partial_results/lik_scores.csv', sep = ',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d157ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder with the partial CSVs\n",
    "folder_path = os.path.abspath(\"../partial_results\")\n",
    "\n",
    "# List all CSV files starting with \"marginals_\"\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.startswith(\"marginals_\") and f.endswith(\".csv\")]\n",
    "\n",
    "# Initialize an empty DataFrame for merging\n",
    "merged_df_marg = None\n",
    "\n",
    "# Loop through each file and merge\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "\n",
    "    # Extract dataset name from filename (e.g., 'MNU' from 'marginals_MNU.csv')\n",
    "    dataset_name = file.replace(\"marginals_\", \"\").replace(\".csv\", \"\")\n",
    "\n",
    "    # Read file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Rename marginal_diff to specific name\n",
    "    df = df.rename(columns={\"marginal_diff\": f\"marginal_diff_{dataset_name}\"})\n",
    "\n",
    "    # Merge\n",
    "    if merged_df_marg is None:\n",
    "        merged_df_marg = df\n",
    "    else:\n",
    "        merged_df_marg = pd.merge(merged_df_marg, df, on=\"sample_idx\", how=\"outer\")\n",
    "merged_df_marg.rename(columns={'marginal_diff_MNU':'marginal_diff_MN-U'},inplace=True)\n",
    "merged_df_marg = merged_df_marg.iloc[:,1:]\n",
    "merged_df_marg.columns = pd.Series(merged_df_marg.columns).str.split('_').str[-1]\n",
    "\n",
    "# filter original results\n",
    "main_ds_initial_results = initial_ds.loc[:,pd.Series(merged_df_marg.columns) ]\n",
    "\n",
    "# new results \n",
    "main_ds_new_results = merged_df_marg + main_ds_initial_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5283f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_results(main_ds):\n",
    "    positive_counts = (main_ds > 0).sum()\n",
    "    negative_counts = (main_ds < 0).sum()\n",
    "    non_nan_counts = main_ds.notna().sum()\n",
    "    positive_rate_percent = (positive_counts / non_nan_counts * 100).round(2)\n",
    "\n",
    "    result_df = pd.DataFrame({\n",
    "        'positive_count': positive_counts,\n",
    "        'negative_count': negative_counts,\n",
    "        'positive_rate_percent': positive_rate_percent\n",
    "    })\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "09f8dcf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive_count</th>\n",
       "      <th>negative_count</th>\n",
       "      <th>positive_rate_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cha</th>\n",
       "      <td>129</td>\n",
       "      <td>171</td>\n",
       "      <td>43.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MN-U</th>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi</th>\n",
       "      <td>216</td>\n",
       "      <td>84</td>\n",
       "      <td>72.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Net</th>\n",
       "      <td>231</td>\n",
       "      <td>69</td>\n",
       "      <td>77.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIM</th>\n",
       "      <td>49</td>\n",
       "      <td>51</td>\n",
       "      <td>49.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIMc</th>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIMG</th>\n",
       "      <td>78</td>\n",
       "      <td>22</td>\n",
       "      <td>78.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIMln</th>\n",
       "      <td>80</td>\n",
       "      <td>20</td>\n",
       "      <td>80.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tuebingen</th>\n",
       "      <td>56</td>\n",
       "      <td>43</td>\n",
       "      <td>56.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           positive_count  negative_count  positive_rate_percent\n",
       "Cha                   129             171                  43.00\n",
       "MN-U                  100               0                 100.00\n",
       "Multi                 216              84                  72.00\n",
       "Net                   231              69                  77.00\n",
       "SIM                    49              51                  49.00\n",
       "SIMc                   50              50                  50.00\n",
       "SIMG                   78              22                  78.00\n",
       "SIMln                  80              20                  80.00\n",
       "Tuebingen              56              43                  56.57"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rate_results(main_ds_initial_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f3e43d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive_count</th>\n",
       "      <th>negative_count</th>\n",
       "      <th>positive_rate_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cha</th>\n",
       "      <td>130</td>\n",
       "      <td>170</td>\n",
       "      <td>43.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MN-U</th>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi</th>\n",
       "      <td>223</td>\n",
       "      <td>77</td>\n",
       "      <td>74.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Net</th>\n",
       "      <td>244</td>\n",
       "      <td>56</td>\n",
       "      <td>81.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIM</th>\n",
       "      <td>52</td>\n",
       "      <td>48</td>\n",
       "      <td>52.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIMc</th>\n",
       "      <td>59</td>\n",
       "      <td>41</td>\n",
       "      <td>59.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIMG</th>\n",
       "      <td>70</td>\n",
       "      <td>30</td>\n",
       "      <td>70.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIMln</th>\n",
       "      <td>82</td>\n",
       "      <td>18</td>\n",
       "      <td>82.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tuebingen</th>\n",
       "      <td>58</td>\n",
       "      <td>41</td>\n",
       "      <td>58.59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           positive_count  negative_count  positive_rate_percent\n",
       "Cha                   130             170                  43.33\n",
       "MN-U                  100               0                 100.00\n",
       "Multi                 223              77                  74.33\n",
       "Net                   244              56                  81.33\n",
       "SIM                    52              48                  52.00\n",
       "SIMc                   59              41                  59.00\n",
       "SIMG                   70              30                  70.00\n",
       "SIMln                  82              18                  82.00\n",
       "Tuebingen              58              41                  58.59"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rate_results(main_ds_new_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
